#+TITLE: 2020 Spring CS6250 Notes

(based on Canvas Modules - Last updated: July 14th, 2020)

** Lesson 1 Introduction, History, and Internet Architecture
   :PROPERTIES:
   :CUSTOM_ID: lesson-1-introduction-history-and-internet-architecture
   :END:

*Introduction*

In the first lecture, we are looking at major milestones in the history
of the Internet and we are reviewing original design choices and
principles of the Internet architecture. Then, we are looking at how
these initial design choices have formed the shape of the Internet
architecture, which looks like an hourglass figure. We are learning
about a model that can help us to explain the hourglass shape. Finally,
we are exploring how it would be, if we redesigned the Internet
architecture from scratch, in other words what if we adopted a
cleanslate design. Towards that goal, we look into some example
architecture design approaches that optimize for network control,
management and accountability, which were not first-class goals when the
Internet was first designed.

The number of devices that connect to the Internet has been exploding
with the number reaching more than 22 billion as of 2019. These devices
or end systems, can be very diverse for example: our laptops, mobile
devices, tablets, cameras or more recently household devices
(refrigerators and temperature control systems).

Through the course we will be learning about what it takes for two hosts
that maybe in different networks and also physically located in
different parts of the world to exchange data. We will be learning about
the protocols and the infrastructure we need, but also how to overcome
the challenges.

In this first lecture, we will start by looking at the devices (bridges
and switches for example) that help provide connectivity between hosts
in the same network.

*Why Study Computer Networks?*

- 

  #+BEGIN_QUOTE
    Internet growth
  #+END_QUOTE

The Internet is one of the most exciting and influential inventions.
Even though it started as a research experiment that escaped from a lab,
it eventually evolved into a global communications infrastructure, that
has been transforming almost all aspects of our lives with tremendous
impact. Given the explosion of applications that are becoming available
and the technologies that make it possible for different types of
devices to connect to the Internet (for example IoT, vehicles, sensors,
home devices, etc), the number of Internet users keeps increasing. As of
June 2018, The number of Internet users has been estimated at about 3.2
billion, while the number of estimated Internet users by 2020 is 4
billion.

- 

  #+BEGIN_QUOTE
    Networks play an instrumental a role in our society
  #+END_QUOTE

Indeed, the Internet has been playing a transformative role in our
lives. Just to name a few examples, it has been changing the way we do
business; for example e-commerce, advertising and cloud computing
applications. It has been changing the way we connect and communicate;
for example, e-mail, instant messaging, social networking and virtual
worlds applications have been extremely popular. The Internet has been
changing even how we fight; for example, we frequently read headline
news about large scale cyber attacks, incidents related to the
distribution of fake news, censorship and nationwide attacks. In turn,
these developments have law implications both nationwide and on a global
scale that we hadn't considered in the past; for example, we have to
examine questions such as which countries are responsible for the
Internet traffic that is crossing national boundaries, while the end
hosts are located in multiple countries across the globe?

- 

  #+BEGIN_QUOTE
    Networking is a playground for interdisciplinary research
    innovations
  #+END_QUOTE

The Internet's huge transformative role is connected with an
ever-expanding and evolving collection of technologies, system and
protocol architectures, algorithms, as well as powerful applications.
Indeed, the Internet is an amazing “playground” of ongoing
cross-disciplinary innovations that are coming from multiple fields such
as distributed systems, operating systems, computer architecture,
software engineering, algorithms and data structures, graph theory,
queuing theory, game theory and mechanism designs stemming from machine
learning and AI, cryptography, programming languages, and formal
methods, and more.

- 

  #+BEGIN_QUOTE
    Networking offers multidisciplinary research opportunities with
    potential for impact
  #+END_QUOTE

Networking is a field that offers tremendous opportunities for
interdisciplinary research work that sometimes crosses fields that can
be very different from each other. For example in the context of
research projects that study how to incentivize Internet providers to
keep their networks clean from infected hosts, the research work may
span across fields such as Internet security, economics, and social
sciences. What is perhaps the most exciting aspect about research in the
networking field, is the ability to design innovative and impactful
solutions, and immediately put them to test by leveraging existing
platforms.

*A Brief History of the Internet*

- 

  #+BEGIN_QUOTE
    J.C.R. Licklider proposed the "Galactic Network" (1962)
  #+END_QUOTE

The first vision of a Network - proposed as “Galactic Network” - by
J.C.R. Licklider was at MIT back in 1962. He envisioned that everyone
could quickly access data through a set of interconnected computers.
Licklider - as the head of the research program at Defense Advanced
Research Projects Agency (DARPA) - led a group of researchers to
experiment connecting two computers. An MIT researcher, Lawrence G.
Roberts connected one computer in MA to another computer located in CA
with a low-speed dial-up telephone line.

- 

  #+BEGIN_QUOTE
    The ARPANET (1969)
  #+END_QUOTE

The results of the first experiments showed that time-shared
infrastructure was working sufficiently well at that moment. But also at
the same time researchers indicated the need for packet switching
technology. Roberts continued developing the computer network concept,
which resulted in the first network which was connecting four nodes
(from UCLA, Stanford Research Institute, UCSB and Univ. of Utah,
respectively) into the initial ARPANET by the end of 1969.

- 

  #+BEGIN_QUOTE
    Network Control Protocol (NCP), an initial ARPANET host-to-host
    protocol (1970)
  #+END_QUOTE

As the number of computers that were added to the ARPANET increased
quickly, research work proceeded to designing protocols. The initial
ARPANET Host-to-Host protocol called Network Control Protocol (NCP) was
introduced in 1970, and it allowed the network users to begin developing
applications. One of the first applications that launched was email in
1972.

- 

  #+BEGIN_QUOTE
    Internetworking and TCP/IP (1973)
  #+END_QUOTE

At the same time, a DARPA team of researchers led by Bob Kahn,
introduced the idea of open-architecture networking so that the
individual networks may be independently designed and developed, in
accordance with the specific environment and user requirements of that
network. This led researchers to develop a new version of the NCP
protocol which would eventually be called the Transmission Control
Protocol / Internet Protocol (TCP/IP). Khan collaborated with Vint Cerf
in Stanford and presented the original TCP paper in 1973. The first
version of TCP later split its functionalities into two protocols, the
simple IP which provided only for addressing and forwarding of
individual packets, and the separate TCP which focused on service
features such as flow control and recovery from lost packets.

- 

  #+BEGIN_QUOTE
    The Domain Name System (DNS) (1983) and the World Wide Web (WWW)
    (1990)
  #+END_QUOTE

The scale of the Internet was increasing rapidly, and as a result it was
no longer feasible to have a single table of hosts to store names and
addresses. The Domain Name System (DNS) - which was designed to
translate domain names to IP addresses by a scalable distributed
mechanism - was introduced by Paul Mockapetris at USC in 1983. More
applications sprung up quickly. One of the first and most popular
applications was the World Wide Web (WWW), which was introduced by a
team of researchers led by Tim Berners-Lee.

*Internet Architecture Introduction*

After looking at the major milestones in the history of the Internet,
let's take a closer look into the current architectural design of the
Internet.

/Connecting hosts running the same applications but located in different
types of networks./ A computer network is a complex system that is built
on top of multiple components. These components can vary in technologies
making up different types of networks that offer different types of
applications. For example in the figure below, we have two BitTorrent
clients that communicate even though they are using very different
networks/technologies (Wifi vs Ethernet). So, how do these technologies
and components interconnect and come together to meet the needs of each
application? The designers of the network protocols provide structure to
the network architecture by organizing the protocols into layers.

[[file:media/image91.png]]

/Architecture, layers and functionalities./ So, the functionalities in
the network architecture are implemented by dividing the architectural
model into layers. Each layer offers different services.

/An analogy./ An analogy we can use to explain a layered architecture is
the airline system. Let's look first at the actions that a passenger
needs to take to move from the origin to the destination place. The
passenger purchases the ticket, checks the bags, goes through the gates
and after the plane takes off, the passenger travels on the plane to
their final destination. At the final destination, the passenger leaves
the aircraft, goes through the gate and claims their baggage.

[[file:media/image71.png]]

We can look at the above picture to identify a structure (or layers) and
the services (or functionalities) that are offered at every component of
the structure. Dividing the services into layers we get the framework
below. We notice that in this framework every layer implements some
functionality. Every layer works based on the service provided by the
layer below it, and also it provides some service to the layer that is
above.

The same principle of layers and functionalities is implemented with the
model of the Internet architecture.

/Layered architecture advantages: scalability, modularity, and
flexibility./ Some of the advantages of having a layered network stack
include scalability, modularity and the flexibility to add or delete
components which makes it easier overall for cost-effective
implementations.

*The OSI Model*

The Internet architecture follows a layered model, where every layer
provides some service to the layer above.

The International Organization for Standardization (ISO) proposed the
seven-layered OSI model shown below, which consists of the following
layers: application layer, presentation layer, session layer, transport
layer, network layer, data link layer, and physical layer.

[[file:media/image76.png]]

We will see in later sections a possible explanation about why the
Internet architecture came eventually to have this form. Separating the
functionalities into layers offers multiple advantages. But, are there
disadvantages of the layered protocol stack model? Some of the
disadvantages include:

1. 

   #+BEGIN_QUOTE
     Some layers functionality depends on the information from other
     layers, which can violate the goal of layer separation.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     One layer may duplicate lower layer functionalities. For example,
     the functionality of error recovery can occur in lower layers, but
     also on upper layers as well.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Some additional overhead that is caused by the abstraction between
     layers.
   #+END_QUOTE

In the following sections, we will go through a brief overview of the
layers, and more specifically we will focus on what each layer does
(service), how the layer is accessed (interface), how the layer is
implemented (example protocols), and how we refer to the packet of
information it handles.

For completeness we describe all layers found in the OSI reference
model.

The Internet architecture model though has five layers. The application,
presentation, and session layers are combined into a single layer, and
this combined layer is called the application layer. The interface
between the application layer and the transport layer are the sockets.
It is up to the application developer to design the functionality of the
overall application.

*Application, Presentation, and Session Layers*

The /application layer/ includes multiple protocols, some of the most
popular ones include: 1) The HTTP protocol (web), SMTP (e-mail), 2) The
FTP protocol (transfers files between two end hosts), and 3) The DNS
protocol (translates domain names to IP addresses). So the services that
this layer offers are multiple depending on the application that is
implemented. The same is true for the interface through which it is
accessed, and the protocol that is implemented. At the application
layer, we refer to the packet of information as a message.

The /presentation layer/ plays the intermediate role of formatting the
information that it receives from the layer below and delivering it to
the application layer. For example, some functionalities of this layer
are formatting a video stream or translating integers from big endian to
little endian format.

The /session layer/ is responsible for the mechanism that manages the
different transport streams that belong to the same session between
end-user application processes. For example, in the case of
teleconference application, it is responsible to tie together the audio
stream and the video stream.

*Transport and Network Layer*

The /transport layer/ is responsible for the end-to-end communication
between end hosts. In this layer, there are two transport protocols,
namely TCP and UDP. The services that TCP offers include: a
connection-oriented service to the applications that are running on the
layer above, guaranteed delivery of the application-layer messages, flow
control which in a nutshell matches the sender's and receiver's speed,
and a congestion-control mechanism, so that the sender slows its
transmission rate when it perceives the network to be congested. On the
other hand, the UDP protocol provides a connectionless best-effort
service to the applications that are running in the layer above, without
reliability, flow or congestion control. At the transport layer, we
refer to the packet of information as a segment.

In this layer, we refer to the packet of information as a datagram. The
/network layer/ is responsible for moving datagrams from one Internet
host to another. A source Internet host sends the segment along with the
destination address, from the transport layer to the network layer. The
network layer is responsible to deliver the datagram to the transport
layer in the destination host. The protocols in the network layer are:
1) The IP Protocol, which we often refer to as “the glue” that binds the
Internet together. All Internet hosts and devices that have a network
layer must run the IP protocol. The IP protocol defines a) the fields in
the datagram, and b) how the source/destination hosts and the
intermediate routers use these fields, so the datagrams that a source
Internet host sends reach their destination. 2) The routing protocols
that determine the routes that the datagrams can take between sources
and destinations.

*Data Link Layer and Physical Layer*

In this layer, we refer to the packets of information as frames. Some
example protocols in this layer include Ethernet, PPP, WiFi.

The /data link layer/ is responsible to move the frames from one node
(host or router) to the next node. More specifically, assuming we have a
sender and receiver host, the network layer will route the datagram
through multiple routers across the path between the sender and the
receiver. At each node across this path, the network layer passes the
datagram to the data link layer, which in turn delivers the datagram to
the next node. Then, at that node, the link layer passes the datagram up
to the network layer.

The /data link layer/ offers services that depend on the data link layer
protocol that is used over the link. Some example services include
reliable delivery, that covers the transmission of the data from one
transmitting node, across one link, and finally to the receiving node.
We note that this specific type of reliable delivery service is
different from the reliable delivery service that is offered by the TCP
protocol which offers reliability from the source host to the
destination end host.

The /physical layer/ facilitates the interaction with the actual
hardware and is responsible to transfer bits within a frame between two
nodes that are connected through a physical link. The protocols in this
layer again depend on the link and on the actual transmission medium of
the link. One of the main protocols in the data link layer, Ethernet,
has different physical layer protocols for twisted-pair copper wire,
coaxial cable, and single-mode fiber optics.

*Layers Encapsulation*

/How do the layers and the protocols that run on each layer communicate
with each other?/ To understand the concepts of encapsulation and
de-encapsulation, let's take a look at the following diagram which shows
the physical path that data take from the sending host to the receiving
host.

[[file:media/image124.png]]

/Encapsulation and De-encapsulation./ The sending host sends an
application layer message M to the transport layer. The transport layer
receives the message, and it appends the transport layer header
information (Ht). The application message along with the transport layer
header is called segment (or transport-layer segment). The segment thus
encapsulates the application layer message. This added information can
help the receiving host to a) inform the receiver-side transport layer
about which application to deliver the message up to, and b) perform
error detection and determine whether bits in the message have been
changed along the route.

The segment is then forwarded to network layer which in turn, adds it's
own network header information (Hn). The entire combination of the
segment and the network header is called datagram. We say that the
datagram encapsulates the segment. The header information that the
network layer appends includes the source and destination addresses of
the end hosts. The same process continues for the link layer which in
turn it appends its own header information(Hl). The message at the link
layer is called frame, which is transmitted across the physical medium.
At each layer the message is a combination of two parts: a) the payload
which is the message from the layer above, and b) the new appended
header information. At the receiving end, the process is reversed, with
headers being stripped off at each layer. This reverse process is known
as de-encapsulation.

/Intermediate devices and encapsulation./ The path that connects the
sending and the receiving hosts may include intermediate layer-3
devices, such as routers, and layer-2 devices such as switches. We will
see later how switches and routers work, but for now we note that both
routers and layer-2 switches implement protocol stacks similarly to
end-hosts. The difference is that routers and layer-2 switches do not
implement all the layers in the protocol stack; routers implement layers
1 to 3, and layer-2 switches implement layers 1 to 2. So, going back to
our diagram, when the data leave the sending host and they are received
by the layer-2 switch, the switch implements the same process of
de-encapsulation to process the data and encapsulation to send the data
forward to the next device.

/A design choice./ We note again that end-hosts implement all five
layers while the intermediate devices don't. This design choice ensures
that the Internet architecture puts much of its complexity and
intelligence at the edges of the network while keeping the core simple.
Next, we will look deeper into the so-called end-to-end principle.

*The End to End Principle*

The end-to-end (e2e) principle is a design choice that characterized and
shaped significantly the current architecture of the Internet. The e2e
principle suggests that specific application-level functions usually
cannot, and preferably should not be built into the lower levels of the
system at the core of the network.

In simple terms, the e2e principle is summarized as: the network core
should be simple and minimal, while the end systems should carry the
intelligence. As mentioned in the seminal paper “End-to-End Arguments in
System Design” by Saltzer, Reed, and Clark: “The function in question
can completely and correctly be implemented only with the knowledge and
help of the application standing at the endpoints of the communications
system. Therefore, providing that questioned function as a feature of
the communications systems itself is not possible.”

The same paper reasoned that many functions can only be completely
implemented at the endpoints of the network, so any attempt to build
features in the network to support specific applications must be
avoided, or only viewed as a tradeoff. The reason was that not all
applications need the same features and network functions to support
them. Thus building such functions in the network core is rarely
necessary. So, systems designers should avoid building any more than the
essential and commonly shared functions into the network.

Many people argue that the e2e principle allowed the internet to grow
rapidly, because evolving innovation took place at the network edge, in
the form of numerous applications and a plethora of services, rather
than in the middle of the network, which could be hard to later modify.

What were the designers' original goals that led to the e2e principle?
Moving functions and services closer to the applications that use them,
increases the flexibility and the autonomy of the application designer
to offer these services to the needs of the specific application. Thus,
the higher-level protocol layers, are more specific to an application.
Whereas the lower-level protocol layers are free to organize the
lower-level network resources to achieve application design goals more
efficiently and independently of the specific application.

*Violations of the End-to-End Principle and NAT Boxes*

Despite the fact that the e2e principle offers multiple advantages to
the Internet and its evolution, there have still been cases where this
principle needs to be violated.

/Some examples of the e2e violation:/

Examples include firewalls and traffic filters. The firewalls usually
operated at the periphery of a network and they monitor the network
traffic that is going through, to allow or drop traffic, if the traffic
is flagged as malicious. Firewalls violate the e2e principle since they
are intermediate devices that are operated between two end hosts and
they can drop the end hosts communication.

Another example of an e2e violation is the Network Address Translation
(NAT) boxes. NAT boxes help us as a bandaid measure to deal with the
shortage of Internet addresses. Let's see in more detail how a
NAT-enabled home router operates. Let's assume we have a home network,
where we have multiple devices we want to connect to the Internet. An
internet service provider typically assigns a single public IP address
(82.10.250.19) to the home router and specifically to the interface that
is facing the public global Internet, as shown in the figure below.

[[file:media/image115.png]]

The other interface of the NAT-enabled router that is facing the home
network (along with all other device-interfaces in the home network)
gets an IP address that belongs to the same private subnet. This subnet
must belong to the address spaces that are reserved as private, eg
10.0.0/24 or 192.168.0.0/24. This means that the IP addresses that
belong to this private subnet only have meaning to devices within that
subnet. So we can have hundreds of thousands of private networks with
the same address range (eg 10.0.0.0/24). But, these private networks are
always behind a NAT, which takes care of the communication between the
hosts on the private network and the hosts on the public Internet.

All traffic that leaves the home router and it is destined to hosts in
the public Internet must have as the source IP address the IP of the
public facing interface of the NAT-enabled router. Similarly, all
traffic that enters the home network through the router, must have as
the destination address the IP of the public facing interface of the
NAT-enabled router. The home router plays the role of a translator
maintaining a NAT translation table, and it rewrites the source and
destination IP addresses and ports.

[[file:media/image94.png]]

The translation table provides a mapping between the public facing IP
address/ports, and the IP addresses/ports that belong to hosts inside
the private network. For example, let's assume that a host 10.0.0.1
inside the private network, uses port 3345 to send traffic to a host in
the public Internet with IP address 128.119.40.186 and port 80. Then the
NAT table says that packets with the source IP address of 10.0.0.1 and
source port 3345, they should be rewritten to a source address
138.76.29.7 and a source port of 5001 (or any source port number that is
not currently used in the NAT translation table). Similarly, packets
with a destination IP address of 138.76.29.7 and destination port of
5001, they will be rewritten to destination IP address 10.0.0.1 and
destination port 3345.

/Why the NAT boxes violate the e2e principle?/

The hosts that are behind NAT boxes are not globally addressable, or
routable. As a result, it is not possible for other hosts on the public
Internet to initiate connections to these devices. So, if we have a host
behind a NAT and a host in the public Internet, then by default they
cannot communicate without the intervention of a NAT box.

There are some workarounds to allow hosts to initiate connections to
hosts that behind NATs. Some example tools and protocols include STUN (a
tool that allows hosts to discover NATs and the public IP address and
port number that the NAT has allocated for the application that the host
wants to communicate with), and UDP hole punching (it established
bidirectional UDP connections between hosts behind NATs).

*The Hourglass Shape of Internet Architecture*

The Internet protocol stack has a layered architecture that resembles an
hourglass shape. Was the Internet architecture always shaped like an
hourglass, and there has always been a single protocol at the network
layer? If we look back in the early nineties, we will see that there
were several other network-layer protocols that were competing with
IPv4. For example, Novell's IPX and the X.25 network protocol used in
Frame Relay. So the network layer did not include only one protocol, but
there were multiple protocols that were competing with each other at
that time.

Why has there have been more frequent innovations at the lower or higher
layers of the protocol hourglass? Why have the protocols at the waist of
the hourglass (mostly IPv4, TCP, and UDP) been difficult to replace, and
have they outcompeted any protocols that offer the same or similar
functionalities? Looking ahead, and assuming that we want to design and
introduce new and potentially better protocols, how can we make it more
likely that the new protocols will outcompete and replaces existing and
widely used incumbent protocols?

Researchers have suggested a model called the Evolutionary Architecture
model, or EvoArch, that can help to study layered architectures and
their evolution in a quantitative manner. Through this model researchers
were able to explain how the hierarchical structure of the layer
architecture eventually lead to the hourglass shape.

In the next topic, we will talk about the details of the model and how
it can help us explain the evolution of the Internet architecture.

*Evolutionary Architecture Model*

[[file:media/image112.jpg]][[file:media/image16.jpg]]

In this section, we will talk about a model that attempts to answer our
previous questions. Researchers have suggested a model - the
Evolutionary Architecture model or EvoArch - that can help to study
layered architectures, and their evolution in a quantitative manner. The
EvoArch model considers an abstract model of the Internet's protocol
stack that has the following components:

- 

  #+BEGIN_QUOTE
    /Layers./ A protocol stack is modeled as a directed and acyclic
    network with L layers.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Nodes./ Each network protocol is represented as a node. The layer
    of a node u is denoted by l(u).
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Edges./ Dependencies between protocols are represented as directed
    edges.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Node incoming edges. If a protocol u at layer l uses the service
    provided by a protocol w at the lower layer l−1, then this is
    represented by an “upwards” edge from w to u.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Node substrates./ We refer to substrates of a node u, S(u), as the
    set of nodes that u is using their services. Every node has at least
    one substrate, except the nodes at the bottom layer.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Node outgoing edges./ The outgoing edges from a node u terminate at
    the products of u. The products of a node u are represented by P(u).
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Layer generality./ Each layer is associated with a probability
    s(l), which we refer to as layer generality. A node u at layer l+1
    selects independently each node of layer l as the substrate with
    probability s(l). The layer generality decreases as we move to
    higher layers, and thus protocols at lower layers are more general
    in terms of their functions or provided services than protocols at
    higher layers. For example, in the case of the Internet protocol
    stack, layer 1 is very general and the protocols at this layer offer
    a very general bit transfer service between two connected points,
    which most higher layer protocols would use.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Node evolutionary value./ The value of a protocol node, v(u), is
    computed recursively based on the products of u. By introducing the
    evolutionary value of each node, the model captures the fact that
    the value of a protocol u is driven by the values of the protocols
    that depend on it. For example, let's consider again the Internet
    protocol stack. TCP has a high evolutionary value because it is used
    by many higher layer protocols and some of them being valuable
    themselves. Let's assume that we introduce a brand new protocol, at
    the same layer as TCP, that may have better performance or other
    great new features. The new protocol's evolutionary value will be
    low if it is not used by important or popular higher layer
    protocols, regardless of the great new features it may have. So the
    evolutionary value determines if the protocol will survive the
    competition with other protocols, at the same layer, that offer
    similar services.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Node competitors and competition threshold./ We refer to the
    competitors of a node u, C(u), as the nodes at layer l that share at
    least a fraction c of node u's products. We refer to the fraction c,
    as the competition threshold. So, a node w competes with a node u,
    if w shares at least a fraction c of u's products.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Node death rate./ The model has a death and birth process in place,
    to account for the protocols that cease or get introduced
    respectively. The competition among nodes becomes more intense, and
    it is more likely that a protocol u dies if at least one of its
    competitors has a higher value than itself. When a node u dies, then
    its products also die, if their only substrate is u.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Node basic birth process./ The model, in its simplest version, has
    a basic birth process in place, where a new node is assigned
    randomly to a layer. The number of new nodes at a given time is set
    to a small fraction (say 1% to 10%) of the total number of nodes in
    the network at that time. So, the larger a protocol stack is, then
    the faster it grows.
  #+END_QUOTE

/Toy Example/

To illustrate the above model and the parameters, let's consider a toy
network example with L equal to 4 layers. The evolutionary value of each
node is shown inside each circle. The generality probability for each
layer is shown at the left of each layer, and it is denoted as s(l). As
we noted earlier, the generality of the layers decreases as we move to
higher layers, so on average, the number of products per node decreases
as well. Let's further assume that we have a competition threshold c =
⅗. Nodes u, q and w compete in layer 2. U and q compete, but this is
unlikely to cause q to die because u and q have comparable evolutionary
values. In contrast, it is likely that w will die because its value is
much less than that of its maximum-value competitor, u.

#+BEGIN_QUOTE
  [[file:media/image110.jpg]]
#+END_QUOTE

/EvoArch iterations:/

EvoArch is a discrete-time model that is executed over rounds. At each
round, we perform the following steps: A) We introduce new nodes, and we
place them randomly at layers. B) We examine all layers, from the top to
the bottom, and we perform the following tasks: 1) We connect the new
nodes that we may have just introduced to that layer, by choosing
substrates based on the generality probabilities of the layer below
s(l−1), and by choosing products for them based on the generality
probability of the current layer s(l). 2) We update the value of each
node at each layer l, given that we may have new nodes added to the same
layer l. 3) We examine all nodes, in order of decreasing value in that
layer, and remove the nodes that should die. C) Finally, we stop the
execution of the model when the network reaches a given number of nodes.

The figure above shows the width of each layer we execute the EvoArch
model for a network of 10 layers over multiple rounds. The main takeaway
message from this figure is that the layer width decreases as we move
from the bottom layer to a middle layer, around layer 5, and then it
increases again as we move towards the top layer.

/Implications for the Internet Architecture and future Internet
architecture:/

With the help of the EvoArch model, how can we explain the survival of
the TCP/IP stack given that it appeared around the 70s or 80s when the
telephone network was very powerful? The EvoArch model suggests that the
TCP/IP stack was not trying to compete with the telephone network
services. The TCP/IP was mostly used for applications such as FTP,
E-mail, and Telnet, so it managed to grow and increase its value without
competing or being threatened by the telephone network, at that time
that it first appeared. Later it gained even more traction, with
numerous and powerful applications relying on it.

IPv4, TCP, and UDP provide a stable framework through which there is an
ever-expanding set of protocols at the lower layers (physical and
data-link layers), as well as new applications and services at the
higher layers. But at the same time, these same protocols have been
difficult to replace or even modify significantly. EvoArch provides an
explanation for this. A large birth rate at the layer above the waist
can cause death for the protocols at the waist if these are not chosen
as substrates by the new nodes at the higher layers. The waist of the
Internet architecture is narrow, but also the next higher layer (the
transport layer) is also very narrow and stable. So, the transport layer
acts as an “evolutionary shield” for IPv4, because any new protocols
that might appear at the transport layer are unlikely to survive the
competition with TCP and UDP which already have multiple products. In
other words, the stability of the two transport protocols adds to the
stability of IPv4, by eliminating any potential new transport protocols,
that could select a new network layer protocol instead of IPv4.

Finally, in terms of future and entirely new Internet architectures, the
EvoArch model predicts that even if these brand new architectures do not
have the shape of an hourglass initially, they will probably do so as
they evolve, which will lead to new ossified protocols. The model
suggests that one way to proactively avoid these ossification effects,
that we now experience with TCP/IP, a network architect should try to
design the functionality of each layer so that the waist is wider,
consisting of several protocols that offer largely non-overlapping but
general services, so that they do not compete with each other.

*Optional Reading: Architecture Redesign*

/Why a clean-slate design approach?/

Some of the major design principles of the current Internet architecture
are layering, packet switching, a network of collaborating networks,
intelligent end-systems as well as the end-to-end argument. Despite our
initial intentions, the Internet is currently facing major challenges in
multiple areas such as security, resilience and availability,
scalability and management, quality of service, user experience, and
economics. But what if we redesigned the Internet architecture from
scratch? Many researchers believe that it is necessary, and timely, to
rethink the fundamental assumptions and design decisions via a
clean-slate design approach. That clean-slate approach would be based on
out of the box thinking, the design of new network architectures, and
experimentation to evaluate the new ideas, to improve them and also to
give them a realistic chance of deployment.

/The clean-slate design as a process:/

An important aspect about designing new Internet architectures through a
clean-slate approach, is the ability to deploy and thoroughly test them,
which can take place at an appropriate experimental facility. Such a
facility has to fulfill some requirements such as offer a large scale
infrastructure, include different technologies, attract real users and
their traffic, enable parallel experiments with distinct networking
architectures such as different naming schemes, different layering
approaches, and different forwarding techniques, while at the same time,
new services should be able to explore the new capabilities and should
be made available to users who opt-in. In this context, a clean-slate
should be viewed as a design process, rather than as a result in itself.
Through this process, we may identify innovative services and
applications, which may become mature enough to be commercially deployed
on the existing Internet. Another possibility may be that we may create
an entirely new network architecture, which eventually replaces today's
architecture. Or perhaps, the most “conservative” outcome may even be
that we learn that the current Internet architecture is the “best”
possible solution. On the other hand, the most “radical” outcome can be
that such an experimental facility, which allows multiple sub-system
architectures and network services to co-exist, becomes the blueprint
for the future Internet.

/Redesigning the Internet architecture to optimize for control and
management:/

One research group (“4D”) for example, started from a small set of clean
slate design principles different from those of the Internet today:
network-level objectives, network-wide views, and direct control, with
functionality in four components: the data, discovery, dissemination,
and decision planes. In their work, the decision plane has a
network-wide view of the topology and traffic, and exerts direct control
over the operation of the data plane -- radically different from today's
Internet - no decision logic is hardwired in protocols distributed among
the network elements. The output of the decision logic is communicated
to routers/switches by the dissemination plane. Their work investigates
an extreme design point where the decision logic is completely separated
from distributed protocols. The 4D group argues that technology trends
toward ever-more powerful, reliable, and inexpensive computing platforms
make their design point attractive in practice. [Greenberg 2005]

/Redesigning the Internet architecture to offer better accountability:/

Given the fact that IP network layer provides little to no protection
against misconfiguration or malicious actions which occur frequently,
Researchers proposed network “accountability” in order to establish the
foundation for defenses against those behaviors. Accountability is an
ability to associate each action with the responsible entity. The
proposed work addressed two accountabilities, i.e., source
accountability and control-plane accountability, and illustrated that
both accountabilities could be improved by a network layer called
Accountable Internet Protocol (AIP).

Source accountability is the ability to trace actions to a particular
end host and stop that host from misbehaving. It is necessary to
describe the address format of AIP first and then illustrate how AIP
improved source accountability. AIP addresses are of the form AD:EID,
where AD is the identifier for the network that the host belongs to, and
EID is a globally unique host identifier. For source accountability, AIP
makes use of this self-certifying addressing to develop simple
mechanisms that verify the source of packets and drop the packets if the
sources are spoofed. In addition, AIP can throttle certain forms of
unwanted traffic using a simple “shut-off message”.

Control-plane accountability is the ability to pinpoint and prevent
attacks on routing. The proposed work suggested ways to improve
control-plane accountability by providing origin authentication
(ensuring that the network that appears to originate paths is indeed the
correct network) and path authentication (checking the integrity of the
network path) to detect misleading route advertisements.

*Interconnecting Hosts and Networks*

We have different types of devices that help to provide connectivity
between hosts that are in the same network, or help interconnect
networks. These devices offer different services and they operate over
different layers.

/Repeaters and Hubs:/ They operate on the physical layer (L1), as they
receive and forward digital signals to connect different Ethernet
segments. They provide connectivity between hosts that are directly
connected (in the same network). The advantage is that they are simple
and inexpensive devices, and they can be arranged in a hierarchy.
Unfortunately, hosts that are connected through these devices belong to
the same collision domain, meaning that they compete for access to the
same link.

/Bridges and Layer2-Switches:/ These devices can enable communication
between hots that are not directly connected. They operate on the data
link layer (L2) based on MAC addresses. They receive packets and they
forward them to reach the appropriate destination. A limitation is the
finite bandwidth of the outputs; If the arrival rate of the traffic is
higher than the capacity of the outputs then packets are temporarily
stored in buffers. But if the buffer space gets full, then this can lead
to packet drops.

/Routers and Layer3-Switches:/ These are devices that operate on Layer
3. We will talk more about these devices and the routing protocols on
the following lectures.

*Learning Bridges*

A bridge is a device with multiple inputs/outputs. A bridge transfers
frames from an input to one (or multiple) outputs. Though it doesn't
need to forward all the frames it receives. In this topic we will talk
about how a bridge learns how to perform that task.

A learnings bridge learns, populates and maintains, a forwarding table.
The bridge consults that table so that it only forwards frames on
specific ports, rather than over all ports.

For example, let's consider the topology on the following figure. When
the bridge receives a frame on port 1, with source Host A and
destination Host B, the bridge does not have to forward it to port 2.

So how does the bridge learn? When the bridge receives any frame this is
a “learning opportunity” to know which hosts are reachable through which
ports. This is because the bridge can view the port over which a frame
arrives and the source host. Going back to our example topology,
eventually the bridge builds the following forwarding table.

[[file:media/image72.jpg]][[file:media/image107.jpg]]

*Looping Problem in Bridges and the Spanning Tree Algorithm*

Unfortunately using bridges to connect LAN's fails, if the network
topology results in loops (cycles). In that case, the bridges loop
through packets forever!

The answer to this problem is excluding links that lead to loops by
running the spanning tree algorithm. Let's represent the topology of the
network as a graph. The bridges are represented as nodes and the links
between the bridges are represented as edges. The goal of the spanning
tree algorithm is to have the bridges select which links (ports) to use
for forwarding eliminating loops.

Let's take a look at how bridges run this distributed algorithm.

Every node (bridge) in the graph has an ID. The bridges eventually
select one bridge as the root of the topology. Let's see how this
selection happens.

The algorithm runs in “rounds” and at every round each node sends to
each neighbor node a configuration message with three fields: a) the
sending node's ID, b) the ID of the roots as perceived by the sending
node, and c) the number of hops between that (perceived) root and the
sending node.

At every round, each node keeps track of the best configuration message
that it has received so far, and it compares that against the
configuration messages it receives from neighboring nodes at that round.

At the very first round of the algorithm, every node thinks that it is
the root. So for a node with an ID 3 for example, the node sends a
configuration message <3, 3, 0> to its neighbors. Note that the distance
of the node from itself (perceived root) is 0.

So how does a node compare two configuration messages? Between two
configurations, a node selects one configuration as better if: a) The
root of the configuration has a smaller ID, or if b) The roots have
equal IDs, but one configuration indicates smaller distance from the
root, or if c) Both roots IDs are the same and the distances are the
same, then the node breaks the tie by selecting the configuration of the
sending node that has with the smallest ID. In addition, a node stops
sending configuration messages over a link (port), when the node
receives a configuration message that indicates that it is not the root,
eg when it receives a configuration message from a neighbor that: a)
either closer to the root, or b) it has the same distance from the root,
but it has a smaller ID.

As an example, let's consider the topology below. By running the above
steps on this topology, we note that in the first round B3 receives
(B2,B2,0) and (B5,B5,0), so it accepts B2 as the root. So in the second
round it sends (B3,B2,1) to its neighbors.

Similarly for B2; In the first round, B2 receives (B3,B3,0) and
(B1,B1,0), it accepts B1 as the root. So in the second round B2 sends
(B2,B1,1).

Finally, B5 receives configuration messages from B3, B7 and B1. B5
accepts B1 as root and sends (B5, B1, 1) to B3. This results to B3 also
accepting B1 as root. In addition, B3 realizes that both its neighbors,
namely B2 and B5 are closer to the root (B1) than itself. This causes B3
to not select any of its links (ports). So B3 stops participating in
forwarding traffic.

[[file:media/image113.png]][[file:media/image93.png]]

*Quiz 1*

Some data link layer protocols, such 802.11 (WiFi), implement some basic
error correction as the physical medium used is easily prone to
interference and noise (such as a nearby running microwave). Is this a
violation of the end-to-end principle?

- 

  #+BEGIN_QUOTE
    Yes
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    No
  #+END_QUOTE

/Answer: No, because violations of the e2e principle typically refer to
scenarios where it is not possible to implement a functionality entirely
at the end hosts, such as NAT and firewalls. In this question, we have a
lower level protocol implementing error checking./

*Quiz 2*

Which of the following are ramifications of the “hourglass shape of the
internet”?

1. 

   #+BEGIN_QUOTE
     Many technologies that were not originally designed for the
     internet have been modified so that they have versions that can
     communicate over the internet (such as Radio over IP).
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     It has been a difficult and slow process to transition to IPv6,
     despite the shortage of public IPv4 addresses.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Applications like BitTorrent leverage peer-to-peer networking
     instead of a more traditional client-server model for better
     performance.
   #+END_QUOTE

/Answer:/

1) 

   #+BEGIN_QUOTE
     /is correct; Modifying a technology so that it is compatible with
     the rest of the internet (i.e., by making it compatible with IP)
     greatly enhances market penetration (from the vendor's
     perspective), and/or decreases the amount of extra development that
     would need to happen./
   #+END_QUOTE

2) 

   #+BEGIN_QUOTE
     /is correct. A big part of the Internet infrastructure uses IPV4
     while the cost of transitioning is high. This reflects as a
     consequence of the narrow waist./
   #+END_QUOTE

3) 

   #+BEGIN_QUOTE
     /is not relevant here. The hourglass shape of the Internet refers
     to Internet architecture in terms of protocols available at the
     different layers./
   #+END_QUOTE

*Quiz 3*

Which of the following statements are correct?

1. 

   #+BEGIN_QUOTE
     The Spanning Tree Algorithm helps to prevent broadcast storms
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The Spanning Tree Algorithm presented in this lecture always
     results in a spanning tree that places the root in a topologically
     central location, so that all the nodes are as “close” as possible
     to the root.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Network traffic cannot traverse an inactive link.
   #+END_QUOTE

/Answer:/

1) 

   #+BEGIN_QUOTE
     /is correct. That is the purpose of the Spanning Tree Algorithm.
     Although it is still possible to have broadcast storms on the
     network (such as from a bad network card), STP prevents broadcast
     storms that result from having loops present in the network
     topology./
   #+END_QUOTE

2) 

   #+BEGIN_QUOTE
     /is incorrect. The Spanning Tree Algorithm presented guarantees a
     unique spanning tree that all the nodes will agree to, but
     sometimes this isn't the most “optimal” tree possible. Network
     administrators can configure the switch ID if they want to have a
     specific spanning tree./
   #+END_QUOTE

3) 

   #+BEGIN_QUOTE
     /is incorrect. Traffic can still reach the link, but the link is
     not used to forward traffic./
   #+END_QUOTE

** Lesson 2: Transport and Applications Layers
   :PROPERTIES:
   :CUSTOM_ID: lesson-2-transport-and-applications-layers
   :END:

*Introduction*

In the second lecture, we are learning about the transport layer and
mainly focusing on the TCP protocol.

As a reminder, our overarching theme of the course is to have a clear
understanding about it what it takes for two hosts to exchange data.
Let's have two hosts that are in different networks and also physically
located in different parts of the world. In this lecture, we are
focusing on the logical end-to-end connection between processes that are
running on these two hosts. This logical connection is happening through
the transport layer. We will focus on the TCP protocol and we will learn
about algorithms that provide important services such as reliability,
flow control, and congestion control.

We will also learn about more recent versions of the TCP protocol that
are designed for better performance, or they are designed to meet
diverse requirements. For example, we know that traffic in datacenters
can be originating from different applications. And some applications
may require predictable latency while others may require sustained
throughput. The original TCP mechanisms were not designed with these
goals in mind.

*Introduction to Transport Layer and the Relationship between Transport
and Network Layer*

The transport layer provides an end-to-end connection between two
applications that are running on different hosts. Of course the
transport layer provides this logical connection regardless if the hosts
are in the same network.

Here is how it works; The transport layer on the sender host receives a
message from the application layer and it appends its own header. We
refer to this combined message as a segment. This transport layer
segment is then sent to the network layer which will further append
(encapsulate) this segment with its header information. Then it will
send it to the receiving host via routers, bridges, switches etc.

One might ask, why do we need an additional layer between the
application and the network layer? Recall, that the network-layer is
based on a best effort delivery service model. According to this model,
the network layer makes a best effort to deliver data packets. Thus, it
doesn't guarantee the delivery of packets, nor it guarantees integrity
in data. So, here is where the transport layer comes to offer some of
these functionalities. This allows application programmers to develop
applications assuming a standard set of functionalities that are
provided by the transport layer. So the applications can run over
diverse networks without having to worry about different network
interfaces or possible unreliability of the network.

Within the transport layer, there are two main protocols: User datagram
protocol (UDP) and the Transmission Control Protocol (TCP). These
protocols differ based on the functionality they offer to the
application developers; UDP provides very basic functionality and relies
on the application-layer to implement the remaining. On the other hand,
TCP provides some strong primitives with a goal to make end-to-end
communication more reliable and cost-effective. In fact, because of
these primitives, TCP has become quite ubiquitous and is used for most
of the applications today. We will now look at these functionalities in
detail.

*Multiplexing: why we need it?*

One of the main desired functionalities of the transport layer is the
ability for a host to run multiple applications to use the network
simultaneously; which we refer to as multiplexing.

Let us consider a simple example to further illustrate why we need
transport-layer multiplexing. Consider a user who is using Facebook
while also listening to music on Spotify. Clearly, both of these
processes involve communication to two different servers. How do we make
sure that the incoming packets are delivered to the correct application?
Note that, the network layer uses only the IP address and an IP address
alone does not say anything about which processes on the host should get
the packets. Thus, we need an addressing mechanism to distinguish the
many processes sharing the same IP address on the same host.

The transport layer solves this problem, by using additional identifiers
known as ports. Each application binds itself to a unique port number by
opening sockets and listening for any data from a remote application.
Thus, the transport layer can do multiplexing by using ports.

There are two ways in which we can use multiplexing. Connectionless and
connection oriented multiplexing. As the name suggests, it depends if we
have a connection established between the sender and the receiver or
not.

In the next topic we are looking into multiplexing and demultiplexing.

*Connection Oriented and Connectionless Multiplexing and Demultiplexing*

In this topic, we will talk about multiplexing and demultiplexing.

[[file:media/image129.png]][[file:media/image41.png]]

 Let's consider the scenario shown in the figure above which includes
three hosts running an application. A receiving host that receives an
incoming transport-layer segment will forward it to the appropriate
socket. The receiving host identifies the appropriate socket by
examining a set of fields in the segment.

The job of delivering the data that are included in a transport-layer
segment to the appropriate socket, as defined in the segment fields, is
called demultiplexing.

Similarly, the sending host will need to gather data from different
sockets, and encapsulate each data chunk with header information (that
will later be used in demultiplexing) to create segments, and then
forward the segments to the network layer. We refer to this job as
multiplexing.

As an example, let's take a closer look at the host in the middle. The
transport layer in the middle host, will need to demultiplex the data
arriving from the network layer to the correct socket (P1 or P2). Also,
the transport layer in the middle host, will need to perform
multiplexing, by collecting the data from sockets P1 or P2, then by
generating transport-layer segments, and then finally by forwarding
these segments to the network layer below.

Now, let's focus at the socket identifiers: The sockets are identified
based on special fields (shown below) in the segment such as the source
port number field and the destination port number field.

/We have two flavors of multiplexing/demultiplexing:/ the connectionless
and connection oriented*.*

[[file:media/image99.png]][[file:media/image11.png]]

/First, we will talk about the connectionless multiplexing and
demultiplexing./ The identifier of a UDP socket is a two-tuple that is
consisted of a destination IP address and a destination port number.
Consider two hosts, A and B, which are running two processes at UDP
ports a and b respectively. Let's suppose that host A sends data to host
B. The transport layer in host A creates transport layer segment by the
application data, the source port and the destination port, and forwards
the segment to the network layer. In turn the network layer encapsulates
the segment into a network-layer datagram and sends it to host B with
best effort delivery. Let's suppose that the datagram is successfully
received by host B. Then the transport layer at host B, identifies the
correct socket by looking at the field of the destination port. In case
that host B runs multiple processes, each process will have its own own
UDP socket and therefore a distinct associated port number. Host B will
use this information to demultiplex receiving data to the correct
socket. If Host B receives UDP segments with destination port number, it
will forward the segments to the same destination process via the same
destination socket, even if the segments are coming from different
source hosts and/or different source port numbers.

/Now let's consider the connection oriented multiplexing and
demultiplexing./

The identifier for a TCP socket is a four tuple that is consisted by the
source IP, source port, destination IP and destination port. Let's
consider the example of a TCP client server as shown in the figure 2.29.
The TCP server has a listening socket that waits for connections
requests coming from TCP clients. A TCP client creates a socket and
sends a connection request, which is a TCP segment that has a source
port number chosen by the client, a destination port number 12000 and a
special connection-establishment bit set in the TCP header. Finally, the
TCP server receives the connection request, and the server creates a
socket that is identified by the four-tuple source IP, source port,
destination IP and destination port. The server uses this socket
identifier to demultiplex incoming data and forward them to this socket.
Now, the TCP connection is established and the client and server can
send and receive data between one another.

Example: Let's look at an example connection establishment.

[[file:media/image135.png]]

In this example, we have three hosts A, B and C. Host C and A initiate
two and one HTTP sessions to server B, respectively. Hosts C and A
assign port numbers to their connections independently of one another.
Host C assigns port numbers 26145 and 7532. In case Host A assigns the
same port number as C, host B will still be able to demultiplex incoming
data from the two connections because the connections are associated
with different source IP addresses.

/Let's add a final note about web servers and persistent HTTP/. Let's
assume, we have a webserver listening for connection requests at port
80. Clients send their initial connection requests and their subsequent
data with destination port 80. The webserver is able to demultiplex
incoming data based on their unique source IP addresses and source port
numbers. The client and the server maybe persistent HTTP, in which case,
they exchange HTTP messages via the same server socket. The client and
the server maybe using non-persistent HTTP, where for every request and
response, a new TCP connection and a new socket are created and closed
for every response/request. In the second case, a busy webserver may
experience severe performance impact.

*A word about the UDP protocol*

This lecture is primarily focused on TCP. Before exploring more topics
on the TCP protocol let's briefly talk about UDP.

UDP is: a) an unreliable protocol as it lacks the mechanisms that TCP
has in place and b) a connectionless protocol that does not require the
establishment of a connection (eg threeway handshake) before sending
packets.

The above description doesn't sound so promising... so why do we have
UDP at the first place? Well, it turns out that it is exactly the lack
of those mechanisms that make UDP more desirable in some cases.

Specifically UDP offers less delays and better control over sending data
because with UDP we have:

1. 

   #+BEGIN_QUOTE
     /No congestion control or similar mechanisms./ With UDP, as soon as
     the application passes data to the transport layer, then UDP
     encapsulates it and sends it over to the network layer. In contrast
     TCP “intervenes” a lot with sending the data e.g. with the
     congestion control mechanism or the retransmissions in case an ACK
     is not received. These TCP mechanisms cause further delays.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /No connection management overhead./ With UDP we have no connection
     establishment and no need to keep track of connection state (eg
     with buffers). Both mean even less delays.
   #+END_QUOTE

So with some real time applications that are sensitive to delays UDP is
a better option, despite possibly higher losses. Eg DNS is using UDP.
Which other applications prefer UDP over TCP? The table below gives us
an idea:

[[file:media/image103.png]][[file:media/image77.png]]

The UDP packet structure: UDP has a 64 bits header consisting of the
following fields:

1. 

   #+BEGIN_QUOTE
     Source and destination ports.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Length of the UDP segment (header and data).
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Checksum (an error checking mechanism). Since there is no guarantee
     for link-by-link reliability, we need a basis mechanism in place
     for error checking. The UDP sender adds the src port, the dest port
     and the packet length. Then it takes the sum and performs an 1s
     complement (all 0s are turned to 1 and all 1s are turned to 0s). If
     during the sum there is an overflow, its wrapped around. The
     receiver adds all the four 16-bit words (including the checksum).
     The result should be all 1s unless an error has occurred.
   #+END_QUOTE

*The TCP Three-Way Handshake*

The TCP Three way Handshake:

Step 1: The TCP client sends a special segment, (containing no data) and
with SYN bit set to 1. The Client also generates an initial sequence
number (client_isn) and includes it in this special TCP SYN segment.

Step 2: The Server upon receiving this packet, allocates the required
resources for the connection and sends back the special
‘connection-granted' segment which we call SYNACK. This packet has SYN
bit set to 1, ack field containing (client_isn+1) value and a randomly
chosen initial sequence number in the sequence number field.

Step 3: When the client receives the SYNACK segment, it also allocates
buffer and resources for the connection and sends an acknowledgment with
SYN bit set to 0.

[[file:media/image43.png]][[file:media/image68.png]]

/Connection Teardown:/

Step 1: When client wants to end the connection, it sends a segment with
FIN bit set to 1 to the server.

Step 2: Server acknowledges that it has received the connection closing
request and is now working on closing the connection.

Step 3: The Server then sends a segment with FIN bit set to 1,
indicating that connection is closed.

Step 4: The Client sends an ACK for it to the server. It also waits for
sometime to resend this acknowledgment in case the first ACK segment is
lost.

*Reliable Transmission*

/What is reliable transmission?/ Recall that the network layer is
unreliable and it may lead to packets getting lost or arriving out of
order. This can clearly be an issue for a lot of applications. For
example, a file downloaded over the Internet might be corrupted if some
of the packets were lost during the transfer.

One option here is to allow the application developers take care of the
network losses as is done in UDP. However, given that reliability is an
important primitive desirable for a lot of applications, TCP developers
decided to implement this primitive in the transport layer. /Thus, TCP
guarantees an in-order delivery of the application-layer data without
any loss or corruption./

Now, let us look at how TCP implements reliability.

In order to have a reliable communication, the sender should be able to
know which segments were received by the remote host and which were
lost. Now, how can we achieve this? One way to do this is by having the
receiver send acknowledgements indicating that it has successfully
received the specific segment. In case the sender does not receive an
acknowledgement within a given period of time, the sender can assume the
packet is lost and resend it. This method of using acknowledgements and
timeouts is also known as /Automatic Repeat Request or ARQ/.

There are various methods in which it can be implemented:

The simplest way would be for the sender to send a packet and wait for
its acknowledgement from the receiver. This is known as /Stop and Wait
ARQ/. Note that the algorithm typically needs to figure out the waiting
time after which it resends the packet and this estimation can be
tricky. A small value of timeout can lead to unnecessary
re-transmissions and a large value can lead to unnecessary delays. In
most cases, the timeout value is a function of the estimated round trip
time of the connection.

Clearly, this kind of alternate sending and waiting for acknowledgement
has a very low performance. In order to solve this problem, the sender
can send multiple packets without waiting for acknowledgements. More
specifically, the sender is allowed to send at most N unacknowledged
packets typically referred to as the window size. As it receives
acknowledgement from the receiver, it is allowed to send more packets
based on the window size. In implementing this, we need to take care of
the following concerns:

1. 

   #+BEGIN_QUOTE
     The receiver needs to be able to identify and notify the sender of
     a missing packet. Thus, each packet is tagged with a unique byte
     sequence number which is increased for subsequent packets in the
     flow based on the size of the packet.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Also, now both sender and receiver would need to buffer more than
     one packet. For instance, the sender would need to buffer packets
     that have been transmitted but not acknowledged. Similarly, the
     receiver may need to buffer the packets because the rate of
     consuming these packets (say writing to a disk) is slower than the
     rate at which packets arrive.
   #+END_QUOTE

Now let's look at how does the receiver notify the sender of a missing
segment.

One way is for the receiver to send an ACK for the most recently
received in-order packet. The sender would then send all packets from
the most recently received in-order packet, even if some of them had
been sent before. The receiver can simply discard any out-of-order
received packets. This is called Go-back-N. For instance, in the figure
below if packet 7 was lost in the network, the receiver will discard any
subsequent packets. The sender will send all the packets starting from 7
again.

[[file:media/image61.png]][[file:media/image1.png]]

Clearly, in the above case, a single packet error can cause a lot of
unnecessary retransmissions. To solve this, TCP uses /selective ACK/ing.
In this, the sender retransmits only those packets that it suspects were
received in error. The receiver in this case would acknowledge a
correctly received packet even if it is not in order. The out-of-order
packets are buffered until any missing packets have been received at
which point the batch of the packets can be delivered to the application
layer.

Note that even in this case, TCP would need to use a timeout as there is
a possibility of ACKs getting lost in the network.

In addition to using timeout to detect loss of packets, TCP also uses
duplicate acknowledgements as a means to detect loss. A duplicate ACK is
additional acknowledgement of a segment for which the sender has already
received acknowledgement earlier. When the sender receives 3 duplicate
ACKs for a packet, it considers the packet to be lost and will
retransmit it instead of waiting for the timeout. This is known as fast
retransmit. For example, in the figure below, once sender receives 3
duplicate ACKs, it will retransmit packet 7 without waiting for timeout.

*Transmission Control*

In this topic we will learn about the mechanisms provided in the
transport-layer to control the transmission rate.

/Why control the transmission rate?/ We will first illustrate why we
need to know and adapt the transmission rate. Consider a scenario when
user A needs to send 1 Gb of file to a remote host B on a 100 Mbps link.
What rate should it send the file? One could say that it should be 100
Mbps. But how does user A determine that given it does not know the link
capacity. Also, what about other users that also would be using the same
link? What happens to the sending rate if the receiver B is also
receiving files from a lot of other users? Finally, which layer in the
network decides the data transmission rate? In this section, we will try
to answer all these questions.

/Where should the transmission control function reside in the network
stack?/ One option is to let the application developers figure out and
implement mechanisms for transmission control. This is what UDP does.
However, it turns out that transmission control is a fundamental
function for most of the applications. Thus it will be easier if it is
implemented in the transport layer. Moreover, it also has to deal with
issues of fairness in using the network as we will see later, thus
making it more convenient to handle it at the transport layer. Thus, TCP
provides mechanisms for transmission control which have been a subject
of interest to network researchers since the inception of computer
networking. We will look at these in detail now.

*Flow Control*

Flow control: Controlling the transmission rate to protect the
receiver's buffer

The first case where we need transmission control is to protect the
buffer of the receiver from overflowing. Recall that TCP uses a buffer
at the receiver end to buffer packets that have not been transmitted to
the application. It could happen that the receiver is involved with
multiple processes and does not read the data instantly. This can cause
accumulation of huge amount of data and overflow the receive buffer.

TCP provides a rate control mechanism also known as flow control that
helps match the sender's rate against the receiver's rate of reading the
data. Sender maintains a variable ‘receive window'. It provides sender
an idea of how much data the receiver can handle at the moment.

We will illustrate its working using an example. Consider two hosts, A
and B, that are communicating with each other over a TCP connection.
Host A wants to send a file to Host B. For this, Host B allocates a
receive buffer of size /RcvBuffer/ to this connection. The receiving
host maintains two variables, /LastByteRead/ (number of byte that was
last read from the buffer ) and /LastByteRcvd/ (last byte number that
has arrived from sender and placed in the buffer). Thus, in order to not
overflow the buffer, TCP needs to make sure that

- 

  #+BEGIN_QUOTE
    LastByteRcvd - LastByteRead <= RcvBuffer
  #+END_QUOTE

The extra space that the receive buffer has, is specified using a
parameter, termed as receive window.

- 

  #+BEGIN_QUOTE
    rwnd = RcvBuffer - [LastByteRcvd - LastByteRead]
  #+END_QUOTE

[[file:media/image39.png]]

The receiver advertises this value of rwnd in every segment/ACK it sends
back to the sender.

The sender also keeps track of two variables, LastByteSent and
LastByteAcked.

- 

  #+BEGIN_QUOTE
    UnAcked Data Sent = LastByteSent - LastByteAcked
  #+END_QUOTE

To not overflow the receiver's buffer, the sender needs to make sure
that the maximum number of unacknowledged bytes it sends are no more
than the rwnd.

Thus we need:

- 

  #+BEGIN_QUOTE
    LastByteSent -- LastByteAcked <= rwnd
  #+END_QUOTE

/Caveat:/ However, there is one scenario where this scheme has a
problem. Consider a scenario, if the receiver had informed the sender
that rwnd = 0, and thus the sender stops sending data. Also, assume that
B has nothing to send to A. Now, as the application processes the data
at the receiver, the receiver buffer is cleared but the sender may never
know that new buffer space is now available and will be blocked from
sending data even when receiver buffer is empty.

TCP resolves this problem by making sender continue sending segments of
size 1 byte even after when rwnd = 0. When the receiver acknowledges
these segments, it will specify the rwnd value and the sender will know
as soon as the receiver has some room in the buffer.

*Congestion Control Introduction*

/Congestion control: Controlling the transmission rate to protect the
network from congestion/

The second and very important reason for transmission control is to
avoid congestion in the network.

Let us look at an example to understand this. Consider a set of senders
and receivers sharing a single link with capacity C. Assume, other links
have capacity > C. How fast should each sender transmit data? Clearly,
we do not want the combined transmission rate to be higher than the
capacity of the link as it can cause issues in the network such as
longer queues, packet drops etc. Thus, we want a mechanism to control
the transmission rate at the sender in order to avoid congestion in the
network. This is known as congestion control.

It is important to note that networks are quite dynamic with users
joining and leaving the network, initiating data transmission and
terminating existing flows. Thus the mechanisms for congestion control
need to be dynamic enough to adapt to these changing network conditions.

*What are the goals of congestion control?*

Let us consider some of the desirable properties of a good congestion
control algorithm:

- 

  #+BEGIN_QUOTE
    /Efficiency./ We should get high throughput or utilization of the
    network should be high.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Fairness./ Each user should its fair share of the network
    bandwidth. The notion of fairness is dependent on the network
    policy. For this context, we will assume that every flow under the
    same bottleneck link should get equal bandwidth.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Low delay./ In theory, it is possible to design protocols that have
    consistently high throughput assuming infinite buffer. Essentially,
    we could just keep sending the packets to the network and they will
    get stored in the buffer and will eventually get delivered. However,
    it will lead to long queues in the network leading to delays. Thus,
    applications that are sensitive to network delays such as video
    conferencing will suffer. Thus, we want the network delays to be
    small.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Fast convergence./ The idea here is that a flow should be able to
    converge to its fair allocation fast. This is important as a typical
    network's workload is composed a lot of short flows and few long
    flows. If the convergence to fair share is not fast enough, the
    network will still be unfair for these short flows.
  #+END_QUOTE

*Congestion control flavors: E2E vs Network-assisted*

Broadly speaking, there can be two approaches to implement congestion
control:

/The first approach is network-assisted congestion control./ In this we
rely on the network layer to provide explicit feedback to the sender
about congestion in the network. For instance, routers could use ICMP
source quench to notify the source that the network is congested.
However, under severe congestion, even the ICMP packets could be lost,
rendering the network feedback ineffective.

/The second approach is to implement end-to-end congestion control./ As
opposed to the previous approach, the network here does not provide any
explicit feedback about congestion to the end hosts. Instead, the hosts
infer congestion from the network behavior and adapt the transmission
rate.

Eventually, TCP ended up using the end-to-end approach. This largely
aligns with the end-to-end principle adopted in the design of the
networks. Congestion control is a primitive provided in the transport
layer, whereas routers operate at the network layer. Therefore, the
feature resides in the end nodes with no support from the network. Note
that this is no longer true as certain routers in the modern networks
can provide explicit feedback to the end-host by using protocols such as
ECN and QCN.

Let us now look at how TCP can infer congestion from the behavior of the
network.

*How a host infers congestion? Signs of congestion*

There are mainly two signals of congestion.

/First is the packet delay./ As the network gets congested, the queues
in the router buffers build up. This leads to increased packet delays.
Thus, an increase in the round trip time, which can be estimated based
on ACKs, can be an indicator of congestion in the network. However, it
turns out that packet delay in a network tend to be variable, making
delay-based congestion inference quite tricky.

/Another signal for congestion is packet loss./ As the network gets
congested, routers start dropping packets. Note that packets can also be
lost due to other reasons such as routing errors, hardware failure, TTL
expiry, error in the links, or flow control problems, although it is
rare.

The earliest implementation of TCP ended up using loss as a signal for
congestion. This is mainly because TCP was already detecting and
handling packet losses to provide reliability.

*How does a TCP sender limit the sending rate?*

The idea of TCP congestion control was introduced so that each source
can determine the network's available capacity and know how many packets
it can send without adding to the network's level of congestion. Each
source uses ACKs as a pacing mechanism. Each source uses the ACK to
determine if the packet released earlier to the network was received by
the receiving host and it is now safe to release more packets into the
network.

TCP uses a congestion window which is similar to the receive window used
for flow control. It represents the maximum number of unacknowledged
data that a sending host can have in transit (sent but not yet
acknowledged).

TCP uses a probe-and-adapt approach in adapting the congestion window.
Under regular conditions, TCP increases the congestion window trying to
achieve the available throughput. Once it detects congestion then the
congestion window is decreased.

In the end, the number of unacknowledged data that a sender can have is
the minimum of the congestion window and the receive window.

- 

  #+BEGIN_QUOTE
    LastByteSent -- LastByteAcked <= min{cwnd, rwnd}
  #+END_QUOTE

In a nutshell, a TCP sender cannot send faster than the slowest
component, which is either the network or the receiving host.

*Congestion control at TCP - AIMD*

TCP decreases the window when the level of congestion goes up, and it
increases the window when the level of congestion goes down. We refer to
this combined mechanism as additive increase/multiplicative decrease
(AIMD).

/Additive Increase:/

The connection starts with a constant initial window, typically 2 and
increases it additively. The idea behind additive increase is to
increase the window by one packet every RTT (Round Trip Time). So, in
the additive increase part of the AIMD, every time the sending host
successfully sends a cwnd number of packets it adds 1 packet to cwnd.

Also, in practice, this increase in AIMD happens incrementally. TCP
doesn't wait for ACKs of all the packets from the previous RTT. Instead,
it increases the congestion window size as soon as each ACK arrives. In
bytes, this increment is a portion of the MSS (Maximum Segment Size).

- 

  #+BEGIN_QUOTE
    Increment = MSS × (MSS / CongestionWindow)
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    CongestionWindow + = Increment
  #+END_QUOTE

[[file:media/image53.png]]

/Multiplicative Decrease:/

Once TCP Reno detects congestion, it reduces the rate at which the
sender transmits. So, when the TCP sender detects that a timeout
occurred, then it sets the CongestionWindow (cwnd) to half of its
previous value. This decrease of the cwnd for each timeout corresponds
to the “multiplicative decrease” part of AIMD. For example, suppose the
cwnd is currently set to 16 packets. If a loss is detected, then cwnd is
set to 8. Further losses would result to the cwnd to be reduced to 4 and
then to 2 and then to 1. The value of cwnd cannot be reduce further than
1 packet.

Figure below shows an example of how the congestion control window
decreases when congestion is detected:

[[file:media/image120.png]][[file:media/image35.png]]

/Signals of congestion:/

TCP Reno uses two types of packet loss detection as a signal of
congestion. First is the triple duplicate ACKs and is considered to be
mild congestion. In this case, the congestion window is reduced to half
of the original congestion window.

The second kind of congestion detection is timeout i.e. when no ACK is
received within a specified amount of time. It is considered a more
severe form of congestion, and the congestion window is reset to the
Initial Window.

/Congestion window sawtooth pattern:/

TCP continually decreases and increases the congestion window throughout
the lifetime of the connection. If we plot the cwnd with respect to
time, we observe that it follows a sawtooth pattern as shown in the
figure.

*Slow start in TCP*

The AIMD approach we saw in the previous topic is useful when the
sending host is operating very close to the network capacity. AIMD
approach reduces the congestion window at a much faster rate than it
increases the congestion window. The main reason for this approach is
that the consequences of having too large a window are much worse than
those of it being too small. For example, when the window is too large,
more packets will be dropped and retransmitted, making network
congestion even worse; thus, it is important to reduce the number of
packets being sent into the network as quickly as possible.

In contrast, when we have a new connection that starts from cold start,
it can take much longer for the sending host to increase the congestion
window by using AIMD. So for a new connection, we need a mechanism which
can rapidly increase the congestion window from a cold start.

To handle this, TCP Reno has a /slow start phase/ where the congestion
window is increased exponentially instead of linearly as in the case of
AIMD. The source host starts by setting cwnd to 1 packet. When it
receives the ACK for this packet, it adds 1 to the current cwnd and
sends 2 packets. Now when it receives the ACK for these two packets, it
adds 1 to cwnd for each of the ACK it receives and sends 4 packets. Once
the congestion window becomes more than a threshold, often referred to
as /slow start threshold/, it starts using AIMD.

The figure below shows the sending host during slow start and an example
of the slow start phase.

[[file:media/image63.png]][[file:media/image74.png]]

Slow start is called “slow” start despite using an exponential increase
because in the beginning it sends only one packet and starts doubling it
after each RTT. Thus, it is slower than starting with a large window.

Finally, we note that there is one more scenario, where slow start kicks
in. When a connection dies while waiting for a timeout to occur. This
happens when the source has sent enough data as allowed by the flow
control mechanism of TCP and times out while waiting for the ACK which
will not arrive. Thus, the source will eventually receive a cumulative
ACK that will reopen the connection and then instead of sending the
available window size worth of packets at once, it will use slow start
mechanism.

In this case, the source will have a fair idea about the congestion
window from the last time it had a packet loss. It will now use this
information as the “target” value to avoid packet loss in future. This
target value is stored in a temporary variable “CongestionThreshold”.
Now, source performs slow start by doubling the number of packets after
each RTT until cwnd value reaches the congestion threshold (a knee
point). After this point, it increases the window by 1 (additive
increase) each RTT until it experiences packet loss (cliff point). After
which it multiplicatively decreases the window.

*TCP Fairness*

Recall that we defined fairness as one of the desirable goals of
congestion control. Note that fairness in this case means that for
k-connections passing through one common link with capacity R bps, each
connection gets an average throughput of R/k.

Let us understand if TCP is fair.

Consider a simple scenario where two TCP connections share a single link
with bandwidth R. For simplicity, we assume that both connections have
same RTT and there are only TCP segments passing through the link. If we
plot a graph for throughput of these two connections, then the
throughput for each should sum up to R. So, the goal is to get
throughput achieved for each link fall somewhere near the intersection
of the equal bandwidth share line and the full bandwidth utilization
line, as shown in below graph:

[[file:media/image101.png]]

At point A in the above graph, total utilized bandwidth is less than R,
so no loss can occur at this point. Therefore, both the connection will
increase their window size, thus the sum of the utilized bandwidth will
grow and graph will move towards B.

At point B, as the total transmission rate is more than R, both
connection may start having packet loss. Now they will decrease their
window size to half and come back to point C.

At point C, again the total throughput is less than R, so both
connection will increase their window size to move towards point D and
will again experience packet loss at D, and so on.

Thus, using AIMD leads to fairness in bandwidth sharing.

*Caution about fairness*

There can be cases when TCP is not fair.

One such case arises due to the difference in the RTT of different TCP
connections. Recall that TCP Reno uses ACK-based adaptation of the
congestion window. Thus, connections with smaller RTT values would
increase their congestion window faster than the ones with longer RTT
values. This leads to an unequal sharing of the bandwidth.

Another case of unfairness arises if a single application uses multiple
parallel TCP connections. Consider, for example, nine applications using
one TCP connection sharing a link of rate R. If a new application
establishes connection on the same link and also uses one TCP
connection, then each application gets fairly the same transmission rate
of R/10. But if the new application had 11 parallel TCP connections,
then it would get an unfair allocation of more than R/2.

*Congestion Control in Modern Network Environments: TCP CUBIC*

Over the years, networks have improved with link speeds increasing
tremendously. This has called for changes in the TCP congestion control
mechanisms mainly with a desire to improve link utilization.

We can see that TCP Reno has low network utilization, especially when
the network bandwidth is high or the delay is large. Such networks are
also known as high bandwidth delay product networks.

To make TCP more efficient under such networks, many improvements to TCP
congestion control have been proposed. Now we will look at one such
version, called TCP CUBIC, which was also implemented in the Linux
kernel. It uses a CUBIC polynomial as the growth function.

[[file:media/image8.png]]

Let us see what happens when TCP experiences a triple duplicate ACK, say
at window=Wmax. This could be because of congestion in the network. To
maintain TCP-fairness, it uses a multiplicative decrease and reduces the
window to half. Let us call this Wmin.

Now, we know that the optimal window size would be in between Wmin and
Wmax and closer to Wmax. So, instead of increasing the window size by 1,
it is okay to increase the window size aggressively in the beginning.
Once the W approaches closer to Wmax, it is wise to increase it slowly
because that is where we detected a packet loss last time. Assuming no
loss is detected this time around Wmax, we keep on increasing the window
a little bit. If there is no loss still, it could be that the previous
loss was due to a transient congestion or non-congestion related event.
Therefore, it is okay to increase the window size with higher values
now.

This window growth idea is approximated in TCP CUBIC using a cubic
function. Here is the exact function it uses for the window growth:

#+BEGIN_QUOTE
  W(t) = C(t-K)3 + Wmax
#+END_QUOTE

Here, Wmax is the window when the packet loss was detected. Here C is a
scaling constant, and K is the time period that the above function takes
to increase W to Wmax when there is no further loss event and is
calculated by using the following equation: K =
$\sqrt[3]{\frac{\text{Wmax\ }\beta}{C}}$

It is important to note that time here is the time elapsed since the
last loss event instead of the usual ACK-based timer used in TCP Reno.
This also makes TCP CUBIC RTT-fair.

*The TCP Protocol: TCP Throughput*

In a previous topic, we saw that the congestion window follows a
sawtooth pattern. As shown in this Figure. The congestion window is
increased by 1 packet every RTT, until it reaches the maximum value W,
at which point a loss is detected and the cwnd is cut in half, W/2.

Given this behavior, we want to have a simple model that predicts the
throughput for a TCP connection.

To make our model more realistic, let's also assume that we have p = the
probability loss. So, we assume that the network delivers 1 out of every
p consecutive packets followed by a single packet loss.

[[file:media/image27.png]]

Because the congestion window (cwnd) size increases a constant rate of 1
packet for every RTT, the height of the sawtooth is W/2 and the width of
the base is W/2, which corresponds to W/2 round trips, or *RTT* W/2.*

The number of packets sent in one cycle the area under the sawtooth.
Therefore, the total number of packets sent:

#+BEGIN_QUOTE
  (W/2)^{2} + 1/2*(W/2)^{2} = 3/8*W^{2}
#+END_QUOTE

As stated in our assumptions about out lossy network, it delivers 1/p
packets followed by a loss. So:

#+BEGIN_QUOTE
  1/p = (w/2)^{2} + 1/2*(w/2)^{2} = 3/8*$W^{2}$, solving for
  W=$\sqrt{\frac{8}{3p}}$
#+END_QUOTE

The rate that data that is transmitted is computed as:

#+BEGIN_QUOTE
  BW = data per cycle / time per cycle
#+END_QUOTE

Substituting from above:
$\frac{\text{data\ per\ cycle}}{\text{time\ per\ cycle}}$ =
$\frac{MSS\ *\ \frac{3}{8}W^{2}}{RTT\ *\ \frac{W}{2}}$ =
$\frac{\ \frac{\text{MSS}}{P}}{RTT\ *\ }$

We can collect all of our constants into C = $$ , compute the
throughput: BW = $\frac{\text{MSS}}{\text{RTT}}$ * $\frac{C}{}$

In practice, because of additional parameters, such as small receiver
windows, extra bandwidth availability, and TCP timeouts, our constant
term C is usually less than 1. This means that bandwidth is bounded by:
BW < $\frac{\text{MSS}}{\text{RTT}}$ * $\frac{1}{}$

*Optional Reading: Datacenter TCP*

At this point, it is important to know that a lot of research has been
going in optimizing the congestion control mechanisms. These
optimizations are called for because of evolution of the networks. We
looked at TCP CUBIC as one such example for high bandwidth delay product
networks.

Similarly, data center (DC) networks are other kinds of networks where
new TCP congestion control algorithms have been proposed and
implemented. There are mainly two differences that have led to this --
the flow characteristics of DC networks are different from the public
Internet. There are many short flows that are sensitive to delay. Thus,
the congestion control mechanisms are optimized for both delay and
throughput and not just the latter alone. Another reason is that DC
networks are often owned by a private entity making any changes in the
transport layer easier as the new algorithms need not co-exists with the
older ones.

DCTCP and TIMELY are two popular examples of TCP designed for DC
environments. DCTCP is based on a hybrid approach of using both implicit
feedback aka packet loss and explicit feedback from the network using
ECN for congestion control. TIMELY uses the gradient of RTT to adjust
its window.

*Quiz 1*

As we have seen, UDP and TCP use port numbers to identify the sending
application and destination application. Why don't UDP and TCP just use
process IDs rather than define port numbers?

/Answer: Process IDs are specific to operating systems and therefore
using process IDs rather than a specially defined port would make the
protocol operating system dependent. Also, a single process can set up
multiple channels of communications and so using the process ID as the
destination identifier wouldn't be able to properly demultiplex,
Finally, having processes listen on well-known ports (like 80 for http)
is an important convention./

*Quiz 2*

UDP and TCP use 1's complement for their checksums. But why is it that
UDP takes the 1's complement of the sum -- why not just use the sum?
Exploring this further, using 1's complement, how does the receiver
compute and detect errors? Using 1's complement, is it possible that a
1-bit error will go undetected? What about a 2-bit error?

/Answer: To detect errors, the receiver adds the four words (the three
original words and the checksum). If the sum contains a zero, the
receiver knows there has been an error. While all one-bit errors will be
detected, but two-bit errors can be undetected (e.g., if the last digit
of the first word is converted to a 0 and the last digit of the second
word is converted to a 1)./

*Quiz 3*

TCP utilizes the Additive Increase Multiplicative Decrease (AIMD) policy
for fairness. Consider other possible policies for fairness in
congestion control would be Additive Increase Additive Decrease (AIAD),
Multiplicative Increase Additive Decrease (MIAD), and Multiplicative
Increase Multiplicative Decrease (MIMD).

/Answer: In AIAD and MIMD, the hosts communicating over the network will
oscillate along the efficiency line, but will not converge as was shown
for AIMD. MIAD will converge just like AIMD. But none of the alternative
policies are as stable. The decrease policy in AIAD and MIAD is not as
aggressive by comparison to AIMD and won't address congestion control as
effectively. The increase policy in MIAD and MIMD is too aggressive./

*Quiz 4*

Explain how in TCP Cubic the congestion window growth becomes
independent of RTTs.

/Answer: The key feature of CUBIC is that its window growth depends only
on the time between two consecutive congestion events. One congestion
event is the time when TCP undergoes fast recovery. This feature allows
CUBIC flows competing in the same bottleneck to have approxi- mately the
same window size independent of their RTTs, achieving good
RTT-fairness./

** Lesson 3: Intradomain Routing
   :PROPERTIES:
   :CUSTOM_ID: lesson-3-intradomain-routing
   :END:

*Introduction*

In this lecture we are focusing on the network layer and on a specific
function of the network layer which is routing within a single
administrative domain.

Let's remember that our overarching theme of the course, is to
understand what it takes for two hosts to exchange data. In this
lecture, we will zoom into the protocols that we need so that data can
travel over a good path from the source to the destination within a
single administrative domain. We'll learn about the two flavors of
intradomain routing algorithms which are the link-state and
distance-vector algorithms. We'll look at example protocols such as RIP
and OSPF. We will also look at challenges that intradomain routing
protocols face such as convergence delay. Finally, we will look at how
routing protocols are used for purposes that go beyond determining a
good path; how we can use routing for traffic engineering purposes so
that we can steer traffic through the network avoiding congested links
for example.

*Routing Algorithms*

Let's assume that we have two hosts that have established a connection
between them using TCP or UDP as we saw in the previous lecture.

Each of the two hosts know the default router (or first-hop router as we
say). When a host sends a packet, the packet is first transferred to
that default router. But what happens after that? In this lecture we
will see the algorithms that we need so that when a packet leaves the
default router of the sending host will travel over a path towards the
default router of the destination host.

A packet will be able to travel from the sending host to the destination
host with the help of intermediate routers. When a packet arrives at a
router, the router is responsible to consult a forwarding table and then
to determine the outgoing link interface to forward the packet. So by
/forwarding/ we refer to transferring a packet from an incoming link to
an outgoing link within a single router. We will talk about forwarding
in the following lecture.

By /routing/ we refer to how routers work together using routing
protocols to determine the good paths (or good routes as we call them)
over which the packets travel from the source to the destination node.

When we have routers that belong to the same administrative domain we
refer to the routing that takes place as intradomain routing.

But when the routers belong to different administrative domains, we
refer to interdomain routing. In this lecture we focus on /intradomain
routing algorithms/ or Interior Gateway Protocols (IGPs).

The two major classes of algorithms that we have are: A) link-state and
B) distance-vector algorithms. For the following algorithms we represent
each router as a node, and a link between two routers as an edge. Each
edge is associated with a cost.

*Linkstate Routing Algorithm*

In this topic, we will talk about the link state routing protocols, and
more specifically about the Dijkstra's algorithm.

In the linkstate routing protocol, the link costs and the network
topology are known to all nodes (for example by broadcasting these
values).

Let's introduce some basic terminology. By u we represent our source
node. By v we present every other node in the network. By D(v) we
represent the cost of the current least cost path from u to v. By p(v)
we present the previous node along the current least cost path from u to
v. By N' we represent the subset of nodes along the current least-cost
path from u to v.

Initialization step. We note that the algorithm starts with an
initialization step, where we initialize all the currently known
least-cost paths from u to its directly attached neighbors. We know
these costs because they are the costs of the immediate links. For nodes
in the network that they are not directly attached to u, we initialize
the cost path as infinity. We also initialize the set N' to include only
the source node u.

[[file:media/image13.jpg]]

/Iterations./ After the initialization step, the algorithm follows with
a loop that is executed for every destination node v in the network. At
each iteration, we look at the set of nodes that are not included in N',
and we identify the node (say w) with the least cost path from the
previous iteration. We add that node w into N'. For every neighbor v of
w, we update D(v) with the new cost which is either the old cost from u
to v (from the previous iteration) or the known least path cost from
source node u to w, plus the cost from w to v, whichever between the two
quantities is the minimum. The algorithm exits by returning the shortest
paths, and their costs, from the source node u to every other node v in
the network.

*Linkstate Routing Algorithm - Example*

Let's look at an example of the linkstate routing algorithm. We have the
graph below and we consider our source node to be u. Our goal is to
compute the least-cost paths from u to all nodes v in the network.

[[file:media/image28.png]][[file:media/image114.png]]

We start with the initialization step, where we set all the currently
known least-cost paths from u to it's directly attached neighbors v, x
and w. For the rest of the nodes in the network we set the cost to
infinity, because they are not immediate neighbors to source node u. We
also initialize the set N' to include only the source node u. The first
row in our table represents the initialization step.

In the first iteration, we look among the nodes that are not yet in N',
and we select the node with the least cost from the previous iteration.
In this case, this is node x. Then we update D for all the immediate
neighbors of x, which in this case are nodes v and w. For example, we
update D(w) as the minimum between: the cost we had from the previous
iteration which is 5, and the cost from u to x (1) plus cost from x to w
(3). The minimum between the two is 4. We update the second row in our
table.

We continue in a similar manner for the rest of the nodes in the table.
The algorithm exits in the 5th iteration (error: The bottom right cell
of the table should be empty, rather contain 4,y )

*Linkstate Routing Algorithm - Computational Complexity*

What is the computational complexity of the linkstate routing algorithm?

In other words, in the worst case, how many computations are needed to
find the least-cost paths from the source to all destinations in the
network? In the first iteration we need to search through all nodes to
find the node with the minimum path cost. But as we proceed in the next
iterations, this number decreases. So in the second iteration we search
through (n-1) nodes. This decrease continues at every step. So by the
end of the algorithm, after we go through all the iterations, we will
need to search through n(n+1)/2 nodes. Thus the complexity of the
algorithm is in the order of n squared O(n^2).

*Distance Vector Routing*

In this section, we will talk about the distance vector routing
algorithm.

The DV routing algorithm is iterative (the algorithm iterates until the
neighbors do not have new updates to send to each other), asynchronous
(the algorithm does not require the nodes to be synchronized with each
other), and distributed (direct nodes send information to one another,
and then they resend their results back after performing their own
calculations, so the calculations are not happening in a centralized
manner).

The DV algorithm is based on the Bellman Ford Algorithm. Each node
maintains its own distance vector, with the costs to reach every other
node in the network. Then, from time to time, each node sends its own
distance vector to its neighbor nodes. The neighbor nodes in turn,
receive that distance vector and they use it to update their own
distance vectors. In other words, the neighboring nodes exchange their
distance vectors to update their own view of the network.

How the vector update is happening? Each node x updates its own distance
vector using the Bellman Ford equation: Dx(y) = minv{c(x,v) + Dv(y)} for
each destination node y in the network. A node x, computes the least
cost to reach destination node y, by considering the options that it has
to reach y through each of its neighbor v. So node x considers the cost
to reach neighbor v, and then it adds the least cost from that neighbor
v to the final destination y. It calculates that quantity over all
neighbors v and it takes the minimum. Formally, the DV algorithm is as
follows:

[[file:media/image20.png]][[file:media/image32.png]]

*Distance Vector Routing Example*

Now, let's see an example of the distance vector routing algorithm.
Let's consider the three node network shown here:

[[file:media/image81.png]]

In the first iteration, each node has its own view of the network, which
is represented by an individual table. Every row in the table is the
distance vector of each node. Node x has it's own table, and the same is
true for nodes y and z. We note that in the first iteration, node x does
not have any information about the y's and z's distance vectors, thus
these values are set to infinity.

In the second iteration, the nodes exchange their distance vectors and
they update their individual views of the network.

Node x computes its new distance vector, using the Bellman Ford equation
for every destination node y and z. For each destination, node x
compares the cost to reach that destination through a neighbor node.

dx(y) = min{c(x,y) + dy(y), c(x,z)+dz(y) } = min{2+0, 7+1} = 2

dx(z) = min{c(x,y) + dy(z), c(x,z)+dz(z) } = min{2+1, 7+0} = 3

At the same time, node x receives the distance vectors from y and z from
the first iteration. So it updates its table to reflect its view of the
network accordingly.

Nodes y and z repeat the same steps to update their own tables.

[[file:media/image130.png]][[file:media/image137.png]]

In the third iteration, the nodes get the distance vectors from the
previous iteration (if they have changed), and they repeat the same
calculations. Finally, each node has its own routing table.

Finally, at this point, there are no further updates send from the
nodes, thus the nodes are not doing any further calculations on their
distance vectors. The nodes enter a waiting mode, until there is a
change in the link costs.

*Link Cost Changes and Failures in DV - Count to Infinity Problem*

Now, we will see what is happening when a node identifies that a link
that it is connecting it to one of its neighbors as changed.

Let's consider the following example topology below:

[[file:media/image70.png]][[file:media/image66.png]]

Let's assume that the link cost between x-y changes to 1.

1. 

   #+BEGIN_QUOTE
     At time t0, y detects that cost to x has changed from 4 to 1, so it
     updates its distance vector and sends it to its neighbors.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     At time t1, z receives the update from y. Now it thinks that is can
     reach x through y with a cost of 2. And it sends its new distance
     vector to its neighbors.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     At time t2, y receives update from z. Y distance vector does not
     change its distance vector and does not send updates.
   #+END_QUOTE

In this scenario, we note that the fact that there was a decrease in the
link cost, it propagated quickly among the nodes, as it only took a few
iterations.

Unfortunately, this is not always the case. Let's consider the following
scenario where a link cost increases by a large amount.

Let's assume that the link y-x has a new cost of 60.

1. 

   #+BEGIN_QUOTE
     At t0 y detects that cost has changed, now it will update its
     distance vector thinking that it can still reach x through z with a
     total cost of 5+1=6
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     At t1, we have a routing loop, where z thinks it can reach x
     through y and y thinks it can reach x through z. This will be
     causing the packets to be bouncing back and forth between y and z
     until their tables change.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     z and y keep updating each other about their new cost to reach x.
     For example, y computes its new cost to be 6, it informs z. Then z
     computes its new cost to be 7, and it informs y, and so on.
   #+END_QUOTE

This back and forth continues for a total of 44 iterations, at which
point z computes its cost to be larger than 50, and that point it will
prefer to reach x directly rather than through y.

In contrast to the previous scenario, this link cost change took a long
time to propagate among the nodes of the network. This is known as the
count-to-infinity problem.

*Poison Reverse*

[[file:media/image123.jpg]]

A solution to the previous problem is the following idea, called poison
reverse: since z reaches x through y, z will advertise to y that the
distance to x is infinity (Dz(x)=infinity). However z knows that this is
not true and Dz(x)=5. z tells this lie to y, as long as it knows that it
can reach to x via y. Since y assumes that z has no path to x except via
y, it will never send packets to x via z.

So z poisons the path from z to y.

Things change when the cost from x to y changes to 60. y will update its
table and send packet to x directly with cost Dy(x)=60. It will inform z
about its new cost to x, after this update is received. Then z will
immediately shift its route to x to be via the direct (z,x) link at cost
50. Since there is a new path to x, z will inform y that Dz(x)=50.

When y receives this update from z, y will update Dy(x)=51=c(y,z)+Dz(x).

Since z is now on least cost path of y to reach x, y poisons the reverse
path from z to x by. Y tells z that Dy(x)=inf, even though y knows that
Dy(x)=51.

This technique will solve the problem with 2 nodes, however poisoned
reverse will not solve a general count to infinity problem involving 3
or more nodes that are not directly connected.

*Distance Vector Routing Protocol Example: RIP*

The /Routing Information Protocol (RIP)/ is based on the Distance Vector
protocol.

The first version, released as a part of the BSD version of Unix, uses
hop count as a metric (i.e. assumes link cost as 1). The metric for
choosing a path could be shortest distance, lowest cost or a
load-balanced path. In RIP, routing updates are exchanged between
neighbors periodically, using a RIP response message, as opposed to
distance vectors in the DV Protocols. These messages, called RIP
advertisements, contain information about sender's distances to
destination subnets.

Let's look at a simple RIP example to illustrate how it works. The
figure below shows a portion of the network. Here, A, B, C and D denote
the routers and w, x, y and z denote the subnet masks.

Each router maintains a routing table, which contains its own distance
vector as well as the router's forwarding table. If we have a look at
the routing table of Router D, we will see that it has three columns:
destination subnet, identification of the next router along the shortest
path to the destination, and the number of hops to get to the
destination along the shortest path. A routing table will have one row
for each subnet in the AS.

[[file:media/image75.jpg]][[file:media/image52.jpg]]

For this example, the table in the above figure indicates that to send a
datagram from router D to destination subnet w, the datagram should
first be forwarded to neighboring router A; the table also indicates
that destination subnet w is two hops away along the shortest path. Now
if router D receives from router A the advertisement (the routing table
information of router A) shown in the figure below it merges the
advertisement with the old routing table.

In particular, router D learns that there is now a path through router A
to subnet z that is shorter than the path through router B. Therefore,
router D updates its table to account for the new shortest path. The
updated routing table is shown in the figure below. As the Distance
Vector algorithm is in the process of converging or as new links or
routers are getting added to the AS, the shortest path is changing.

[[file:media/image26.jpg]][[file:media/image79.png]]

Each node maintains a RIP Table (Routing Table), which will have one row
for each subnet in the AS. RIP version 2 allows subnet entries to be
aggregated using route aggregation techniques.

If a router does not hear from its neighbor at least once every 180
seconds, that neighbor is considered to be no longer reachable (broken
link). In this case, the local routing table is modified and changes are
propagated. Routers send request and response messages over UDP, using
port number 520, which is layered on top of network-layer IP protocol.
RIP is actually implemented as an application-level process.

Some of the challenges with RIP include updating routes, reducing
convergence time, and avoiding

loops/count-to-infinity problems.

*Linkstate Routing Protocol Example: OSPF*

/Open Shortest Path First (OSPF)/ is a routing protocol which uses a
link state routing algorithm to find the best path between the source
and the destination router. OSPF was introduced as an advancement of the
RIP Protocol, operating in upper-tier ISPs. It is a link-state protocol
that uses flooding of link-state information and a Dijkstra least-cost
path algorithm. Advances include authentication of messages exchanged
between routers, the option to use multiple same cost paths, and support
for hierarchy within a single routing domain.

As we have seen already, a link state routing algorithm is a dynamic
routing algorithm in which each router shares knowledge of its neighbors
with every other router in the network. The network topology that is
built as a result can be viewed as a directed graph with preset weights
for each edge assigned by the administrator.

/Hierarchy./ An OSPF autonomous system can be configured hierarchically
into areas. Each area runs its own OSPF link-state routing algorithm,
with each router in an area broadcasting its link state to all other
routers in that area. Within each area, one or more area border routers
are responsible for routing packets outside the area.

Exactly one OSPF area in the AS is configured to be the backbone area.
The primary role of the backbone area is to route traffic between the
other areas in the AS. The backbone always contains all area border
routers in the AS and may contain non-border routers as well.

For packets routing between two different areas, it is required that the
packet be sent through an area border router, through the backbone and
then to the area border router within the destination area, before
finally reaching the destination.

/Operation./ First, a graph (topological map) of the entire AS is
constructed. Then, considering itself as the root node, each router
computes the shortest-path tree to all subnets, by running Djikstra's
algorithm locally. The link costs have been pre-configured by a network
administrator. The administrator has a variety of choices while
configuring the link costs. For instance, he may choose to set them to
be inversely proportional to link capacity, or set them all to one.
Given set of link weights, OSFP provides the mechanisms for determining
least-cost path routing.

Whenever there is a change in a link's state, the router broadcasts
routing information to all other routers in the AS, not just to its
neighboring routers. It also broadcasts a link's state periodically even
if its state hasn't changed.

/Link State Advertisements./ Every router within a domain that operates
on OSPF uses Link State Advertisements (LSAs). LSA communicates the
router's local routing topology to all other local routers in the same
OSPF area. In practice, LSA is used for building a database (called the
link state database) containing all the link states. LSAs are typically
flooded to every router in the domain. This helps form a consistent
network topology view. Any change in the topology requires corresponding
changes in LSAs.

/Refresh rate for LSAs./ OSPF typically has a refresh rate for LSAs,
which has a default period of 30 minutes. If a link comes alive before
this refresh period is reached, they routers connected to that link
ensure LSA flooding. Since the process of flooding can happen multiple
times, every router receives multiple copies of refreshes or changes -
and stores the first received LSA change as new, and the subsequent ones
as duplicates.

*Processing OSPF Messages in the Router*

In the previous section, we looked at OSPF fundamentals and how it
operates using Link State Advertisements (LSA). In this section we will
look at how the OSPF messages are processed in the router in more
detail.

[[file:media/image82.jpg]]

To do this, let's begin with a simple model of a router given in the
figure above. The router consists of a route processor (which is the
main processing unit) and interface cards that receive data packets
which are forwarded via a switching fabric. Let us break down router
processing in a few steps:

1. 

   #+BEGIN_QUOTE
     Initially, the LS update packets which contain LSAs from a
     neighboring router reaches the current router's OSPF (which is the
     route processor). This is the first trigger for the route
     processor. As the LS Updates reach the router, a consistent view of
     the topology is being formed and this information is stored in the
     link-state database. Entries of LSAs correspond to the topology
     which is actually visible from the current router.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Using this information from the link-state database, the current
     router calculates the shortest path using shortest path first (SPF)
     algorithm. The result of this step is fed to the Forwarding
     Information Base (FIB)
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     The information in the FIB is used when a data packet arrives at an
     interface card of the router, where the next hop for the packet is
     decided and its forwarded to the outgoing interface card.
   #+END_QUOTE

To further understand OSPF processing, let's look at the following flow
chart and view it in time slices (T1, T2, ..., T7).

[[file:media/image127.png]][[file:media/image95.png]]

We've already noted that the processing tasks begin at the receipt of an
LS update packet (T1). For every LSA unpacked from the update packet,
the OSPF protocol checks whether it is a new or a duplicate LSA. This is
done by referring to the link-state database, and checking for the
sequence number of the LSA to a matching LSA instance in the database.
For every new LSA, the database is updated, an SPF calculation is
scheduled (T2) and it's determined which interface the LSA needs to be
flooded out of. In modern routers, the when of LSA flooding can be based
on a timer.

When all the LSAs from an LS update packet have been processed (T3), the
LSAs are prepared and flooded out as an LS Update packet to the next
router (T4). After this, we move on to the actual execution of SPF
calculation within the router (T5 and T6). Since SPF calculation is a
CPU-intensive task, SPF calculations are scheduled and carried out over
a period of time (usually when LSA's are changed) so as to offset the
CPU costs. After the SPF calculation is completed, the FIB is updated
(T7).

*Hot Potato Routing*

In large networks, routers rely both on interdomain and intradomain
routing protocols to route the traffic.

The routers within the network use the intradomain routing protocols to
find the best path to route the traffic within the network. In case when
the final destination of the traffic is outside the network, then the
traffic will travel towards the networks exit (egress points) before
leaving the network. In some cases there are multiple egress points that
the routers can choose from. These egress points (routers) can be
equally good in the sense that they offer similarly good external paths
to the final destination.

In this case, hot potato routing is a technique/practice of choosing a
path within the network, by choosing the closest egress point based on
intradomain path cost (Interior Gateway Protocol/IGP cost).

[[file:media/image24.jpg]]

Let's look at an example.In the figure above, we have a network, and
specifically we are looking at the a router located in Dallas and the
router needs to forward traffic towards a destination. It could do so
via New York or San Francisco. We assume that both egress points offer
BGP (Border Gateway Protocol) path costs, so they are equally good
egress points. In this case, the router has multiple egress points. We
see that the IGP path cost for SF is 9 while the path cost for NY is 10.
Thus, the router uses hot potato routing to choose to send the traffic
to the destination via SF.

Hot potato routing simplifies computations for the routers as they are
already aware of the IGP path costs. It makes sure that the path remains
consistent, since the next router in the path will also choose to send
the packet to the same egress point.Hot potato routing also effectively
reduces the network's resource consumption by getting the traffic out as
soon as possible.

*Optional Reading: An Example Traffic Engineering Framework*

In this section we will see an example traffic engineering framework. It
involves three main components: measure, model and control as shown in
the below figure.

[[file:media/image117.jpg]]

As a first step, the network operator measures the topology of the
network and traffic demands. The next step involves predicting the
effect of change in IGP parameters on the traffic flow to evaluate
different link weights. Finally, once the weights are decided, the new
values are updated on the routers.

/Measure:/ The efficient assignment of link weights depends on the real
time view of the network state which includes:

1. 

   #+BEGIN_QUOTE
     the operational routers and links,
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     the link capacity and IGP parameters configuration.
   #+END_QUOTE

The status of the network elements can be obtained using Simple Network
Management Protocol (SNMP) polling or via SNMP traps. The link capacity
and the IGP parameters can be gathered from the configuration data of
the routers or external databases that enable the provisioning of the
network elements. Furthermore, a software router could act as an IGP
route monitor by participating in OSPF/IS-IS with operational routers
and reporting real time topology information.

In addition to the current network state, the network operator also
requires an estimate of the traffic in the network that can be acquired
either by prior history or by using the following measurement
techniques:

1. 

   #+BEGIN_QUOTE
     Directly from the SNMP Management Information Bases (MIBs)
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     By combining packet-level measurements at the network edge using
     the information in routing tables
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Network tomography which involves observing the aggregate load on
     the links along with the routing data
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     Direct observation of the traffic using new packet sampling
     techniques
   #+END_QUOTE

/Model:/ This involves predicting the traffic flow through the network
based on the IGP configuration. The best path between two routers is
selected by calculating the shortest path between them when all the
links belong to the same OSPF/IS-IS area. In case of large networks
consisting of multiple OSPF/IS-IS areas, the path selection among
routers in different areas is dependent on the summary information
passed across the area boundaries. If there are multiple shortest paths
between two routers, it is leveraged for load balancing by splitting the
traffic almost evenly over these paths.

The routing model thus aims to compute a set of paths between each pair
of routers, with each path representing the fraction of traffic that
passes through each link. The volume of traffic on a link can now be
estimated by combining the output of the routing model and the estimated
traffic demands.

/Control:/ The new link weights are applied on the affected routers by
connecting to the router using telnet or ssh. The exact commands are
dependent on the operating systems of the router. These updates may be
automated or done manually depending on the size of the network.

Once a router receives a weight change, it updates its link-state
database and floods the newly updated value to the entire network. On
receiving the updated value, each router in turn updates its link-state
database, recomputes the shortest paths and updates affected entries in
its forwarding table. Similar to when there is a topology change or a
failure, this involves a transition period where there is a slightly
inconsistent view of the shortest path for few destinations. Although
the convergence after a weight change is faster than a failure scenario
(as there is no delay in detecting a failure), it still involves a
transient period in the network. Hence, understandably, changing the
link weights is not done frequently and only done in scenarios where
there is new hardware, equipment failures or changes in traffic demands.

*Quiz 1*

In this lecture, we discuss intradomain routing, where all the nodes and
subnets are owned and managed by the same organization. (In contrast,
interdomain routing is about routing between different organizations --
such as between two ISPs.) Before we begin talking about intradomain
routing algorithms, what could weights on the graph edges represent in
these diagrams, when we are seeking the least-cost path between two
nodes?

1. 

   #+BEGIN_QUOTE
     Length of the cable
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Time delay to traverse the link
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Monetary cost
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     Business relationships
   #+END_QUOTE

5. 

   #+BEGIN_QUOTE
     Link capacity
   #+END_QUOTE

6. 

   #+BEGIN_QUOTE
     Current load on the link
   #+END_QUOTE

/Answer: All except D. A special note about current load on the link --
this means that the weighgs are not static and change dramatically as
traffic moves over the network. This is a non-trivial problem which can
cause pathological behavior with link-state algorithms./

*Quiz 2*

In the previous example, node u was the source node, and distances were
calculated from u to each other node. Consider the same example, but let
x be the source node. Notice that node x has more direct neighbors than
u does. Suppose x is executing the linkstate algorithm as discussed, and
has just finished the initialization step. Which of the following
statements are true?

[[file:media/image109.png]]

1. 

   #+BEGIN_QUOTE
     Node x will execute fewer iterations than node u did, as there were
     fewer “infinity distance nodes” after initialization.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Node x will execute the same number of iterations that node u did,
     as the number of immediate neighbors has no impact on the number of
     iterations the algorithm requires.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Node x will execute more iterations than node u did, as there are
     more immediate neighbors to consider.
   #+END_QUOTE

/Answer: B/

** Lesson 4 AS Relationships and Interdomain Routing
   :PROPERTIES:
   :CUSTOM_ID: lesson-4-as-relationships-and-interdomain-routing
   :END:

*Introduction*

/AS Relationships and Interdomain Routing/

In the previous lecture, we learned about the protocols we have in place
so data can be routed within a network. But how data travel between
networks?

We know that the Internet is an ecosystem that consists of thousands of
independently operated networks. Each of these networks operate in their
own interest and they have independent economic and traffic engineering
objectives, and yet they must interconnect to provide global
connectivity. In this lecture, we learn about the protocol, called BGP,
that provides the glue for this connectivity. We will also learn about
the different types of interconnections that are based on different
business relationships between networks. Finally, we learn about
increasingly popular infrastructures, called Internet Exchange Points
which primarily provide interconnection services so that the participant
networks can directly exchange traffic with each other.

*Autonomous Systems and Internet Interconnection*

/The Internet is a complex ecosystem./ Today's Internet is a complex
ecosystem that is built of a network of networks. The basis of this
ecosystem includes Internet Service Providers (ISPs), Internet Exchange
Points (IXPs), and Content Delivery Networks (CDNs).

[[file:media/image62.png]][[file:media/image128.png]]

Let's talk more about each type of these networks: First, ISPs can be
categorized into three tiers or types: access ISPs (or Tier-3), regional
ISPs (or Tier-2) and large global scale ISPs (or Tier-1). There is a
dozen of large scale Tier-1 ISPs that operate at a global scale, and
essentially they form the “backbone” network over which smaller networks
can connect. Some example Tier-1 ISPs include AT&T, NTT, Level-3, and
Sprint. In turn regional ISPs connect to Tier-1 ISPs, and smaller access
ISPs connect to regional ISPs.

Second, IXPs are interconnection infrastructures, which provide the
physical infrastructure, where multiple networks (eg ISPs and CDNs) can
interconnect and exchange traffic locally. As of 2019, there are
approximately 500 IXPs around the world.

Third, CDNs are networks that are created by content providers with the
goal of having greater control of how the content is delivered to the
end-users, and also to reduce connectivity costs. Some example CDNs
include Google and Netflix. These networks have multiple data centers -
and each one of them may be housing hundreds of servers -- that are
distributed across the world.

/Competition and cooperation among networks./ This ecosystem we just
described, forms a hierarchical structure, since smaller networks (eg
access ISPs) connect to larger networks (eg Tier-3 ISPs). In other
words, an access ISP receives Internet connectivity becoming the
customer of a larger ISP. In this case, the larger ISP becomes the
provider of the smaller ISP. This leads to competition at every level of
the hierarchy. For example, Tier-1 ISPs compete with each other, and the
same is true for regional ISPs which compete with each other as well.
But, at the same time, competing ISPs need to cooperate to provide
global connectivity to their respective customer networks. ISPs deploy
multiple interconnection strategies depending on the number of customers
in their network and also the geographical location of these networks.

/More interconnection options in the Internet ecosystem./ To complete
the picture of today's Internet interconnection ecosystem we note that
ISPs may also connect through Points of Presence (PoPs), multihoming and
peering. PoPs are one (or more) routers in a provider's network, which
can be used by a customer network to connect to that provider. Also, an
ISP may choose to multi-home by connecting to one or more provider
networks. Finally, two ISPs may choose to connect through a
settlement-free agreement where neither network pays the other to send
traffic to one another directly.

/The Internet topology: hierarchical vs flat./ As we said, this
ecosystem we just described, forms a hierarchical structure, especially
in the earlier days of the Internet. But, its important to note that as
the Internet has been evolving and especially with the dominant presence
of IXPs, and CDNs, the structure has been morphing from hierarchical to
flat.

/Autonomous Systems./ Each of the types of networks that we talked about
above (eg ISPs and CDNs) may operate as an Autonomous System (AS). An AS
is a group of routers (including the links among them) that operate
under the same administrative authority. An ISP, for example, may
operate as a single AS or it may operate through multiple ASes. Each AS
implements its own set of policies, makes its own traffic engineering
decisions and interconnection strategies, and also determines how the
traffic leaves and enters the network.

/Protocols for routing traffic between and within ASes./ The border
routers of the ASes use the Border Gateway Protocol (BGP) to exchange
routing information with one another. In contrast, the Internal Gateway
Protocols (IGPs), operate within an AS and they are focused on
“optimizing a path metric” within that network. Example IGPs include
Open Shortest Paths First (OSPF), Intermediate System - Intermediate
System (IS-IS), Routing Information Protocol (RIP), E-IGRP. In this
lesson, we will focus on BGP.

*AS Business Relationships*

In this topic, we will talk about the prevalent forms of business
relationships between ASes:

1. 

   #+BEGIN_QUOTE
     /Provider-Customer relationship (or transit)./ This relationship is
     based on a financial settlement which determines how much the
     customer will pay the provider, so the provider forwards the
     customer's traffic to destinations found in the provider's routing
     table (including the opposite direction of the traffic as well).
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Peering relationship./ In a peering relationship, two ASes share
     access to a subset of each other's routing tables. The routes that
     are shared between two peers are often restricted to the respective
     customers of each one. The agreement holds provided that the
     traffic exchanged between the two peers is not highly asymmetric.
     Peering relationships are formed between Tier-1 ISPs but also
     between smaller ISPs. In the case of Tier-1 ISPs, the two peers
     need to be of similar size and handle similar amounts of traffic.
     Otherwise, the larger ISP would lack the incentive to enter a
     peering relationship with a smaller size ISP. In the case of
     peering between two smaller size ISPs, the incentive they both have
     is to save the money they would pay their providers by directly
     forwarding to each other their traffic, provided that there is a
     significant amount of traffic that is destined for each other (or
     each other's customers).
   #+END_QUOTE

[[file:media/image14.png]]

/How do providers charge customers?/

While peering allows networks to get their traffic forwarded without
cost, provider ASes have a financial incentive to forward as much of
their customers' traffic as possible. One major factor that determines a
provider's revenue is the data rate of an interconnection. A provider
usually charges in one of two ways:

1. 

   #+BEGIN_QUOTE
     Based on a fixed price given that the bandwidth used is within a
     predefined range.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Based on the bandwidth used. The bandwidth usage is calculated
     based on periodic measurements, e.g., on five min intervals. The
     provider then charges by taking the 95th percentile of the
     distribution of the measurements.
   #+END_QUOTE

Sometimes in practice, we observe complex routing policies. In some
cases, the driving force behind these policies is to increase the amount
of traffic from a customer to its provider, and therefore increase the
providers' revenue.

*BGP Routing Policies: Importing and Exporting Routes*

In the previous topic, we talked about AS business relationships. AS
business relationships drive an AS' routing policies and influence which
routes an AS needs to import or export. In this topic, we will talk
about why it matters which routes an AS imports/exports.

[[file:media/image73.png]]

*Exporting Routes*

Deciding which routes to export is an important decision with business
and financial implications. This is the case because, advertising a
route for a destination to a neighboring AS, means that this route may
be selected by that AS and traffic will start to flow through. Deciding
which routes to advertise is a policy decision and it is implemented
through route filters; route filters are essentially rules that
determine which routes an AS will allow to advertise to other
neighboring ASes.

Let's look at the different types of routes that an AS (let's call it X)
decides whether to export:

- 

  #+BEGIN_QUOTE
    Routes learned from customers. These are the routes that X receives
    as advertisements from its customers. Since provider X is getting
    paid to provide reachability to a customer AS, it makes sense that X
    wants to advertise these customer routes to as many other
    neighboring ASes as possible. This will likely cause more traffic
    towards the customer (through X) and hence more revenue to X.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Routes learned from providers. These are the routes that X receives
    as advertisements from its providers. Advertising these routes
    doesn't make sense, since X does not have the financial incentive to
    carry traffic for its provider's routes. These routes are withheld
    from X's peers and other X's providers, but they are advertised to
    X's customers.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Routes learned from peers. These are routes that X receives as
    advertisements from its peers. As we saw earlier, it doesn't make
    sense for X to advertise to a provider A the routes that it receives
    from another provider B. Because in that case, these providers A and
    B are going to use X to reach the advertised destinations without X
    making revenue. The same is true for the routes that X learns from
    peers.
  #+END_QUOTE

/Importing Routes/

Similarly as exporting, ASes are selective about which routes to import
based, primarily, on which neighboring AS advertises them and what type
of business relationship is established. An AS receives route
advertisements from its customers, providers and peers.

When an AS receives multiple route advertisements towards the same
destination, from multiple ASes, then it needs to rank the routes before
selecting which one to import. The routes that are preferred first are
the customer routes, then the peer routes and finally the provider
routes. The reasoning behind this ranking is that an AS...

1. 

   #+BEGIN_QUOTE
     wants to ensure that routes towards its customers do not traverse
     other ASes unnecessarily generating costs,
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     uses routes learned from peers since these are usually “free”
     (under the peering agreement),
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     and finally resorts to import routes learned from providers as
     these will add to costs.
   #+END_QUOTE

*BGP and Design Goals*

In the previous topics, we talked about importing and exporting routes.
In the following topics, we will learn how the default routing
protocol - Border Routing Protocol or BGP - is used to implement routing
policies.

Let's first start with the design goals of the BGP protocol:

/Scalability:/ As the size of the Internet grows, the same is true for
the number of ASes, the number of prefixes in the routing tables, the
network churn, and the BGP traffic exchanged between routers. One of the
design goals of BGP is to manage the complications of this growth, while
achieving convergence in reasonable timescales and providing loop-free
paths.

/Express routing policies:/ BGP has defined route attributes that allow
ASes to implement policies (which routes to import and export), through
route filtering and route ranking. Each ASes routing decisions can be
kept confidential, and each AS can implement them independently of one
another.

/Allow cooperation among ASes:/ Each individual AS can still make local
decisions (which routes to import and export) while keeping these
decisions confidential from other ASes.

/Security:/ was not included in the original design goals for BGP. But
as the complexity and size of the Internet has been increasing, so is
the need to provide security measures. We notice an increasing need for
protection against malicious attacks, misconfigurations or faults, but
also their early detection. These vulnerabilities still cause routing
disruptions and connectivity issues for individual hosts, networks and
sometimes even entire countries. There have been several efforts to
enhance BGP security ranging from protocols (eg S-BGP), additional
infrastructure (eg registries to maintain up to date information about
which ASes own which prefixes ASes), public keys for ASes, etc. Also,
there has been extensive research work to develop machine learning based
approaches and systems. But these solutions have not been widely
deployed or adopted due to multiple reasons that include difficulties to
transition to new protocols and lack of incentives.

*BGP Protocol Basics*

In this topic, we will review some of the basics of the BGP protocol.

/BGP session/*.* A pair of routers, known as /BGP peers/, exchange
routing information over a semi-permanent TCP port connection called a
/BGP session/. To begin a BGP session a router will send an OPEN message
to another router. Then the sending and receiving router will send each
other announcements from their individual routing tables. Depending on
the number of routes being exchanged, this can take from seconds up to
several minutes.

A BGP session between a pair of routers in two different ASes is called
/external BGP (eBGP)/ session, and a BGP session between routers that
belong to the same AS is called */internal BGP (iBGP)/* session.

In the following diagram, we can see 3 different ASes along with iBGP
(eg between 3c and 3a) and eBGP (eg between 3a and 1c ) sessions between
their border routers.

[[file:media/image46.png]]

/BGP messages./ After a session is established between BGP peers, the
peers can exchange BGP messages to provide reachability information and
enforce routing policies. We have two types of BGP messages:

1. 

   #+BEGIN_QUOTE
     UPDATE
   #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Announcements: These messages advertise new routes and updates to
    existing routes. They include several standardized attributes.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Withdrawals: These messages are sent when a previously announced
    route is removed. This could be due to some failure or due to a
    change in the routing policy.
  #+END_QUOTE

2. KEEPALIVE: These messages are exchanged to keep a current session
going.

/BGP prefix reachability./ In the BGP protocol, destinations are
represented by IP Prefixes. Each prefix represents a subnet or a
collection of subnets that an AS can reach. Gateway routers running eBGP
advertise the IP Prefixes they can reach according to the AS's specific
export policy to routers in neighboring ASes. Then, using separate iBGP
sessions, the gateway routers disseminate routes to internal routers
according to the AS's import policy. Internal routers run iBGP to
propagate the routes to other internal iBGP speaking routers.

/Path Attributes and BGP Routes./ In addition to the reachable IP prefix
field, advertised BGP routes consist of a number of BGP attributes. Two
notable attributes are AS-PATH and NEXT-HOP.

- 

  #+BEGIN_QUOTE
    AS-PATH. Each AS, as identified by the AS's autonomous system number
    (ASN), that the route passes through is included in the AS-PATH.
    This attribute is used to prevent loops and to choose between
    multiple routes to the same destination, the route with the shortest
    path.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    NEXT-HOP. This attribute refers to the IP address (interface) of the
    next-hop router along the path towards the destination. Internal
    routers use the field to store the IP address of the border router.
    Internal BGP routers will have to forward all traffic bound for
    external destinations through the border router. If there is more
    than one such router on network and each advertises a path to the
    same external destination, NEXT-HOP allows the internal router to
    store in the forwarding table the best path according to the AS
    routing policy.
  #+END_QUOTE

*iBGP and eBGP*

In the previous topic we saw that we have two flavors of BGP: eBGP (for
sessions are between border routers of neighboring ASes) and iBGP (for
sessions between internal routers of the same AS).

Both protocols are used to disseminate routes for external destinations.

The eBGP speaking routers learn routes to external prefixes and they
disseminate them to all routers within the AS. This dissemination is
happening with iBGP sessions. For example, as we see in the figure
below, the border routers of AS1, AS2, and AS3 establish eBGP sessions
to learn external routes. Inside AS2, these routes are disseminated
using iBGP sessions.

Also, we note that the dissemination of routes within the AS is done by
establishing a full mesh of iBGP sessions between the internal routers.
Each eBGP speaking router has an iBGP session with every other BGP
router in the AS, so that is can send updates about the routes it learns
(over eBGP).

[[file:media/image86.png]][[file:media/image60.png]]

Finally, we note that iBGP is not another IGP-like protocol (eg RIP or
OSPF). IGP-like protocols are used to establish paths between the
internal routers of an AS based on specific costs within the AS. In
contrast, iBGP is only used to disseminate external routes within the
AS.

*BGP Decision Process: Selecting Routes at a Router*

As we already discussed in earlier topics ASes are operated and managed
by different administrative authorities, and they can operate with
different business goals, and network conditions (eg volumes of
traffic). Of course, all these factors can affect the BGP policies for
each AS independently.

Still, routers follow the same process to select routes. Let's zoom into
what is happening as the routers exchange BGP messages to select routes.

[[file:media/image31.png]][[file:media/image105.png]]

Conceptually, we can consider the model of a router as in the figure
above (Reference:
https://www.cc.gatech.edu/home/dovrolis/Papers/bgp-scale-conext08.pdf
(Links to an external site.)). A router receives incoming BGP messages
and processes them. When a router receives advertisements, first it
applies the import policies to exclude routes entirely from further
consideration.

Then the router implements the decision process to select the best
routes that reflect the policy in place. The new selected routes are
installed in the forwarding table. Finally, the router decides which
neighbors to export the route to, by applying the export policy.

/The router's decision process./

Let's take a look at the router's decision process. Now, let's suppose
that a router receives multiple route advertisements to the same
destination. How does the router choose which route to import? In a
nutshell, the decision process is how the router compares routes, by
going through the list of attributes in the route advertisements. In the
simplest scenario, where there is no policy in place (meaning it doesn't
matter which route will be imported), the router uses the attribute of
the pathlength to select the route with the fewest number of hops. But
in practice, this simple scenario is rarely the case.

A router compares a pair of routes, by going through the list of
attributes - as shown in the figure below. For each attribute, it
selects the route with the attribute value that will help apply the
policy. If for a specific attribute, the values are the same, then it
goes to the next attribute.

Let's focus on two attributes, LocalPref and MED (Multi-Exit
Discriminator) , and let's see how we can use them to influence the
decision process.

/Influencing the route decision using the LocalPref./ The LocalPref
attribute is used to prefer routes learned through a specific AS over
other ASes. For example, suppose AS B learns of a route to the same
destination x via A and C. If B prefers to route its traffic through A,
due to peering or business, it can assign a higher LocalPref value to
routes it learns from A. And therefore, by using LocalPref, AS B can
control where the traffic exits the AS. In other words, it will
influence which routers will be selected as exit points for the traffic
that leaves the AS (outbound traffic).

[[file:media/image49.png]][[file:media/image69.png]]

As we saw earlier in this lesson, an AS ranks the routes it learns by
preferring first the routes learned from its customers, then the routes
learned from its peers and finally the routes learned from its
providers. An operator can assign a non-overlapping range of values to
the LocalPref attribute according to the type of relationship. So
assigning different LocalPref ranges will influence which routes are
imported. For example, there may be the following scheme in place, to
reflect the business relationships.

/Influencing the route decision using the MED attribute./ The MED
(Mutli-Exit Discriminator) value is used by ASes connected by multiple
links to designate which of those links are preferred for inbound
traffic. For example, the network operator of AS B will assign different
MED values to its routes advertised to AS A through R1 and different MED
values to its routes advertised through R2. As a result of different MED
values for the same routes, AS A will be influenced to choose R1 to
forward traffic to AS B, if R1 has lower MED value, and if all other
attributes are equal.

We have seen in the previous topics that an AS does not have an economic
incentive to export routes that it learns from providers or peers to
other providers or peers. An AS can reflect this by tagging routes with
a MED value to “staple” the type of business relationship. Also, an AS
filters routes with specific MED values before exporting them to other
ASes. We note that influencing the route exports will also affect how
the traffic enters an AS (the routers that are entry points for the
traffic that enters the AS).

/So, where/how are the attributes controlled?/ The attributes are set
either: a) locally by the AS (eg LocalPref), b) by the neighboring AS
(eg MED), or c) they are set by the protocol (eg if a route is learned
through eBGP or iBGP).

*Challenges with BGP: Scalability and Misconfigurations*

Unfortunately, the BGP protocol in practice can suffer from two major
limitations: misconfigurations and faults. A possible misconfiguration
or an error can result in an excessively large number of updates which
in turn can result in route instability, router processor and memory
overloading, outages, and router failures.

One way that ASes can help to reduce the risk that these events will
happen is by limiting the routing table size and also by limiting the
number of route changes.

An AS can limit the routing table size using filtering. For example,
long (very specific) prefixes can be filtered to encourage route
aggregation. Routers can limit the number of prefixes that are
advertised from a single source on a per-session basis. Some small ASes
also have the option to configure /default routes/ into their forwarding
tables. ASes can likewise protect other ASes by using route aggregation
and exporting less specific prefixes where possible.

Also, an AS can limit the number of routing changes, specifically
limiting the propagation of unstable routes, by using a mechanism known
as /flap damping/. To apply this technique, an AS will track the number
of updates to a specific prefix over a certain amount of time. If the
tracked value reaches a configurable value, the AS can suppress that
route until a later time. Because this can affect reachability, an AS
can be strategic about how it uses this technique for certain prefixes.
For example, more specific prefixes could be more aggressively
suppressed (lower thresholds), while routes to known destinations that
require high availability could be allowed higher thresholds.

*Peering at IXPs*

In the previous topics we talked about ASes' business relationships.
ASes can either peer with one another directly or they can peer at
Internet Exchange Points (IXPs) which are infrastructures that
facilitate peering but also provide more services.

/What are IXPs?/

IXPs are physical infrastructures that provide the means for ASes to
interconnect and directly exchange traffic with one another. The ASes
that interconnect at an IXP are called participant ASes. The physical
infrastructure of an IXP is usually a network of switches that are
located either in the same physical location, or they can be distributed
over a region or even at a global scale. Typically, the infrastructure
has fully redundant switching fabric that provides fault-tolerance, and
the equipment is usually located in facilities such as data centers to
provide reliability, sufficient power and physical security.

For example, in the figure below we see an IXP infrastructure (2012),
called DE-CIX that is located in Frankfurt, Germany. The figure shows
the core of the infrastructure (noted as 3 and 6) and additional sites
(1-4 and 7) that are located at different colocation facilities in the
area.

[[file:media/image21.png]]

Why have IXPs become increasingly popular and are important to study?
Some of the most important reasons include:

1. 

   #+BEGIN_QUOTE
     /IXPs are interconnection hubs handling large traffic volumes/: A
     2012 study by Ager et al. analyzed a large European IXP and showed
     the presence of more than 50,000 actively used peering links! For
     some large IXPs (mostly located in Europe), the daily traffic
     volume is comparable to the traffic volume handled by global Tier 1
     ISPs.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Important role in mitigating DDoS attacks/: As IXPs have become
     increasingly popular interconnection hubs, they are able to observe
     the traffic to/from an increasing number of participant ASes. In
     this role, IXPs can play the role of a “shield” to mitigate DDoS
     attacks and stop the DDoS traffic before it hits a participant AS.
     There are a plethora of DDoS events that have been mitigated by
     IXPs. For example, back in March 2013, a huge DDoS attack took
     place that involved Spamhaus, Stophaus, and CloudFare. At the
     lecture on Security, we will look into specific techniques that
     IXPs have to mitigate DDoS based on BGP blackholing.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /“Real-world” infrastructures with a plethora of research
     opportunities:/ IXPs play an important role in today's Internet
     infrastructure. Studying this peering ecosystem, the end-to-end
     flow of network traffic, and the traffic that traverses these
     facilities can help us understand how the Internet landscape is
     changing. IXPs also provide an excellent “research playground” for
     multiple applications. Such as security applications. For example
     BGP blackholing for DDoS mitigation, or applications for Software
     Defined Networking.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     /IXPs are active marketplaces and technology innovation hubs/: IXPs
     are active marketplaces, especially in North America and Europe.
     They provide an expanding plethora of services that go beyond
     interconnection, for example DDoS mitigation, or SDN-based
     services. IXPs have been evolving from interconnection hubs to
     technology innovation hubs.
   #+END_QUOTE

/What are the steps for an AS to peer at an IXP?/

Each participating network must have a public Autonomous System Number
(ASN). Each participant brings a router to the IXP facility (or one of
its locations in case the IXP has an infrastructure distributed across
multiple data centers) and connects one of its ports to the IXP switch.
The router of each participant must be able to run BGP since the
exchange of routes across the IXP is via BGP only. Each participant has
to agree to the IXP's General Terms and Conditions (GTC).

Thus, for two networks to publicly peer at an IXP (i.e., use the IXP's
network infrastructure to establish a connection for exchanging traffic
according to their own requirements and business relationships), they
each incur a one-time cost for establishing a circuit from their
premises to the IXP, a monthly charge for using a chosen IXP port
(higher port speeds are more expensive), and possibly an annual fee for
membership to the entity that owns and operates the IXP. In particular,
exchanging traffic over an established public peering link at an IXP is
in principle “settlement-free” (i.e., involves no from of payment
between the two parties) as IXPs typically do not charge for exchanged
traffic volume. Moreover, IXPs typically do not interfere with the
bilateral relationships that exist between the IXP's participants,
unless they are in violation of the GTC. For example, the two parties of
an existing IXP peering link are free to use that link in ways that
involve paid peering, or some networks may even offer transit across an
IXP's switching fabric. Depending on the IXP, the time it takes to
establish a public peering link can range from a few days to a couple of
weeks.

/Why networks choose to peer at IXPs?/

- 

  #+BEGIN_QUOTE
    Keeping local traffic local. In other words, the traffic that is
    exchanged between two networks does not need to travel unnecessarily
    through other networks if both networks are participants in the same
    IXP facility.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Lower costs. Typically peering at an IXP is offered at lowered cost
    than eg relying on third-parties to transfer the traffic charging
    based on volume.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Improved network performance due to reduced delay.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Incentives. Critical players in today's Internet ecosystem often
    “incentivize” other networks to connect at IXPs. For example, a big
    content provider may require another network to be present at a
    specific IXP(s) in order to peer with them.
  #+END_QUOTE

Now, let's take a look at the services that IXPs provide:

1. 

   #+BEGIN_QUOTE
     /Public peering:/ The most well-known use of IXPs is public peering
     service - in which two networks use the IXP's network
     infrastructure to establish a connection to exchange traffic based
     on their bilateral relations and traffic requirements. The costs
     required to set up this connection are - one-time cost for
     establishing the connection, monthly charge for using the chosen
     IXP port (those with higher speeds are more expensive) and perhaps
     an annual fee of membership in the entity owning and operating the
     IXP. However, the IXPs do not usually charge based on the amount of
     exchanged volume. They also do not usually interfere with bilateral
     relations between the participants unless there is a violation of
     the GTC. Even with the set-up costs, IXPs are usually cheaper than
     other conventional methods of exchanging traffic (such as relying
     on third parties which charge based on the volume of exchanged
     traffic). IXP participants also often experience better network
     performance and QoS because of reduced delays and routing
     efficiencies. In addition, many companies that are major players in
     the Internet space (such as Google) incentivize other networks to
     connect at IXPs by making it a requirement to peering with them.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Private peering:/ Most operational IXPs also provide a private
     peering service (Private Interconnects - PIs) that allow direct
     traffic exchange between two parties of a PI and don't use the
     IXP's public peering infrastructure. This is commonly used when the
     participants want a well-provisioned dedicated link capable of
     handling high-volume, bidirectional and relatively stable traffic.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Route servers and Service level agreements:/ Many IXPs also
     include service level agreements (SLAs) and free use of the IXP's
     route servers for participants. This allows participants to arrange
     instant peering with a large number of co-located participant
     networks using essentially a single agreement/BGP session.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     /Remote peering through resellers:/ Another popular service is IXP
     reseller/partner programs. This allows third parties to resell IXP
     ports wherever they have infrastructure connected to the IXP. These
     third parties are allowed to offer the IXP's service remotely,
     which allows networks that have little traffic to also use the IXP.
     This also enables remote peering - networks in distant geographic
     areas can use the IXP.
   #+END_QUOTE

5. 

   #+BEGIN_QUOTE
     /Mobile peering:/ Some IXPs also provide support for mobile
     peering - a scalable solution for interconnection of mobile GPRS/3G
     networks.
   #+END_QUOTE

6. 

   #+BEGIN_QUOTE
     /DDoS blackholing:/ A few IXPs provide support for
     customer-triggered blackholing, which allows users to alleviate the
     effects of DDoS attacks against their network.
   #+END_QUOTE

7. 

   #+BEGIN_QUOTE
     /Free value-added services:/ In the interest of ‘good of the
     Internet', a few IXPs such as Scandinavian IXP Netnod offer free
     value-added services like Internet Routing Registry (IRR), consumer
     broadband speed tests9, DNS root name servers, country-code
     top-level domain (ccTLD) nameservers, as well as distribution of
     the official local time through NTP.
   #+END_QUOTE

*Peering at IXPs: How Does a Route Server Work?*

Generally, the manner in which two ASes exchange traffic through the
switching fabric was utilizing a two-way BGP session, called a
/bilateral/ BGP session. Since there has been an increasing number of
ASes peering at an IXP, we have another challenge to accommodate an
increasing number of BGP sessions. Obviously this option does not scale
with a large number of participants. To mitigate this some IXPs operate
a route server, which helps to make peering more manageable. In summary,
a Route Server (RS):

- 

  #+BEGIN_QUOTE
    Collects and shares routing information from its peers or
    participants that connects with (i.e. IXP members that connect to
    the RS).
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Executes its own BGP decision process and also re-advertise the
    resulting information (I.e. best route selection) to all RS's peer
    routers.
  #+END_QUOTE

The figure below shows a /multilateral BGP peering session/, which is
essentially an RS that facilitates and manages how multiple ASes can
“talk” on the control plane simultaneously.

[[file:media/image45.png]][[file:media/image54.png]]

/How does a route server (RS) maintain multi-lateral peering sessions?/

Let's look at a modern RS architecture in the figure below to understand
how RSes work. A typical routing daemon maintains a Routing Information
Base (RIB) which contains all BGP paths that it receives from its
peers - the Master RIB. The router server also maintains AS-specific
RIBs to keep track of the individual BGP sessions they maintain with
each participant AS.

RSes maintain two types of route filters: a) /Import filters/ are
applied to ensure that each member AS only advertises routes that it
should advertise, b) /Export filters/ which are typically triggered by
the IXP members themselves to restrict the set of other IXP member ASes
that receive their routes. Let's look at an example where AS X and AS Z
exchange routes through a multi-lateral peering sessions that the route
server holds.

Steps:

1. 

   #+BEGIN_QUOTE
     In the first step, AS X advertises a prefix p1 to the RS which is
     added to the route server's AS X specific RIB.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The route server uses the peer-specific import filter, to check
     whether AS X is allowed to advertise p1. If it passes the filter,
     the prefix p1 is added to the Master RIB.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     The route server applies the peer-specific export filter to check
     if AS X allows AS Z to receive p1, and if true it adds that route
     to the AS Z-specific RIB.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     Now, RS advertises p1 to AS Z with AS X as the next hop.
   #+END_QUOTE

*Optional Reading: Remote Peering*

/What is remote peering?/

Remote peering (RP) is peering at the peering point without the
necessary physical presence. The /remote peering provider/ is an entity
that sells access to IXPs through their own infrastructure. RP removes
the barrier to connecting to IXPs around the world, which in itself can
be a more cost-effective solution for localised or regional network
operators.

/How to detect remote peering?/

An interesting problem is how we can tell if an AS is directly connected
to an IXP or it is connected through remote peering. Researchers have
studied this problem and identified methodologies to detect remote
peering with high accuracy and by performing experiments with a large
number of IXPs.

[[file:media/image37.png]]

The primary method of identifying remote peering is to measure the
round-trip time (RTT) between a vantage point (VP) inside the IXP and
the IXP peering interface of a member. However, this method fails to
account for the changing landscape of IXPs today, and even misinfers
latencies of remote members as local and local members as being remote.
Instead, a combination of methods can achieve detection of remote
peering in a more tractable way, some of which include:

1. 

   #+BEGIN_QUOTE
     /Information about the port capacity/: One way to find reseller
     customers is via port capacities. The capacity of peering port for
     each IXP member, can be obtained through the IXP website or
     PeeringDB. IXPs offer to ASes connectivity to ports with capacity
     typically between 1 and 100 Gbit/s. But resellers usually offer
     connectivity through their virtual ports with smaller capacities
     and lower prices.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Gathering colocation information/. An AS needs to be physically
     present (actually deploy routing equipment) in at least one
     colocation facility where the IXP has deployed switching equipment.
     Even though it should easy to locate the colocation facilities
     where both AS and IXPs are colocated, though in practice this
     information is imperfect.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Multi-IXP router inference/: An AS can operatesa multi-IXP router
     which is a router connected to multiple IXPs to reduce operational
     costs. If a router is connected to multiple IXPs and say, we infer
     the AS as local or remote to one of these IXPs from a previous
     step, we can extend the inference to the rest of the involved IXPs
     based on whether they share co-location facilities or not.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     /Private connectivity with multiple existing AS participants/: If
     an AS has private peers over the same router that connects it to an
     IXP, and the private peers are physically co-located to the same
     IXP facilities, it can be inferred that the AS is also local to the
     IXP.
   #+END_QUOTE

*Optional Reading: BGP Configuration Verification*

/Control of BGP configuration is complex/ and easily misconfigured both
at the eBGP configuration level and within an AS, at the iBGP level,
where route propagation happens in a full mesh or via “route
reflectors”. Configuration languages vary among routing manufacturers
and may not be well-designed. Adding to the complexity, is the
distributed nature of BGP's implementation.

BGP correctness is defined by two main aspects of persistent routing.
They are /path visibility and route validity/.

- 

  #+BEGIN_QUOTE
    Path visibility - means that route destinations are correctly
    propagated through the available links in the network.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Route validity - means that the traffic meant for a given
    destination reaches it.
  #+END_QUOTE

/The router configuration checker, or rcc/, is a tool proposed by
researchers, that detects BGP configuration faults. rcc uses static
analysis to check for correctness prior to running the configuration on
an operational network, before deployment. rcc analyzes router
configuration settings and outputs a list of configuration faults.

To analyze a single router or a network-wide BGP configuration, rcc will
first “factor” the configuration to a normalized model by focusing on
how the configuration is set to handle (1) route dissemination, (2)
route filtering, and (3) route ranking.

Although rcc is designed to be used prior before running a live BGP
configuration, it can be used to analyze the configuration of live
systems and potentially detect live faults. In analyzing real-world
configurations it was found that most Path Visibility Faults were the
result of:

1. 

   #+BEGIN_QUOTE
     problems with “full mesh” and route reflector configurations in
     iBGP settings leading to signaling partitions
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Route reflector cluster problems
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Incomplete iBGP sessions where an iBGP session is active on one
     router but not the other.
   #+END_QUOTE

Route Validity Faults were determined to stem from filtering and
dissemination problems. The specific filtering behaviors included legacy
filtering policies not being fully removed when changes occur,
inconsistent export to peer behavior, inconsistent import policies,
undefined references in policy definitions, or non-existent or
inadequate filtering. Dissemination problems included unorthodox AS
prepending practices and iBGP sessions with “next-hop self”. These
issues suggest that routing might be less prone to faults if there were
improvements to iBGP protocols when it comes to making updates and
scaling.

Because rcc is intended to run prior to deployment, it may help network
operators detect issues before they become major problems in a live
setting, which often go undetected right away. rcc is implemented as
static analysis and does not offer either completeness or soundness; it
may generate false positives and it may not detect all faults.

** 
   :PROPERTIES:
   :CUSTOM_ID: section
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-1
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-2
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-3
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-4
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-5
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-6
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-7
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-8
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-9
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-10
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-11
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-12
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-13
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-14
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-15
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-16
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-17
   :END:

** Lesson 5: Router Design and Algorithms (Part I)
   :PROPERTIES:
   :CUSTOM_ID: lesson-5-router-design-and-algorithms-part-i
   :END:

*Introduction*

As we have seen in previous lectures a critical part of the Internet
physical infrastructure is the routers. We remember that our overarching
theme through the course, is to understand what needs to have in place
for two hosts to exchange data. Routers are an integral part of this
process because they forward the data along the path from source to
destination.

In this lecture, we will look deeper into how routers work.

We learn about the components of a router's architecture and we zoom
into the router's most important task, which is to transfer the received
packet from an input link interface to the appropriate output link
interface, towards the packet's destination. When a packet arrives at
the input link the router's job is to look at the destination IP address
of the packet, and determine the output link by consulting the
forwarding table.

But performing this important task at scale has challenges, and
especially when handling large volumes of traffic in high-speed
networks. At the same time given the increasing number of applications
and services, some traffic does not require just forwarding, but the
packets require different quality of service or security guarantees. The
routers need to handle packets based on multiple criteria (flags,
quality of service). We will learn about the algorithms that the routers
use to handle these tasks: such as the longest prefix matching, packet
classification and scheduling.

The first lecture focuses on the longest prefix match algorithms, and
the second lecture focuses on packet classification and scheduling.

*What's Inside a Router?*

What are the basic components of a router?

The main job of a router is to implement the forwarding plane functions
and the control plane functions.

/Forwarding (or switching) function:/

This is the router's action to transfer a packet from an input link
interface to the appropriate output link interface. Forwarding takes
place at very short timescales (typically a few nanoseconds), and is
typically implemented in hardware.

The router's architecture is shown in the figure below. The main
components of a router are the input/output ports, the switching fabric
and the routing processor.

[[file:media/image9.png]][[file:media/image12.png]]

/Input ports:/

There are several functionalities that are performed by the input ports.

1. 

   #+BEGIN_QUOTE
     If we look at the figure from left to right, the first function is
     to physically terminate the incoming links to the router.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Second, the data link processing unit decapsulates the packets.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Finally, and most importantly, the input ports perform the lookup
     function. At this point, the input ports consult the forwarding
     table to ensure that each packet is forwarded to the appropriate
     output port through the switch fabric.
   #+END_QUOTE

/Switching fabric:/

Simply put, the switching fabric moves the packets from input to output
ports, and it makes the connections between the input and the output
ports. There are three types of switching fabrics: memory, bus, and
crossbar.

/Output ports:/

An important function of the output ports is to receive and queue the
packets which come from the switching fabric and then send them over to
the outgoing link.

/Router's control plane functions:/

By control plane functions we refer to implementing the routing
protocols, maintaining the routing tables, computing the forwarding
table. All these functions are implemented in software in the routing
processor, or as we will see in the SDN chapter, these functions could
be implemented by a remote controller.

[[file:media/image19.png]][[file:media/image126.png]]

*Router Architecture*

In this topic, we will take a closer look at the router's architecture.

[[file:media/image106.png]]

A router has input links and output links and its main task is to switch
a packet from an input link to the appropriate output link based on the
destination address. We note that in this figure, the input/output links
are shown separately but often they are put together.

Now let's look at what happens when a packet arrives at an input link.
First, let's take a look at the most time-sensitive tasks: lookup,
switching, and scheduling.

/Lookup:/ When a packet arrives at the input link, the router looks at
the destination IP address and determines the output link by looking at
the forwarding table (or Forwarding Information Base or FIB). The FIB
provides a mapping between destination prefixes and output links.

To resolve any disambiguation, the routers use the longest prefix
matching algorithms which we will see in a few topics. Also, some
routers offer a more specific and complex type of lookup, called packet
classification, where the lookup is based on destination or source IP
addresses, port, and other criteria.

/Switching:/ After lookup, the switching system takes over to transfer
the packet from the input link to the output link. Modern fast routers
use crossbar switches for this task. Though scheduling the switch
(matching available inputs with outputs) is a difficult task because
multiple inputs may want to send packets to the same output.

/Queuing:/ After the packet has been switched to a specific output, it
will need to be queue (if the link is congested). The queue may be as
simple as First-In-First-Out (FIFO) or it may be more complex (eg
weighted fair queuing) to provide delay guarantees or fair bandwidth
allocation.

Now, let's look at some less time-sensitive tasks that take place in the
router.

/Header validation and checksum:/ The router checks the packet's version
number, it decrements the time-to-live (TTL) field, and also it
recalculates the header checksum.

/Route processing:/ The routers build their forwarding tables using
routing protocols such as RIP, OSPF, and BGP. These protocols are
implemented in the routing processors.

/Protocol Processing:/ The routers, in order to implement their
functions, need to implement the following protocols: a) The simple
network management protocol (SNMP) that provides a set of counters for
remote inspection, b) TCP and UDP for remote communication with the
router, c) Internet control message protocol (ICMP), for sending error
messages, eg when time to live time is exceeded.

*Different Types of Switching*

Let's take a closer look into the switching fabric.

The switching fabric is the brains of the router, as it actually
performs the main task to switch (or forward) the packets from an input
port to an outport port.

Let's look at the ways that this can be accomplished:

/Switching via memory./ Input/Output ports operate as I/O devices in an
operating system, and they are controlled by the routing processor. When
an input port receives a packet, it sends an interrupt to the routing
processor and the packet is copied to the processor's memory. Then the
processor extracts the destination address and looks into the forward
table to find the output port, and finally the packet is copied into
that output's port buffer.

/Switching via bus:/ In this case, the routing processor does not
intervene as we saw the switching via memory. When an input port
receives a new packet, it puts an internal header that designates the
output port, and it sends the packet to the shared bus. Then all the
output ports will receive the packet, but only the designated one will
keep it. When the packet arrives at the designated output port, then the
internal header is removed from the packet. Only one packet can cross
the bus at a given time, and so the speed of the bus limits the speed of
the router.

[[file:media/image58.png]][[file:media/image44.png]]

/Switching via interconnection network:/ A crossbar switch is an
interconnection network that connects N input ports to N output ports
using 2N buses. Horizontal buses meet the vertical buses at crosspoints
which are controlled by the switching fabric. For example, let's suppose
that a packet arrives at port A that will need to be forwarded to output
port Y, the switching fabric closes the crosspoint where the two buses
intersect, so that port A can send the packets onto the bus and then the
packet can only be picked up by output port Y. Crossbar network can
carry multiple packets at the same time, as long as they are using
different input and output ports. For example, packets can go from
A-to-Y and B-to-X at the same time.

[[file:media/image22.png]]

*Challenges that the router faces*

The fundamental problems that a router faces revolve around:

1. 

   #+BEGIN_QUOTE
     /Bandwidth and Internet population scaling:/ These scaling issues
     are caused by: a) An increasing number of devices that connect to
     the Internet, 2) Increasing volumes of network traffic due to new
     applications, and 3) New technologies such as optical links that
     can accommodate higher volumes of traffic.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Services at high speeds:/ New applications require services such
     as protection against delays in presence of congestion, and
     protection during attacks or failures. But offering these services
     at very high speeds is a challenge for routers.
   #+END_QUOTE

To understand why, let's look at the bottlenecks that routers face in
more detail:

[[file:media/image97.png]]

/Longest prefix matching:/ As we have seen in previous topics, routers
need to look up a packet's destination address to forward it. The
increasing number of the Internet hosts and networks, has made it
impossible for routers to have explicit entries for all possible
destinations. Instead routers group destinations into prefixes. But
then, routers run into the problem of more complex algorithms for
efficient longest prefix matching.

/Service differentiation./ Routers are also able to offer service
differentiation which means different quality of service (or security
guarantees) to different packets. In turn, this requires the routers to
classify packets based on more complex criteria that go beyond
destination and they can include source or applications/services that
the packet is associated with.

/Switching limitations./ As we have seen a fundamental operation of
routers is to switch packets from input ports to output ports. A way to
deal with high-speed traffic, is to use parallelism by using crossbar
switching. But in high speeds, this comes with its own problems and
limitations (eg head of line blocking).

/Bottlenecks about services./ Providing performance guarantees (quality
of service) in high speeds is nontrivial. Finally, providing support for
new services such as measurements and security guarantees.

In the following topics, we explore suggested solutions to deal with
these bottlenecks.

*Prefix-Match Lookups*

/What is prefix-match lookup? Why is it required?/

In the next topics, we will look into the prefix-matching algorithms.
The Internet continues to grow both in terms of networks (AS numbers)
and IP addresses. One of the challenges that a router faces is the
scalability problem. One way to help with the scalability problem is to
“group” multiple IP addresses by the same prefix.

/Prefix notation:/

The different ways to denote prefix are:

1. 

   #+BEGIN_QUOTE
     /Dot decimal/
   #+END_QUOTE

Example of 16-bit prefix: 132.234

Binary form of the first octet: 10000100

Binary of second octet: 11101010

Binary prefix of 132.234: 1000010011101010*,

The * indicates wildcard character to say that the remaining bits do not
matter.

2. 

   #+BEGIN_QUOTE
     Slash notation
   #+END_QUOTE

Standard notation: A/L (where A=Address, L=Length)

Example: 132.238.0.0/16

Here, 16 denotes that only the first 16 bits are relevant for prefixing

3. 

   #+BEGIN_QUOTE
     Masking
   #+END_QUOTE

We can use a mask instead of the prefix length.

Example: The Prefix 123.234.0.0/16 is written as 123.234.0.0 with a mask
255.255.0.0

The mask 255.255.0.0 denotes that only the first 16 bits are important.

/What is the need for variable length prefixes?/

In the earlier days of the Internet, we used an IP addressing model
based on classes (fixed length prefixes). With the rapid exhaustion of
IP addresses, in 1993, the Classless Internet Domain Routing (CIDR) came
into effect. CIDR essentially assigns IP addresses using
arbitrary-length prefixes. CIDR has helped to decrease the router table
size but at the same time it introduced us to a new problem:
longest-matching-prefix lookup.

/Why do we need (better) lookup algorithms?/

In order to forward an incoming packet, a router first checks the
forwarding table to determine the port and then does switching to
actually send the packet. There are various challenges that the router
needs to overcome performing a lookup to determine the output port.
These challenges revolve around lookup speed, memory, and update time.

The table below mentions some basic observations around network traffic
characteristics. The table shows for every observation, the consequence
(inference) that motivates and impacts the design of prefix lookup
algorithms. The four take away observations are:

1. 

   #+BEGIN_QUOTE
     Measurement studies on network traffic had shown a large number (in
     the order of hundred thousands, 250,000 according to a measurement
     study in the earlier days of the Internet) of concurrent flows of
     short duration. This already large number has only been increasing.
     This has a consequence that a caching solution would not work
     efficiently.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The important element while performing any lookup operation is how
     fast it is done (lookup speed). A large part of the cost of
     computation for lookup is accessing memory.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     An unstable routing protocol may adversely impact the update time
     in the table: add, delete or replace a prefix. Inefficient routing
     protocols increase this value up to additional milliseconds.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     An important trade-off is memory usage. We have the option to use
     expensive fast memory (cache in software, SRAM in hardware) or
     cheaper but slower memory (e.g., DRAM, SDRAM).
   #+END_QUOTE

[[file:media/image56.png]]

*Unibit Tries*

To start our discussion on prefix matching algorithms, we will use an
example prefix database with nine prefixes as shown below. One of the
simplest techniques for prefix lookup is the unibit trie. For the
example database we have, the figure below shows a unibit trie:

[[file:media/image84.png]][[file:media/image5.png]]

Every node has a 0 or 1 pointer. Starting with the root, pointer 0
points to a subtrie for all prefixes that start with 0, and similarly
pointer 1 points to a subtrie for all prefixes that start with 1. Moving
forward in a similar manner, we construct more subtries by allocating
the remaining bits of the prefix.

When we are doing prefix matching we follow the path from the root node
down to the trie. Let's take an example from the above table and see how
we can do prefix matching in the unibit trie.

For example:

1. 

   #+BEGIN_QUOTE
     Assume that we are doing a longest prefix match for P1=101* (from
     our prefix database). We start at the root node and trace a
     1-pointer to the right, then a 0-pointer to the left and then a
     1-pointer to the right
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     For P7=100000*, we start at the root node and trace a 1-pointer to
     the right, then five 0-pointers the left
   #+END_QUOTE

These are the steps we follow to perform a prefix match:

1. 

   #+BEGIN_QUOTE
     We begin the search for a longest prefix match by tracing the trie
     path.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     We continue the search until we fail (no match or an empty pointer)
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     When our search fails, the last known successful prefix traced in
     the path is our match and our returned value.
   #+END_QUOTE

Two final notes on the unibit trie:

1. If a prefix is a substring of another prefix, the smaller string is
stored in the path to the longer (more specific prefix). For example, P4
= 1* is a substring of P2 = 111*, and thus P4 is stored inside a node
towards the path to P2.

2. One-way branches. There may be nodes that only contain one pointer.
For example let's consider the prefix P3 = 11001. After we match 110 we
will be expecting to match 01. But in our prefix database, we don't have
any prefixes that share more than the first 3 bits with P3. So if we had
such nodes represented in our trie, we would have nodes with only one
pointer. The nodes with only one pointer each are called a one-way
branch. For efficiency, we compress these one-way branches to a single
text string with 2 bits (shown as node P9).

*Multibit Tries*

/Why do we need multibit tries?/

While a unibit trie is very efficient and also offers advantages such as
fast lookup and easier updates, its biggest problem is the number of
memory accesses that it requires to perform a lookup. For 32 bit
addresses, we can see that looking up the address in a unibit trie might
require 32 memory accesses, in the worst case. Assuming a 60 nsec
latency, the worst case search time is 1.92 microseconds. This could be
very inefficient in high speed links.

Instead, we can implement lookups using a stride. By stride we refer to
the number of bits that we check at each step.

So an alternative to unibit tries are the multibit tries. A multibit
trie is a trie where each node has 2k children, where k is the stride.
Next we will see that we can have two flavors of multibit tries: fixed
length stride tries and variable length stride tries.

*Prefix Expansion*

Consider a prefix such as 101* (length 3) and a stride length of 2 bits.
If we search in 2 bit lengths, we will miss out on prefixes like 101*.
To combat this, we use a strategy called controlled prefix expansion,
where we expand a given prefix to more prefixes. We ensure that the
expanded prefix is a multiple of the chosen stride length. At the same
time we remove all lengths that are not multiples of the chosen stride
length. We end up with a new database of prefixes, which may be larger
(in terms of actual number of prefixes) but with fewer lengths. So, the
expansion gives us more speed with an increased cost of the database
size.

The figure below shows how we have expanded our original database of
prefixes. Originally we had 5 prefix lengths. Now we have more prefixes
but only two lengths (3 and 6).

For example we substitute (expand) P3 = 11001* with 110010* and 110011*.

When we expand our prefixes, there may be a collision, i.e. when an
expanded prefix collides with an existing prefix. In that case the
expanded prefix gets dropped. In the figure, we see that in the fourth
expansion of P6=1000* which collides with P7, and thus gets removed.

[[file:media/image92.png]]

*Multibit tries: Fixed-Stride*

As we introduced multibit tries in the previous section, here we will
look at a specific example of a *fixed-stride trie* of length 3. Every
node has 3 bits.

We are using the same database of prefixes as in the previous section.
We can see that the prefixes (P1, P2, P3, P5, P6, P7, P8 and P9) are all
represented in the expanded trie.

Some key points to note here:

1. 

   #+BEGIN_QUOTE
     Every element in a trie represents two pieces of information: a
     pointer and a prefix value.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The prefix search moves ahead with the preset length in n-bits (3
     in this case)
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     When the path is traced by a pointer, we remember the last matched
     prefix (if any).
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     Our search ends when an empty pointer is met. At that time, we
     return the last matched prefix as our final prefix match.
   #+END_QUOTE

[[file:media/image90.jpg]]

/Example:/ We consider an address A which starts with 001. The search
for A starts with the 001 entry at the root node of the trie. Since
there is no outgoing pointer, the search terminates here and returns P5.
Whereas if we search for 100000, the search would terminate with P7.

*Multibit Tries: Variable Stride*

/Why do we need variable strides?/

In this topic, we will talk about a more flexible version of the
algorithm which offers us variable number of strides. With this scheme,
we can examine a different number of bits every time.

We encode the stride of the trie node using a pointer to the node. The
root node stays as is (in the previous scheme).

We note that the rightmost node still needs to examine 3 bits because of
P7.

But at the leftmost node need only to examine 2 bits, because P3 has 5
bits in total. So we can rewrite the leftmost node as in the figure
below.

So now we have 4 fewer entries than our fixed stride scheme. So by
varying the strides we could make the our prefix database smaller, and
optimize for memory.

[[file:media/image64.jpg]]

Some key points about variable stride:

1. 

   #+BEGIN_QUOTE
     Every node can have a different number of bits to be explored
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The optimizations to the stride length for each node are all done
     in pursuit of saving trie memory and the least memory access
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     An optimum variable stride is selected by using dynamic programming
   #+END_QUOTE

** Lesson 6 Router Design and Algorithms (Part 2) (Optional)
   :PROPERTIES:
   :CUSTOM_ID: lesson-6-router-design-and-algorithms-part-2-optional
   :END:

*Why we need packet classification?*

As the Internet becomes increasingly complex, networks require quality
of service guarantees and security guarantees for their traffic. Packet
forwarding based on the longest prefix matching of destination IP
addresses is not enough and we need to handle packets based on multiple
criteria (TCP flags, source addresses) - we refer to this finer packet
handling as packet classification.

Variants of packet classification are already established:

1. 

   #+BEGIN_QUOTE
     /Firewalls/: Routers implement firewalls at the entry and exit
     points of the network to filter out unwanted traffic, or to enforce
     other security policies.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Resource reservation protocols:/ For example, DiffServ has been
     used to reserve bandwidth between a source and a destination.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Routing based on traffic type:/ Routing based on the specific type
     of traffic helps avoid delays for time-sensitive applications.
   #+END_QUOTE

[[file:media/image104.png]]

/Example for traffic type sensitive routing./ The above figure shows an
example topology where networks are connected through router R.
Destinations are shown as S1, S2, X, Y and D. L1 and L2 denote specific
connection points for router R. The table shows some example packet
classification rules. The first rule is for routing video traffic from
S1 to D via L1. The second rule drops all traffic from S2, for example,
in the scenario that S2 was an experimental site. The third rule
reserves 50 Mbps of traffic from prefix X to prefix Y, which is an
example of rule for resource reservation.

*Packet Classification: Simple Solutions*

Before looking into algorithmic solutions to the packet classification
problem, let's take a look at simplest approaches that we have:

/Linear Search/

Firewall implementations perform a linear search of the rules database
and keep track of the best-match rule. This solution can be reasonable
for a few rules but the time to search through a large database that may
have thousands of rules can be prohibitive.

/Caching/

Another approach is to cache the results so that future searches can run
faster. This has two problems:

1. 

   #+BEGIN_QUOTE
     The cache-hit rate can be high (eg 80-90%), but still we will need
     to perform searches for missed hits.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Even with a high 90% hit rate cache, a slow linear search of the
     rule space will result in poor performance. For example, suppose
     that a search of the cache costs 100 nsec (one memory access) and
     that a linear search of 10,000 rules costs 1,000,000 nsec = 1 msec
     (one memory access per rule). Then the average search time with a
     cache hit rate of 90% is still 0.1 msec, which is rather slow.
   #+END_QUOTE

/Passing Labels/

The Multiprotocol label switching (MPLS) and Diffserv use this
technology. MPLS is useful for traffic engineering. A label-switched
path is set up between the two sites A and B. Before traffic leaves site
A, a router does packet classification and maps the web traffic into an
MPLS header. Then the intermediate routers between A and B apply the
label without having to redo packet classification.DiffServ follows a
similar approach, applying packet classification at the edges to mark
packets for special quality of service.

*Fast Searching Using Set-Pruning Tries*

Let's assume that we have a two-dimensional rule. For example, we want
to classify packets both using the source and the destination IP
addresses.

As an example, let's consider the table below as our two-dimensional
rule.

[[file:media/image33.jpg]][[file:media/image100.jpg]]

The simplest way to approach the problem would be to build a trie on the
destination prefixes in the database, and then for every leaf-node at
the destination trie to “hang” source tries.

We start building a trie that looks like this figure below:

By /S/1 we denote the source prefix of rule /R/1, /S/2 of rule /R/2,
etc. Thus for every destination prefix /D/ in the destination trie we
“/prune”/ the set of rules to those that are compatible with /D/.

We first match the destination IP address in a packet in the destination
trie. Then we traverse the corresponding source trie to find the longest
prefix match for the source IP. The algorithm keeps track of the
lowest-cost matching rule. The algorithm concludes with the least-cost
rule.

/Challenge*.*/ The problem that we need to solve now, is which source
prefixes to store at the sources tries? For example, let's consider the
destination D = 00∗. Both rules R4 and R5 have D as the destination
prefix. So the source tries for D will need to include the source
prefixes 1∗ and 11∗.

But if we restrict to 1* and 11*, then this is not sufficient. Because
the prefix 0∗, also matches 00*, and it is found in rules R1, R2, R3,
R7. So we will need to include all the corresponding source prefixes.

Moving forward, the problem with the set pruning tries is memory
explosion. Because a source prefix can occur in multiple destination
tries.

*Reducing Memory Using Backtracking*

The set pruning approach has high cost in memory to reduce time.The
opposite approach is to pay in time to reduce memory.

Lets assume a destination prefix D. The backtracking approach has each
destination prefix D point to a source trie that stores the rules whose
destination field is exactly D. The search algorithm then performs a
“backtracking” search on the source tries associated with all ancestors
of D.

So first, the algorithm goes through the destination trie and finds the
longest destination prefix D matching the header. Then it works its way
back up the destination trie and search the source trie associated with
every ancestor prefix of D that points to a nonempty source trie.

Since each rule now is stored exactly once, the memory requirements are
lower than the previous scheme. But, the lookup cost for backtracking is
worse than for set-pruning tries.

*Grid of Tries*

We have previously seen two solutions for the two dimensional problem:
a) the set pruning approach and b) backtracking.

The set pruning approach has a high cost in memory. Because we construct
a trie on the destination prefixes, and then for every destination
prefix, we have a trie on the source prefixes.

On the other hand, the backtracking approach has a high cost in terms of
time.

With the grid of tries approach, we can reduce the wasted time in the
backtracking search by using precomputation. When there is a failure
point in a source trie, we precompute a switch pointer. Switch pointers
take us directly to the next possible source trie that can contain a
matching rule.

Let's look at an example, let's consider that we search for the packet
with destination address 001 and source address 001. We start the search
with the destination trie which gives us D = 00 as the best match. The
search at that point fails. Instead of backtracking, the grid of tries
has a switch pointer (labeled 0) that points to x. At which point it
fails again. We follow another switch pointer to node y. At that point
the algorithm terminates.

So the precomputed switch pointers allow us to take shortcuts. Using
these pointers we do not do backtracking to find an ancestor node and
then to traverse the source trie. We still proceed to match the source,
and we keep track of our current best source match. But we are skipping
source tries with source fields that are shorter than our current source
match.

[[file:media/image102.png]][[file:media/image134.png]]

*Scheduling and Head of Line Blocking*

In this topic, we start discussing about the problem of scheduling.

/Scheduling/

Let's assume that we have an NxN crossbar switch, with N input lines, N
output lines, and N2 crosspoints. Each crosspoint needs to be controlled
(on/off), and we need to make sure that each input link is connected
with at most one output link. Also, for better performance we want to
maximize the number of input/output links pairs that communicate in
parallel.

/Take the ticket algorithm/

A simple scheduling algorithm is the “take the ticket algorithm”. Each
output line maintains a distributed queue for all input lines that want
to send packets to it. When an input line wants to send a packet to a
specific output line, it requests a ticket. The input line waits for the
ticket to be served. At that point, the input line connects to the
output line, the crosspoint is turned on, and the input line sends the
packet.

As an example, let's consider the figure below that shows three input
lines which want to connect to four output lines. Next to each input
line, we see the queue of the output lines it wants to connect with. For
example, input lines A and B want to connect with output lines 1,2,3.

At the first round, the input lines make ticket requests. For example
line A requests a ticket for link1. The same for B and C. So output link
1 grants three tickets, and it will process them in order. First the
ticket for A, then for B and then for C. Input A observes that its
ticket is served, so it connects to output link 1 and sends the packet.

At the second round, A repeats the process to request a ticket and
connect with link 2. Also B requests a ticket and connects with link 2.

At the third round, A and B move forward repeating the steps for their
next connection. C gets the chance to make its first request and connect
with output link 1. All this time C was blocked waiting for A and B.

The following figure, which shows how the entire process progresses. For
each output link we can see the time line as it connects with input
links. The empty spots mean there was no packet send at the
corresponding time.

As we see, in the first iteration while A sends its packet, the entire
queue for B and C are waiting. We refer to this problem as /head-of-line
(HOL)/ blocking because the entire queue is blocked by the progress of
the head of the queue.

[[file:media/image116.png]][[file:media/image139.png]]

[[file:media/image88.png]][[file:media/image17.png]]

*Avoiding Head of Line Blocking*

/Avoiding head-of-line blocking via output queuing:/

Suppose that we have an NxN crossbar switch. Can we send the packet to
an output link without queueing? If we could, then assuming that a
packet arrives at an output link, it can only block packets that are
sent to the same output link. We could achieve that, if we have the
fabric running N times faster than the input links.

A practical implementation of this approach is the Knockout scheme. It
relies on breaking up packets into fixed size (cell). We suppose that in
practice the same output rarely receives N cells and the expected number
is k (smaller than N). Then we can have the fabric running k times as
fast as an input link, instead of N. We may still have scenarios where
the expected case is violated. To accommodate these scenarios, we have
one or more of a primitive switching element that randomly picks the
chosen output:

- 

  #+BEGIN_QUOTE
    k = 1 and N = 2. randomly pick the output that is chosen. The
    switching element in this case is called a concentrator.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    k = 1 and N > 2. One output is chosen out of N possible outputs. In
    this case, we can use the same strategy of multiple 2-by-2
    concentrators.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    k needs to be chosen out of N possible cells, with k and N arbitrary
    values. We create k knockout trees to calculate the first k winners.
  #+END_QUOTE

The drawback with this approach is that is it is complex to implement.

/Avoiding head-of-line blocking by using parallel iterative matching:/

The main idea is that we can still allow queueing for the input lines,
but in such a way so we avoid the head of line blocking. With this
approach, we schedule both the head of the queue but also more packets,
so that the queue makes progress in the case that the head is blocked.

How can we do that? Let's suppose that we have a single queue at an
input line. We break down the single queue into virtual queues, with one
virtual queue per output link.

Let's consider the following graph that shows A,B,C input links and
1,2,3,4 output links.

The algorithm runs in three rounds.

In the first round, the scheme works by having all inputs send requests
in parallel to all outputs they want to connect with. This is the
request phase of the algorithm.

In the grant phase, the outputs that receive multiple requests pick a
random input, so the output link 1 randomly chooses B. Similarly, the
output link 2 randomly chooses A (between A and B).

[[file:media/image136.png]][[file:media/image42.png]]

[[file:media/image51.png]]

Finally, in the accept phase, inputs that receive multiple grants
randomly pick an output to send to.

We have two output ports (2 and 3) that have chosen the same input (A).
A randomly chooses port 2. B and C choose 1 and 4, respectively.

In the second round, the algorithm repeats by having each input send to
two outputs. And finally the third row repeats by having each input send
to one output.

Thus in four cell times (of which the fourth cell time is sparsely used
and could have been used to send more traffic) all the traffic is sent.
This is clearly more efficient than the take-a-ticket

*Scheduling Introduction*

Busy routers rely on scheduling for the handling of routing updates,
management queries and data packets. For example, scheduling enables
routers to allow certain types of data packets to get different service
from other types. It is important to note that this scheduling is done
in real time. Due to the increasing link speeds (over 40 gigabit), these
scheduling decisions need to be made in the minimum inter-packet times!

/FIFO with tail drop:/

The simplest method of router scheduling is FIFO with tail-drop. In this
method, packets enter a router on input links. They are then looked up
using the address lookup component -- which gives the router an output
link number. The switching system within the router then places the
packet in the corresponding output port. This port is a FIFO (first in,
first out) queue. If the output link buffer is completely full, incoming
packets to the tail of the queue are dropped. This results in fast
scheduling decisions, but potential loss in important data packets.

/Need for Quality of Service (QoS):/

There are other methods of packet scheduling such as priority, round
robin, etc. These methods are useful in providing quality of service
(QoS) guarantees to a flow of packets on measures such as delay and
bandwidth. A flow of packets refers to a stream of packets that travels
the same route from source to destination and require the same level of
service at each intermediate router and gateway. In addition, flows must
be identifiable using fields in the packet headers. For example, an
internet flow could consist of all packets with a either a source or
destination port number of 23.

The reasons to make scheduling decisions more complex than FIFO with
tail drop are:

- 

  #+BEGIN_QUOTE
    /Router support for congestion/
  #+END_QUOTE

Congestion in the internet is increasingly possible as the usage has
increased faster than the link speeds. While most traffic is based on
TCP (which has its own ways to handle congestion), additional router
support can improve the throughput of sources by helping handle
congestion.

- 

  #+BEGIN_QUOTE
    Fair sharing of links among competing flows
  #+END_QUOTE

During periods of backup, these packets tend to flood the buffers at an
output link. If we use FIFO with tail drop, this blocks other flows,
resulting in important connections on the clients' end freezing. This
provides a sub-optimal experience to the user, indicating a change is
necessary!

- 

  #+BEGIN_QUOTE
    Providing QoS guarantees to flows
  #+END_QUOTE

One way to enable fair sharing is to guarantee certain amounts of
bandwidths to a flow. Another way is to guarantee the delay through a
router for a flow. This is noticeably important for video flows --
without a bound on delays, live video streaming will not work well.

Thus, finding time-efficient scheduling algorithms that provide
guarantees for bandwidth and delay are important!.

*Deficit Round Robin*

Let us look at a method to enforce bandwidth reservations in schedulers.

We saw that FIFO queue with tail drop could result in important flows
being dropped. To avoid this, and introduce fairness in servicing
different flows, we consider round robin. If we were to alternate
between packets from different flows, the difference in packets sizes
could result in some flows getting serviced more frequently. To avoid
this, researchers came up with bit-by-bit round robin.

/Bit-by-bit Round Robin/

Imagine a system where in a single round, one bit from each active flow
is transmitted in a round robin manner. This would ensure fairness in
bandwidth allocation. However, since it's not possible in the real world
to split up the packets, we consider an imaginary bit-by-bit system to
calculate the packet-finishing time and send a packet as a whole.

Let R(t) to be the current round number at time t. If the router can
send µ bits per second and the number of active flows is N, the rate of
increase in round number is given by

- 

  #+BEGIN_QUOTE
    dR / dt = µ / N
  #+END_QUOTE

The rate of increase in round number is indirectly proportional to the
number of active flows. An important takeaway is that the number of
rounds required to transmit a packet does not depend on the number of
backlogged queues.

Consider a flow α. Let a packet of size p bits arrive as the i-th packet
in the flow. If it arrives at an empty queue, it reaches the head of the
queue at the current round R(t). If not, it reaches the head after the
packet in front of it finishes it. Combining both the scenarios, the
round number at which the packet reaches the head is given by

- 

  #+BEGIN_QUOTE
    S(i) = max(R(t), F(i−1) )
  #+END_QUOTE

where R(t) is the current round number and F(i−1) is the round at which
the packet ahead of it finishes. The round number at which a packet
finishes, which depends only on the size of the packet, is given by

- 

  #+BEGIN_QUOTE
    F(i) = S(i) + p(i)
  #+END_QUOTE

where p(i) is the size of the i-th packet in the flow. Using the above
two equations, the finish round of every packet in a queue can be
calculated.

/Packet-level Fair Queuing/

This strategy emulates the bit-by-bit fair queueing by sending the
packet which has the smallest finishing round number. At any round, the
packet chosen to be sent out is garnered from the previous round of the
algorithm. The packet which had been starved the most while sending out
the previous packet from any queue, is chosen. Let's consider the
following example:

[[file:media/image67.png]]

The figure above shows the state of the packets along with their
finishing numbers (F) in their respective queues, waiting to be
scheduled.

[[file:media/image6.png]]

The packet with the smallest finishing number (F=1002) is transmitted.
This represents the packet that was the most starved during the previous
round of scheduling.

[[file:media/image23.png]]

Similarly, in the next round (above figure), the packet with F=1007 is
transmitted and in the subsequent round (below figure), packet with
F=1009 is transmitted.

[[file:media/image4.png]]

Although this method provides fairness, it also introduces new
complexities. We will need to keep track of the finishing time at which
the head packet of each queue would depart and choose the earliest one.
This requires a priority queue implementation, which has time complexity
which is logarithmic in the number of flows! Additionally, if a new
queue becomes active, all timestamps may have to change -- which is an
operation with time complexity linear in the number of flows. Thus, the
time complexity of this method makes it hard to implement at gigabit
speeds.

*Deficit Round Robin (DRR)*

Although the bit-by-bit round robin gave us bandwidth and delay
guarantees, the time complexity was too high. It is important to note
that several applications benefit only providing bandwidth guarantees.
We could use a simple constant-time round robin algorithm with a
modification to ensure fairness.

For each flow, we assign a quantum size, Qi, and a deficit counter, Di.
The quantum size determines the share of bandwidth allocated to that
flow. For each turn of round robin, the algorithm will serve as many
packets in the flow i with size less than (Qi + Di). If there are
packets remaining in the queue, it will store the remaining bandwidth in
Di for the next run. However, if all packets in the queue are serviced
in that turn, it will clear Di to 0 for the next turn.

Consider the following example of deficit round robin:

[[file:media/image50.jpg]][[file:media/image40.jpg]]

In this router, there are four flows -- F1, F2, F3 and F4. The quantum
size for all flows is 500. Initially the deficit counters for all flows
are set to 0. Initially, the round-robin pointer points to the first
flow. The first packet of size 200 will be sent through, however the
funds are insufficient to send the second packet of size 750 too. Thus,
a deficit of 300 will remain in D1. For F2, the first packet of size 500
will be sent, leaving D2 as empty. Similarly, the first packets of F3
and F4 will be sent with D3 = 400 and D4 = 320 after the first
iteration. For the second iteration, the D1+ Q1 = 800, meaning there are
sufficient funds to send the second and third packet through. Since
there are no remaining packets, D1 will be set to 0 instead of 30 (which
is the actual remaining amount).

*Traffic Scheduling: Token Bucket*

There are scenarios where we want to set bandwidth guarantees for flows
in the same queue without separating them. For example, we can have a
scenario where we want to limit a specific type of traffic (eg news
traffic) in the network to no more than X Mbps, without putting this
traffic into a separate queue.

We will start by describing the idea of token bucket shaping. This
technique can limit the burstiness of a flow by: a) limiting the average
rate (eg 100 Kbps), and b) limiting the maximum burst size (eg the flow
can send a burst of 4KB at a rate of its choice).

[[file:media/image85.png]]

The bucket shaping technique assumes a bucket per flow, that fills with
tokens with rate R per second, and it also can have up to B tokens at
any given time. If the bucket is full with B tokens, then additional
tokens are dropped. When a packet arrives, it can go through if there
are enough tokens (equal to the size of packet in bits). If not, the
packet needs to wait until enough tokens are in the bucket. Given the
max size of B, a burst is limited to B bits per second.

In practice, the bucket shaping idea is implemented using a counter
(can't go more than max value B, and gets decremented when a bit
arrives) and a timer (to increment the counter at a rate R).

The problem with this technique is that we have one queue per flow. This
is because a flow may have a full token bucket, whereas other flows may
have an empty token bucket and therefore will need to wait.

To maintain one single queue, we use a modified version of token bucket
shaper, called token bucket policing. When a packet arrives will need to
have tokens at the bucket already there. If the bucket is empty the
packet is dropped.

*Traffic Scheduling: Leaky Bucket*

/What is the difference between policing and shaping? What is leaky
bucket? How is it used for traffic policing and shaping?/

Traffic policing and traffic shaping are mechanisms to limit the output
rate of a link. The output rate is controlled by identifying traffic
descriptor violations and then responding to them in two different ways.

[[file:media/image65.jpg]]

* Policer: When traffic rate reaches the maximum configured rate, excess
traffic is either dropped, or the setting or “marking” of a packet is
changed. The output rate appears as a saw-toothed wave.

* Shaper: A shaper typically retains excess packets in a queue or a
buffer and this excess is scheduled for later transmission. The result
is that excess traffic is delayed instead of dropped. Thus, when the
data rate is higher than the configured rate, the flow is shaped or
smoothed. Traffic shaping and policing can work in tandem.

The above figure shows the difference in the appearance of the output
rate in case of traffic policing and shaping.

Leaky Bucket is an algorithm which can be used in both traffic policing
and traffic shaping.

/Leaky Bucket/

The leaky bucket algorithm is analogous to water flowing into a leaky
bucket with the water leaking at a constant rate. The bucket, say with
capacity b, represents a buffer that holds packets and the water
corresponds to the incoming packets. The leak rate, r, is the rate at
which the packets are allowed to enter the network, which is constant
irrespective of the rate at which packets arrive.

If an arriving packet does not cause an overflow when added to the
bucket, it is said to be conforming. Otherwise, it is said to be
non-conforming. Packets classified as conforming are added to the bucket
while non-conforming packets are discarded. So if the bucket is full,
the new packet that arrives to the bucket is dropped.

Irrespective of the input rate of packets, the output rate is constant
which leads to uniform distribution of packets send to the network. This
algorithm can be implemented as a single server queue.

[[file:media/image78.jpg]]

** 
   :PROPERTIES:
   :CUSTOM_ID: section-18
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-19
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-20
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-21
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-22
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-23
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-24
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-25
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-26
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-27
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-28
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-29
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-30
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-31
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-32
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-33
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-34
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-35
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-36
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-37
   :END:

** 
   :PROPERTIES:
   :CUSTOM_ID: section-38
   :END:

** Lesson 7 SDN I
   :PROPERTIES:
   :CUSTOM_ID: lesson-7-sdn-i
   :END:

*Introduction*

In this lecture, we learn about Software Defined Networking (SDN). The
need to separate control from data plane, coupled with increasing
challenges that networks have been facing gradually led to the
development of the SDN technology. We start with a brief overview of the
stages that took place and eventually led to the development of the SDN
technology. We learn about the architecture of the SDN controllers and
we look into some example controllers. Finally, we learn about
OpenDaylight, a popular and open source project for network
programmability.

In the second lecture, we focus on data plane programmability, the P4
language (a high-level language for protocol independent packet
processors). Also, we look at a P4 example application. The SDN paradigm
has been used across multiple applications such as traffic engineering,
security, and data center network applications. In this lecture we are
looking at how an SDN approach would work within an Internet Exchange
Point.

*What led us to SDN?*

Software Defined Networking (SDN) arose as part of the process to make
computer networks more programmable. Computer networks are very complex
and especially difficult to manage for two main reasons:

- 

  #+BEGIN_QUOTE
    Diversity of equipment on the network
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Proprietary technologies for the equipment
  #+END_QUOTE

/Diversity of equipment/

Computer networks have a wide range of equipment - from routers and
switches to middleboxes such as firewalls, network address translators,
server load balancers, and intrusion detection systems (IDSs). The
network has to handle different software adhering to different protocols
for each of these equipment. Even with a network management tool
offering a central point of access, they still have to operate at a
level of individual protocols, mechanisms and configuration interfaces,
making network management very complex.

/Proprietary Technologies/

Equipment like routers and switches tend to run software which is closed
and proprietary. This means that configuration interfaces vary between
vendors. In fact, these interfaces could also differ between different
products offered by the same vendor! This makes it harder for the
network to manage all these devices centrally.

These characteristics of computer networks made them highly complex,
slow to innovate, and drove up the costs of running a network.

SDN offers new ways to redesign networks to make them more manageable!
It employs a simple idea - separation of tasks. We've seen that our code
becomes more modular and easy to manage when we divide them into smaller
functions with focused tasks. Similarly, SDN divides the network into
two planes - control plane and data plane. It uses this separation to
simplify management and speed up innovation!

*A Brief History of SDN: The Milestones*

The history of SDN can be divided into three phases:

1. Active networks

2. Control and data plane separation

3. OpenFlow API and network operating systems

Let's take a look at each phase.

/1. Active networks/

This phase took place from the mid-1990s to the early 2000s. During this
time, the internet takeoff resulted in an increase in the applications
and appeal of the internet. Researchers were keen on testing new ideas
to improve network services. However, this required standardization of
new protocols by the IETF (Internet Engineering Task Force), which was a
slow and frustrating process.

This tediousness led to the growth of active networks, which aimed at
opening up network control. Active networking envisioned a programming
interface (a network API) that exposed resources/network nodes and
supported customization of functionalities for subsets of packets
passing through the network nodes. This was the opposite of the popular
belief in the internet community - the simplicity of the network core
was important to the internet success!

In the early 1990s, the networking approach was primarily via IP or ATM
(Asynchronous Transfer Mode). Active networking became one of the first
‘clean slate' approaches to network architecture.

There were two types of programming models in active networking. These
models differ based on where the code to execute at the nodes was
carried.

- 

  #+BEGIN_QUOTE
    Capsule model -- carried in-band in data packets
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Programmable router/switch model -- established by out-of-band
    mechanisms.
  #+END_QUOTE

Although the capsule model was most closely related to active
networking, both models have some effect on the current state of SDNs.
By carrying the code in data packets, capsules brought a new data-plane
functionality across networks. They also used caching to make code
distribution more efficient. Programmable routers made decision making a
job for the network operator.

/Technology push:/

The pushes that encouraged active networking were:

- 

  #+BEGIN_QUOTE
    Reduction in computation cost. This enabled us to put more
    processing into the network.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Advancement in programming languages. For languages like Java, the
    options of platform portability, code execution safety, and VM
    (virtual machine) technology to protect the active node in case of
    misbehaving programs.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Advances in rapid code compilation and formal methods.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Funding from agencies such as DARPA (U.S. Defense Advanced Research
    Projects Agency) for a collection promoted interoperability among
    projects. This was especially beneficial because there were no
    short-term use cases to use to alleviate the skepticism people had
    about the use of active networking.
  #+END_QUOTE

/Use pull:/

The use pulls for active networking were:

- 

  #+BEGIN_QUOTE
    Network service provider frustration concerning the long timeline to
    develop and deploy new network services.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Third party interests to add value by implementing control at a more
    individualistic nature. This meant dynamically meeting the needs of
    specific applications or network conditions.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Researchers interest in having a network that would support
    large-scale experimentation.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Unified control over middleboxes. We discussed the disadvantage of
    having diverse programming models which varied not only based on the
    type of middlebox (for example, firewalls, proxies, etc) but based
    on the vendor. Active networking envisioned unified control that
    could replace individually managing these boxes. This actually
    foreshadows the trends we see now in network functions
    virtualization -- where we also attempt to provide a central
    unifying framework for networks with complex middlebox functions.
  #+END_QUOTE

It is interesting to note that the use pulls for active networks in the
mid-1990s are similar to those for SDN now!

In addition to these use cases, active networks made three major
contributions related to SDN:

1. 

   #+BEGIN_QUOTE
     Programmable functions in the network to lower the barrier to
     innovation.
   #+END_QUOTE

Active networks were one of the first to introduce the idea of using
programmable networks to overcome the slow speed of innovation in
computer networking. While many early visions for SDN concentrated on
increasing programmability of the control-plane, active networks focused
on the programmability of the data-plane. This has continued to develop
independently. Recently, this data-plane programmability has been
gaining more traction due to the emerging NFV initiatives. In addition,
the concept of isolating experimental traffic from normal traffic had
emerged from active networking and is heavily used in OpenFlow and other
SDN technologies.

2. 

   #+BEGIN_QUOTE
     Network virtualization, and the ability to demultiplex to software
     programs based on packet headers.
   #+END_QUOTE

Active networking produced a framework that described a platform that
would support experimentation with different programming models. This
was the need that led to network visualization.

3. 

   #+BEGIN_QUOTE
     The vision of a unified architecture for middlebox orchestration.
   #+END_QUOTE

The last use-pull for SDN, i.e., unified control over middleboxes was
never fully realized in the era of active networking. While it did not
directly influence network function virtualization (NFV), some lessons
from its research is useful while trying to implement unified
architecture now!

One of the biggest downfalls for active networking was that it was too
ambitious! Since it required end users to write Java code, it was too
far removed from the reality at that time, and hence was not trusted to
be safe. Since active networking was more involved in redesigning the
architecture of networks, not as much emphasis was given to performance
and security -- which users were more concerned about. However, it is
worthwhile to note that some efforts did aim to build high-performance
active routers, and there were a few notable projects that did address
the security of networks. Since there were no specific short-term
problems that active networks solved, it was harder for them to see
widespread deployment. The next efforts had a more focused scope and
distinguished between control and data planes. This difference made it
easier to focus on innovation in a specific plane and inflict widespread
change.

/2. Control and data plane separation/

This phase lasted from around 2001 to 2007. During this time, there was
a steady increase in traffic volumes and thus, network reliability,
predictability and performance became more important. Network operators
were looking for better network-management functions such as control
over paths to deliver traffic (traffic engineering). Researchers started
exploring short-term approaches that were deployable using existing
protocols. They identified the challenge in network management lay in
the way existing routers and switches tightly integrated the control and
data planes. Once this was identified, efforts to separate the two
began.

Technology push:

The technology pushes that encouraged control and data plane separation
were:

- 

  #+BEGIN_QUOTE
    Higher link speeds in backbone networks led vendors to implement
    packet forwarding directly in the hardware, thus separating it from
    the control-plane software.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Internet Service Providers (ISPs) found it hard to meet the
    increasing demands for greater reliability and new services (such as
    virtual private networks), and struggled to manage the increased
    size and scope of their networks.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Servers had substantially more memory and processing resources than
    those deployed one-two years prior. This meant that a single server
    could store all routing states and compute all routing decisions for
    a large ISP network. This also enabled simple backup replication
    strategies -- thus, ensuring controller reliability.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Open source routing software lowered the barrier to creating
    prototype implementations of centralized routing controllers.
  #+END_QUOTE

These pushes inspired two main innovations:

- 

  #+BEGIN_QUOTE
    Open interface between control and data planes
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Logically centralized control of the network
  #+END_QUOTE

This phase was different from active networking in several ways:

- 

  #+BEGIN_QUOTE
    It focused on spurring innovation by and for network administrators
    rather than end users and researchers.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    It emphasized programmability in the control domain rather than the
    data domain.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    It worked towards network-wide visibility and control rather than
    device-level configurations.
  #+END_QUOTE

Use pulls:

Some use pulls for the separation of control and data planes were:

- 

  #+BEGIN_QUOTE
    Selecting between network paths based on the current traffic load
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Minimizing disruptions during planned routing changes
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Redirecting/dropping suspected attack traffic
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Allowing customer networks more control over traffic flow
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Offering value-added services for virtual private network customers
  #+END_QUOTE

Most work during this phase tried to manage routing within a single ISP,
but there were some proposals about ways to enable flexible route
control across many administrative domains.

The attempt to separate the control and data planes resulted in a couple
of concepts which were used in further SDN design:

- 

  #+BEGIN_QUOTE
    Logically centralized control using an open interface to the data
    plane.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Distributed state management.
  #+END_QUOTE

Initially, many people thought separating the control and data planes
was a bad idea, since there was no clear idea as to how these networks
would operate if a controller failed. There was also some skepticism
about moving away from a simple network where all have a common view of
the network state to one where the router only had a local view of the
outcome of route-selection. However, this concept of separation of
planes helped researchers think clearly about distributed state
management. Several projects exploring clean-slate architecture
commenced and laid the foundation for OpenFlow API.

/3. OpenFlow API and network operating systems/

This phase took place from around 2007 to 2010. OpenFlow was born out of
the interest in the idea of network experimentation at a scale (by
researchers and funding agencies). It was able to balance the vision of
fully programmable networks and the practicality of ensuring real world
deployment. OpenFlow built on the existing hardware and enabled more
functions than earlier route controllers. Although this dependency on
hardware limited its flexibility, it enabled immediate deployment.

The basic working of an OpenFlow switch is as follows. Each switch
contains a table of packet-handling rules. Each rule has a pattern, list
of actions, set of counters and a priority. When an OpenFlow switch
receives a packet, it determines the highest priority matching rule,
performs the action associated with it and increments the counter.

Technology push:

OpenFlow was adopted in the industry, unlike its predecessors. This
could be due to:

- 

  #+BEGIN_QUOTE
    Before OpenFlow, switch chipset vendors had already started to allow
    programmers to control some forwarding behaviors.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    This allowed more companies to build switches without having to
    design and fabricate their own data plane.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Early OpenFlow versions built on technology that the switches
    already supported. This meant that enabling OpenFlow initially was
    as simple as performing a firmware upgrade!
  #+END_QUOTE

Use pulls:

- 

  #+BEGIN_QUOTE
    OpenFlow came up to meet the need of conducting large scale
    experimentation on network architectures. In the late 2000s,
    OpenFlow testbeds were deployed across many college campuses to show
    its capability on single-campus networks and wide area backbone
    networks over multiple campuses.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    OpenFlow was useful in data-center networks -- there was a need to
    manage network traffic at large scales.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Companies started investing more in programmers to write control
    programs, and less in proprietary switches that could not support
    new features easily. This allowed many smaller players to become
    competitive in the market by supporting capabilities like OpenFlow.
  #+END_QUOTE

Some key effects that OpenFlow had were:

- 

  #+BEGIN_QUOTE
    Generalizing network devices and functions.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    The vision of a network operating system.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Distributed state management techniques.
  #+END_QUOTE

*Why Separate the Data Plane from the Control Plane?*

We know that SDN differentiates from traditional approaches by
separating the control and data planes.

The control plane contains the logic that controls the forwarding
behavior of routers such as routing protocols and network middlebox
configurations. The data plane performs the actual forwarding as
dictated by the control plane. For example, IP forwarding and Layer 2
switching are functions of the data plane.

The reasons we separate the two are:

1. 

   #+BEGIN_QUOTE
     Independent evolution and development
   #+END_QUOTE

#+BEGIN_QUOTE
  In the traditional approach, routers are responsible for both routing
  and forwarding functionalities. This meant that a change to either of
  the functions would require an upgrade of hardware. In this new
  approach, routers only focus on forwarding. Thus, innovation in this
  design can proceed independently of other routing considerations.
  Similarly, improvement in routing algorithms can take place without
  affecting any of the existing routers. By limiting the interplay
  between these two functions, we can develop them more easily.
#+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Control from high-level software program
   #+END_QUOTE

#+BEGIN_QUOTE
  In SDN, we use software to compute the forwarding tables. Thus, we can
  easily use higher-order programs to control the routers' behavior. The
  decoupling of functions makes debugging and checking the behavior of
  the network easier.
#+END_QUOTE

Separation of the control and data planes supports the independent
evolution and development of both. Thus, the software aspect of the
network can evolve independent of the hardware aspect. Since both
control and forwarding behavior are separate, this enables us to use
higher-level software programs for control. This makes it easier to
debug and check the network's behavior.

/In addition, this separation leads to opportunities in different
areas./

1. Data centers. Consider large data centers with thousands of servers
and VMs. Management of such large network is not easy. SDN helps to make
network management easier.

2. Routing. The interdomain routing protocol used today, BGP, constrains
routes. There are limited controls over inbound and outbound traffic.
There is a set procedure that needs to be followed for route selection.
Additionally, it is hard to make routing decisions using multiple
criteria. With SDN, it is easier to update the router's state, and SDN
can provide more control over path selection.

3. Enterprise networks. SDN can improve the security applications for
enterprise networks. For example, using SDN it is easier to protect a
network from volumetric attacks such as DDoS, if we drop the attack
traffic at strategic locations of the network.

4. Research networks. SDN allows research networks to coexist with
production networks.

*Control Plane and Data Plane Separation*

Two important functions of the network layer are:

1. 

   #+BEGIN_QUOTE
     Forwarding
   #+END_QUOTE

/Forwarding is one of the most common, yet important functions of the
network layer. When a router receives a packet at its input link, it
must determine which output link that packet should be sent through./
This process is called forwarding. It could also entail blocking a
packet from exiting the router, if it is suspected to have been sent by
a malicious router. It could also duplicate the packet and send it along
multiple output links. Since forwarding is a local function for routers,
it usually takes place in nanoseconds and is implemented in the hardware
itself. Forwarding is a function of the data plane.

So, /a router looks at the header of an incoming packet and consults the
forwarding table, to determine the outgoing link to send the packet to./

2. 

   #+BEGIN_QUOTE
     Routing
   #+END_QUOTE

/Routing involves determining the path from the sender to the receiver
across the network. Routers rely on routing algorithms for this
purpose./ It is an end-to-end process for networks. It usually takes
place in seconds and is implemented in the software. /Routing is a
function of the control plane./

/In the traditional approach,/ the routing algorithms (control plane)
and forwarding function (data plane) are closely coupled. The router
runs and participates in the routing algorithms. From there it is able
to construct the forwarding table which consults it for the forwarding
function.

[[file:media/image87.png]]

/In the SDN approach/, on the other hand, there is a remote controller
that computes and distributes the forwarding tables to be used by every
router. This controller is physically separate from the router. It could
be located in some remote data center, managed by the ISP or some other
third party.

We have a separation of the functionalities. The routers are solely
responsible for forwarding, and the remote controllers are solely
responsible for computing and distributing the forwarding tables. The
controller is implemented in software, and therefore we say the network
is software-defined.

These software implementations are also increasingly open and publicly
available, which speeds up innovation in the field.

[[file:media/image121.png]]

*The SDN Architecture*

In the figure below we see the main components of an SDN network:

- 

  #+BEGIN_QUOTE
    SDN-controlled network elements
  #+END_QUOTE

The SDN-controlled network elements, sometimes called the infrastructure
layer, is responsible for the forwarding of traffic in a network based
on the rules computed by the SDN control plane.

- 

  #+BEGIN_QUOTE
    SDN controller
  #+END_QUOTE

The SDN controller is a logically centralized entity that acts as an
interface between the network elements and the network-control
applications.

- 

  #+BEGIN_QUOTE
    Network-control applications
  #+END_QUOTE

The network-control applications are programs that manage the underlying
network by collecting information about the network elements with the
help of SDN controller.

** [[file:media/image48.png]]
   :PROPERTIES:
   :CUSTOM_ID: section-39
   :END:

Let us now take a look at the four defining features in an SDN
architecture:

/1) Flow-based forwarding:/ The rules for forwarding packets in the
SDN-controlled switches can be computed based on any number of header
field values in various layers such as the transport-layer,
network-layer and link-layer. This differs from the traditional approach
where only the destination IP address determines the forwarding of a
packet. For example, OpenFlow allows up to 11 header field values to be
considered.

/2) Separation of data plane and control plane:/ The SDN-controlled
switches operate on the data plane and they only execute the rules in
the flow tables. Those rules are computed, installed, and managed by
software that runs on separate servers.

/3) Network control functions:/ The SDN control plane, (running on
multiple servers for increased performance and availability) consists of
two components: the controller and the network applications. The
controller maintains up-to-date network state information about the
network devices and elements (for example, hosts, switches, links) and
provides it to the network-control applications. This information, in
turn, is used by the applications to monitor and control the network
devices.

/4) A programmable network:/ The network-control applications act as the
“brain” of SDN control plane by managing the network. Example
applications can include network management, traffic engineering,
security, automation, analytics, etc. For example, we can have an
application that determines the end-to-end path between sources and
destinations in the network using Dijkstra's algorithm.

*The SDN Controller Architecture*

The SDN controller is a part of the SDN control plane and acts as an
interface between the network elements and the network-control
applications.

/An SDN controller can be broadly split into three layers:/

- 

  #+BEGIN_QUOTE
    Communication layer: communicating between the controller and the
    network elements
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Network-wide state-management layer: stores information of
    network-state
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Interface to the network-control application layer: communicating
    between controller and applications
  #+END_QUOTE

Let's look at each layer in detail starting from the bottom:

1. 

   #+BEGIN_QUOTE
     /Communication Layer:/ This layer consists of a protocol through
     which the SDN controller and the network controlled elements
     communicate. Using this protocol, the devices send locally observed
     events to the SDN controller providing the controller with a
     current view of the network state. For example, these events can be
     a new device joining the network, heartbeat indicating the device
     is up, etc. The communication between SDN controller and the
     controlled devices is known as the “southbound” interface. OpenFlow
     is an example of this protocol, which is broadly used by SDN
     controllers today.
   #+END_QUOTE

[[file:media/image47.png]]

2. 

   #+BEGIN_QUOTE
     /Network-wide state-management layer:/ This layer is about the
     network-state that is maintained by the controller. The
     network-state includes any information about the state of the
     hosts, links, switches and other controlled elements in the
     network. It also includes copies of the flow tables of the
     switches. Network-state information is needed by the SDN control
     plane to configure the flow tables.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /The interface to the network-control application layer:/ This
     layer is also known as the controller's “northbound” interface
     using which the SDN controller interacts with network-control
     applications. Network-control applications can read/write network
     state and flow tables in controller's state-management layer. The
     SDN controller can notify applications of changes in the network
     state, based on the event notifications sent by the SDN-controlled
     devices. The applications can then take appropriate actions based
     on the event. A REST interface is an example of a northbound API.
   #+END_QUOTE

The SDN controller, although viewed as a monolithic service by external
devices and applications, is implemented by distributed servers to
achieve fault tolerance, high availability and efficiency. Despite the
issues of synchronization across servers, many modern controllers such
as OpenDayLight and ONOS have solved it and prefer distributed
controllers to provide highly scalable services.

** Lesson 8 SDN II
   :PROPERTIES:
   :CUSTOM_ID: lesson-8-sdn-ii
   :END:

*Revisiting the Motivation for SDN*

In this topic, we are talking about the motivation that led to SDN.

As IP networks grew in adoption worldwide, there were a few challenges
that became more and more pronounced, such as:

1. 

   #+BEGIN_QUOTE
     Handling the ever growing complexity and dynamic nature of
     networks: The implementation of network policies required changes
     right down to each individual network device, which were often
     carried out by vendor-specific commands and required manual
     configurations. This was a heavy upkeep for operators. Traditional
     IP networks are quite far away from achieving automatic response
     mechanisms to dynamic network environment changes.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Tightly coupled architecture: The traditional IP networks consist
     of a control plane (handles network traffic) and a data plane
     (forwards traffic based on the control plane's decisions) that are
     bundled together. They are contained inside networking devices, and
     are thus not flexible to work on. This is evidenced by the fact
     that any new protocol update takes as long as 10 years, because of
     the way these changes need to percolate down to every networking
     device that is a part of the IP network.
   #+END_QUOTE

/Software Defined Networking./ This networking paradigm is an attempt to
overcome limitations of the legacy IP networking paradigm. It starts by
separating out the control logic (in the control plane) from the data
plane. With this separation, the network switches simply perform the
task of forwarding, and the control logic is purely implemented in a
logically centralized controller (or a network OS), thereby making it
possible for innovation to occur in areas of network reconfiguration and
policy enforcement. Despite the centralized nature of control logic, in
practice, production-level SDNs need a physically distributed control
plane to achieve performance, reliability and scalability.

The separation of control and data plane is achieved by using a
programming interface between the SDN controller and the switches. The
SDN controller controls the data plane elements via the API. An example
of such an API is OpenFlow. A switch in OpenFlow has one or more tables
for packet handling rules. Each rule matches a subset of network traffic
and performs actions such as dropping, forwarding, modifying etc. An
OpenFlow switch can be instructed by the controller to behave like a
firewall, switch, router, or even perform other roles like load
balancer, traffic shaper, etc.

As opposed to traditional IP networks, SDN principles allow for a
separation of concerns introduced between the definition of networking
policies, their implementation in hardware, and the forwarding of
traffic. It is this separation that allows for networking control
problems to be viewed as tractable pieces, allowing for newer networking
abstractions and simplifying networking management, allowing innovation.

[[file:media/image125.png]]

Traditionally viewed, computer networks have three planes of
functionality, which are all abstract logical concepts:

- 

  #+BEGIN_QUOTE
    /Data plane:/ These are functions and processes that forward data in
    the form of packets or frames.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Control plane:/ These refer to functions and processes that
    determine which path to use by using protocols to populate
    forwarding tables of data plane elements.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    /Management plane:/ These are services that are used to monitor and
    configure the control functionality, e.g. SNMP-based tools.
  #+END_QUOTE

In short, say if a network policy is defined in the management plane,
the control plane enforces the policy and the data plane executes the
policy by forwarding the data accordingly.

*The SDN Landscape*

In this topic we are looking at an overview of the SDN-landscape. The
landscape of the SDN architecture can be decomposed into layers as shown
in the figure below.

[[file:media/image83.png]]

Each layer performs its own functions through different technologies.
The figure above presents three perspectives of the SDN landscape: (a) a
plane-oriented view, (b) the SDN layers, and (c) a system design
perspective.

Next, for each layer, we are providing an overview of the technologies
that have been developed. Also, for some representative technologies we
are referencing links to actively maintained tutorials.

[[file:media/image57.png]]

/1. Infrastructure:/ Similar to traditional networks, an SDN
infrastructure consists of networking equipment (routers, switches and
other middlebox hardware). What is now different is that these physical
networking equipment are merely forwarding elements that do a simple
forwarding task, and any logic to operate them is directed from the
centralized control system. Popular examples of such infrastructure
equipment include OpenFlow (software) switches such as SwitchLight, Open
vSwitch, Pica8, etc. For further details and hands on tutorial please
look through these links:

OpenFlow: https://github.com/mininet/openflow-tutorial/wiki (Links to an
external site.)

/2. Southbound interfaces:/ These are interfaces that act as connecting
bridges between connecting and forwarding elements, and since they sit
in between control and data plane, they play a crucial role in
separating control and data plane functionality. These APIs are tightly
coupled with the forwarding elements of the underlying physical or
virtual infrastructure. The most popular implementation of Southbound
APIs for SDNs is OpenFlow, however there are other APIs proposed such as
ForCES, OVSDB, POF, OpFlex, OpenState, etc. For more reading and hands
on for OVSDB: http://docs.openvswitch.org/en/latest/ref/ovsdb.7/ (Links
to an external site.)

/3. Network virtualization:/ For a complete virtualization of the
network, the network infrastructure needs to provide support for
arbitrary network topologies and addressing schemes, similar to the
computing layer. Existing virtualization constructs such as VLAN, NAT
and MLPS are able to provide full network virtualization, however these
technologies are connected by a box-by-box basis configuration and there
is no unifying abstraction that can be leveraged to configure these in a
global manner, thereby making current network provisioning tasks as long
as months and years. New advancements in SDN network virtualization such
as VxLAN, NVGRE, FlowVisor, FlowN, NVP are promising.

/4. Network operating systems:/ The promise of SDN is to ease network
management and solve networking problems by using a logically
centralized controller by way of a network operating system (NOS). The
value of a NOS is in providing abstractions, essential services and
common APIs to developers. For example, while programming a network
policy, if a developer doesn't need to worry about low-level details
about data distribution among routing elements, that is an abstraction.
Such systems propel more innovation by reducing inherent complexity of
creating new network protocols and network applications. Some popular
NOSs are OpenDayLight, OpenContrail, Onix, Beacon and HP VAN SDN.

For more details and tutorials for OpenDayLight, please follow this
link:
https://www.opendaylight.org/technical-community/getting-started-for-developers/tutorials
(Links to an external site.)

/5. Northbound interfaces:/ The two core abstractions of an SDN
ecosystem are the Southbound and Northbound interfaces. We have already
seen Southbound interfaces, and that it already has a widely acceptable
norm (OpenFlow). Compared to that, a standard for Northbound interface
is still an open problem, as are its use cases. What is relatively clear
is that Northbound interfaces are supposed to be a mostly software
ecosystem, as opposed to the Southbound interfaces. Another key
requirement is the abstraction that guarantees programming language and
controller independence. Some popular examples are Floodlight, Trema,
NOX, Onix and SFNet.

For a tutorial to get a more hands on experience on Floodlight:
https://floodlight.atlassian.net/wiki/spaces/floodlightcontroller/pages/1343514/Tutorials
(Links to an external site.)

/6. Language-based virtualization:/ An important characteristic of
virtualization is the ability to express modularity and allowing
different levels of abstraction. For example, using virtualization we
can view a single physical device in different ways. This takes away the
complexity away from application developers without compromising on
security which is inherently guaranteed. Some popular examples of
programming languages that support virtualization are Pyretic,
libNetVirt, AutoSlice, RadioVisor, OpenVirteX, etc.

/7. Network programming languages:/ Network programmability can be
achieved using low-level or high-level programming languages. Using
low-level languages, it is difficult to write modular code, reuse it and
it generally leads to more error-prone development. High level
programming languages in SDNs provide abstractions, make development
more modular, code more reusable in control plane, do away with device
specific and low-level configurations, and generally allow faster
development. Some examples of network programming languages in SDNs are
Pyretic, Frenetic, Merlin, Nettle, Procera, FML, etc.

For a tutorial on Frenetic programming language:

http://frenetic-lang.github.io/tutorials/Introduction/

Pyretic: https://github.com/frenetic-lang/pyretic/wiki

/8. Network applications:/ These are the functionalities that implement
the control plane logic and translate to commands in the data plane.
SDNs can be deployed on traditional networks, and can find itself in
home area networks, data centers, IXPs etc. Due to this, there is a wide
variety of network applications such as routing, load balancing,
security enforcement, end-to-end QoS enforcement, power consumption
reduction, network virtualization, mobility management, etc. Some well
known solutions are Hedera, Aster*x, OSP, OpenQoS, Pronto, Plug-N-Serve,
SIMPLE, FAMS, FlowSense, OpenTCP, NetGraph, FortNOX, FlowNAC, VAVE, etc.

*SDN Infrastructure Layer*

In the previous topic, we talked about the landscape of an SDN
infrastructure. In this topic we are zooming into the infrastructure
layer.

The SDN infrastructure composes of networking equipment (routers,
switches and appliance hardware) performing simple forwarding tasks. The
physical devices do not have embedded intelligence or control, as the
network intelligence is now delegated to a logically centralized control
system - the Network Operating System (NOS). An important difference in
these networks is that they are built on top of open and standard
interfaces that ensure configuration and communication compatibility and
interoperability among different control plane and data plane devices.
As opposed to traditional networks that use proprietary and closed
interfaces, these networks are able to dynamically program heterogeneous
network devices as forwarding devices.

In the SDN architecture, a data plane device is a hardware or software
entity that forwards packets, while a controller is a software stack
running on commodity hardware. A model derived from OpenFlow is
currently the most widely accepted design of SDN data plane devices. It
is based on a pipeline of flow tables where each entry of a flow table
has three parts: a) a matching rule, b) actions to be executed on
matching packets, and c) counters that keep statistics of matching
packets. Other SDN-enabled forwarding device specifications include
Protocol-Oblivious Forwarding (POF) and Negotiable Datapath Models
(NDMs).

In an OpenFlow device, when a packet arrives, the lookup process starts
in the first table and ends either with a match in one of the tables of
the pipeline or with a miss (when no rule is found for that packet).
Some possible actions for the packet include:

1. 

   #+BEGIN_QUOTE
     Forward the packet to outgoing port
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Encapsulate the packet and forward it to controller
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Drop the packet
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     Send the packet to normal processing pipeline
   #+END_QUOTE

5. 

   #+BEGIN_QUOTE
     Send the packet to next flow table
   #+END_QUOTE

*SDN Southbound Interfaces*

In the previous topic, we talked about the landscape of an SDN
infrastructure. In this topic we are zooming into the Southbound
Interfaces.

The Southbound interfaces or APIs are the separating medium between the
control plane and data plane functionality.

From a legacy standpoint, development of a new switch typically takes up
to two years for commercialization. Added to that is the upgrade cycles
and time required to software development for the new product. Since the
southbound APIs represent one of the major barriers for introduction and
acceptance of any new networking technology, API proposals like OpenFlow
have received good reception. These standards promote interoperability
and deployment of vendor-agnostic devices. This has already been
achieved by the OpenFlow-enabled equipments from different vendors.

Currently , OpenFlow is the most widely accepted southbound standard for
SDNs. It provides specification to implement OpenFlow-enabled forwarding
devices, and for the communication channel between data and control
plane devices. There are three information sources provided by OpenFlow
protocol:

1. 

   #+BEGIN_QUOTE
     Event-based messages that are sent by forwarding devices to
     controller when there is a link or port change
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Flow statistics are generated by forwarding devices and collected
     by controller
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Packet messages are sent by forwarding devices to controller when
     they do not know what to do with a new incoming flow
   #+END_QUOTE

These three channels are key to provide flow-level info to the Network
Operating System (NOS).

Despite OpenFlow being the most popular southbound interface for SDN,
there are others API proposals such as ForCES, OVSDB, POF, OpFlex,
OpenState, etc. In case of ForCES, it provides a more flexible approach
to traditional network management without changing the current
architecture of the network, i.e, it does not need a logically
centralized controller. The control and data planes are separated but
potentially can also be kept in the same network element. OVSDB is
another southbound API that acts complementary to OpenFlow or Open
vSwitch. It allows the control elements to create multiple vSwitch
instances, set QoS policies on interfaces, attach interfaces to the
switches, configure tunnel interfaces on the OpenFlow data paths, manage
queues and collect statistics.

*SDN Controllers: Centralized vs Distributed*

As we've seen earlier, the biggest drawback of traditional networks is
that they are configured using low-level, device-specific instruction
sets and run mostly proprietary network operating systems. This
challenges the notion of device-agnostic developments and abstraction,
which are key ideas to solve networking problems. SDN offers these by
means of a logically centralized control. A controller is a critical
element in an SDN architecture as it is the key supporting piece for
control logic (applications) to generate network configuration based on
the policies defined by the network operator. There is a broad range of
architectural choices when it comes to controllers and control
platforms.

/Core controller functions/

Some base network service functions are what we consider the essential
functionality all controllers should provide. Functions such as
topology, statistics, notifications, device management, along with
shortest path forwarding and security mechanisms are essentials network
control functionalities that network applications may use in building
its logic. For example, security mechanisms are critical components to
provide basic isolation and security enforcements between services and
applications. For instance, high priority services' rules should always
take precedence over rules created by applications with low priority.

SDN Controllers can be categorized based on many aspects. In this topic
we will categorize them based on centralized or distributed
architecture.

/Centralized controllers/

In this architecture, we typically see a single entity that manages all
forwarding devices in the network, which is a single point of failure
and may have scaling issues. Also, a single controller may not be enough
to handle a large number of data plane elements. Some enterprise class
networks and data centers use such architectures, such as Maestro,
Beacon, NOX-MT. They use multi-threaded designs to explore parallelism
of multi-core computer architectures. For example, Beacon can deal with
more than 12 million flows per second by using large sized computing
nodes of cloud providers. Other single controller architectures such as
Trema, Ryu NOS, etc. target specific environments such as data centers,
cloud infras. Controllers such as Rosemary offer specific functionality
and guarantees security and isolation of applications by using a
container based architecture called micro-NOS.

/Distributed controllers/

Unlike single controller architectures that cannot scale in practice, a
distributed network operating system (controller) can be scaled to meet
the requirements of potentially any environment - small or large
networks. Distribution can occur in two ways: it can be a centralized
cluster of nodes or physically distributed set of elements. Typically, a
cloud provider that runs across multiple data centers interconnected by
a WAN may require a hybrid approach to distribution - clusters of
controllers inside each data center and distributed controller nodes in
different sites. Properties of distributed controllers:

1. 

   #+BEGIN_QUOTE
     Weak consistency semantics
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Fault tolerance
   #+END_QUOTE

*An example Controller: ONOS*

As we saw in the previous topic there are different types of SDN
controllers. In this topic, we are talking about an example distributed
controller.

ONOS (Open Networking Operating System) is a distributed SDN control
platform. It aims to provide a global view of the network to the
applications, scale-out performance and fault tolerance. The prototype
was built based on Floodlight, an open-source single-instance SDN
controller. The below figure depicts the high level architecture.

[[file:media/image18.png]]

Owing to the distributed architecture of ONOS, there are several ONOS
instances running in a cluster. The management and sharing of the
network state across these instances is achieved by maintaining a global
network view. This view is built by using the network topology and state
information (port, link and host information, etc) that is discovered by
each instance.

To make forwarding and policy decisions, the applications consume
information from the view and then update these decisions back to the
view. The corresponding OpenFlow managers receive the changes the
applications make to the view, and the appropriate switches are
programmed.

Titan, a graph database and a distributed key value store Cassandra is
used to implement the view. The applications interact with the network
view using the Blueprints graph API.

The distributed architecture of ONOS offers scale-out performance and
fault tolerance. Each ONOS instance serves as the master OpenFlow
controller for a group of switches. The propagation of state changes
between a switch and the network view is handled solely by the master
instance of that switch. The workload can be distributed by adding more
instances to the ONOS cluster in case the data plane increases in
capacity or the demand in the control plane goes up.

To achieve fault tolerance, ONOS redistributes the work of a failed
instance to other remaining instances. Each switch in the network
connects to multiple ONOS instances with only one instance acting as its
master. Each ONOS instance acts as a master for a subset of switches.
Upon failure of an ONOS instance, an election is held on a consensus
basis to choose a master for each of the switches that were controlled
by the failed instance. For each switch, a master is selected among the
remaining instances with which the switch had established connection. At
the end of election for all switches, each switch would have at most one
new master instance.

Zoopkeeper is used to maintain the mastership between the switch and the
controller.

*Programming the Data Plane: The Motivation*

In this topic, we are talking about the need to offer programmability on
the data plane and we are introducing P4 which is a language that was
developed for this purpose.

P4 (Programming Protocol-independent Packet Processors) is a high-level
programming language to configure switches which works in conjunction
with SDN control protocols. The popular vendor-agnostic OpenFlow
interface, which enables the control plane to manage devices from
different vendors, started with a simple rule table to match packets
based on a dozen header fields. However, this specification has grown
over the years to include multiple stages of the rule tables with
increasing number of header fields to allow better exposure of a
switch's functionalities to the controller.

Thus, to manage the demand for increasing number of header fields, a
need arises for an extensible, flexible approach to parse packets and
match header fields while also exposing an open interface to the
controllers to leverage these capabilities.

P4 is used to configure the switch programmatically and acts as a
general interface between the switches and the controller with its main
aim of allowing the controller to define how the switches operate. The
below figure explains the relationship between P4 and existing APIs such
as OpenFlow, which targets to populate forwarding rules in fixed
function switches:

[[file:media/image119.png]]

The following are the primary goals of P4:

/- Reconfigurability:/ The way parsing and processing of packets takes
place in the switches should be modifiable by the controller.

/- Protocol independence:/ To enable the switches to be independent of
any particular protocol, the controller defines a packet parser and a
set of tables mapping matches and their actions. The packet parser
extracts the header fields which are then passed on to the match+action
tables to be processed.

/- Target independence:/ The packet processing programs should be
programmed independent of the underlying target devices. These
generalized programs written in P4 should be converted into
target-dependent programs by a compiler which are then used to configure
the switch.

*Programming the Data Plane: P4's Forwarding Model*

In this topic, we are talking in more detail about P4 and the forwarding
model that this approach proposes.

The switches using P4 use a programmable parser and a set of
match+action tables to forward packets. The tables can be accessed in
multiple stages in a series or parallel manner. This contrasts with
OpenFlow, which supports only fixed parsers based on predetermined
header fields and only a series combination of match+actions tables.

The P4 model allows generalization of packet processing across various
forwarding devices such as routers, load balancers, etc., using multiple
technologies such as fixed function switches, NPUs, etc.. This
generalization allows the design of a common language to write packet
processing programs that are independent of the underlying devices. A
compiler then maps these programs to different forwarding devices.

The following are the two main operations of the P4 forwarding model:

/(i) Configure:/ These sets of operations are used to program the
parser. They specify the header fields to be processed in each
match+action stage and also define the order of these stages.

/(ii) Populate:/ The entries in the match+action tables specified during
configuration may be altered using the populate operations. It allows
addition and deletion of the entries in the tables.

In short, configuration determines the packet processing and the
supported protocols in a switch whereas population decides the policies
to be applied to the packets.

[[file:media/image98.png]]

*SDN Applications: Overview*

In this topic we are looking at an overview of the application areas of
SDN.

/1. Traffic Engineering/

This is one of the major areas of interest for SDN applications with
main focus on optimizing the traffic flow so as to minimize power
consumption, judiciously use network resources, perform load balancing,
etc. With the help of optimization algorithms and monitoring the data
plane load via southbound interfaces, the power consumption can be
reduced drastically while still maintaining the desired goals of
performance. ElasticTree is one such application which identifies and
shut downs specific links and devices depending on the traffic load.
Load balancing applications such as Plug-n-Serve and Aster*x achieve
scalability by creating rules based on wildcard patterns which enables
handling of large numbers of requests from a particular group. Another
use case of SDN applications is to automate the management of router
configuration to reduce the growth in routing tables due to duplication
of data. Large scale service providers also use SDN for traffic
optimization to scale dynamically, e.g. ALTO VPN enables dynamic
provisioning of VPNs in cloud infrastructure.

/2. Mobility and Wireless/

The existing wireless networks face various challenges in its control
plane including management of the limited spectrum, allocation of radio
resources and load-balancing. The deployment and management of various
wireless networks (WLANS, cellular networks) is made easier using SDN.
SDN-based wireless networks offer a variety of features including
on-demand virtual access points (VAPs), usage of spectrum dynamically,
sharing of wireless infrastructure, etc. OpenRadio, which is considered
as the OpenFlow for wireless, enables decoupling of the wireless
protocols from the underlying hardware by providing an abstraction
layer. Light virtual access points (LVAPs) offer an improved way of
managing wireless networks by using a one-to-one mapping between LVAPs
and clients. The Odin framework leverages LVAPs and the applications
built on it provide features such as mobility management, channel
selection algorithms, etc. In contrast to traditional wireless networks,
a user can move between APs without any visible lag as the mobility
manager can automatically move the client LVAP to a different AP.

/3. Measurement and Monitoring/

The first class of applications in this domain aims to add features to
other networking services. For example, new functions can be added
easily to measurement systems such as BISmark in an SDN-based broadband
connection, which enables the system to respond to change in network
conditions. A second class of these applications aim to improve the
existing features of SDNs using OpenFlow such as reducing the load on
the control plane arising from collection of data plane statistics using
various sampling and estimation techniques. OpenSketch is a southbound
API that offers flexibility for network measurements. OpenSample and
PayLess are examples of monitoring frameworks.

/4. Security and Dependability/

The applications in this area focus majorly on improving the security of
networks. One approach of using SDN to enhance security is to impose
security policies on the entry point to the network. Another approach is
to use programmable devices to enforce security policies on a wider
network. DDoS detection, an SDN application identifies and mitigates
DDoS flooding attacks by leveraging the timely information collected
from the network. Furthermore, SDN has also been used to detect any
anomalies in the traffic, to randomly mutate the IP addresses of hosts
to fake dynamic IPs to the attackers (OF-RHM) , and monitoring the cloud
infrastructures (CloudWatcher).

With regards to improving the security of SDN itself, there have been
simple approaches like rule prioritizations for applications. However,
there's still significant room for research and improvement in this
area.

/5. Data Center Networking/

Data Center networking can be revolutionized by the use of SDN which
aims to offer services such as live migration of networks,
troubleshooting, real-time monitoring of networks among various other
features. SDN applications can also help detect anomalous behavior in
data centers by defining different models and building application
signatures from observing the information collected from network devices
in the data center. Any deviation from the signature history can be
identified and appropriate measures can be taken. SDN also helps in
performing dynamic reconfigurations of virtual networks involved in a
live virtual network migration, which is an important feature of virtual
networks in cloud. LIME is one such SDN application which aims to
provide live migration and FlowDiff is an application which detects
abnormalities.

*SDN Application Example: A Software Defined Internet Exchange*

In a previous topic, we talked about the Internet Exchange Points (IXPs)
and their importance in today's Internet ecosystem. In this topic we are
looking at how the SDN technology could be applied to improve the
operation of an IXP. The SDN technology has many applications. In this
topic we are only looking into an example SDN application for IXPs.

The routing of packets across the Internet is currently handled through
the popular Border Gateway Protocol (BGP). However, BGP has limitations
which makes Internet routing unreliable and difficult to manage. The
three main limitations are:

1. 

   #+BEGIN_QUOTE
     Routing only on destination IP prefix - The routing is decided
     based on the destination prefix IP of the incoming packet. There's
     no flexibility to customize rules for example based on the traffic
     application or the source/destination network.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Networks have little control over end-to-end paths - Networks can
     only select paths advertised by direct neighbors. Networks cannot
     directly control preferred paths but instead have to rely on
     indirect mechanisms such as “AS Path prepending”.
   #+END_QUOTE

Using SDN, researchers have proposed to address the above BGP
limitations. SDN can perform multiple actions on the traffic by matching
over various header fields, not only by matching on the destination
prefix.

We have talked about IXPs at previous lecture, but as a reminder, an
Internet Exchange Point (IXP) is a physical location that facilitates
interconnection between networks so that they can exchange traffic and
BGP routes. In the context of the IXPs, researchers have proposed an SDN
based architecture, called SDX.

SDX was proposed to implement multiple applications including:

- 

  #+BEGIN_QUOTE
    Application specific peering - Custom peering rules can be installed
    for certain applications, such as high-bandwidth video applications
    like Netflix or YouTube which constitute a significant amount of
    traffic volume.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Traffic engineering - Controlling the inbound traffic based on
    source IP or port numbers by setting forwarding rules.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Traffic load balancing - The destination IP address can be rewritten
    based on any field in the packet header to balance the load.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Traffic redirection through middleboxes - Targeted subsets of
    traffic can be redirected to middleboxes.
  #+END_QUOTE

/SDX Architecture/

Let's look into the proposed SDX architecture.

In a traditional IXP the participant ASes connect their BGP-speaking
border router to a shared layer-two network and a BGP route server. The
layer-2 network is used for forwarding packets (data plane) and the BGP
route server is used for exchanging routing information (control plane).

In the SDX architecture, each AS the illusion of its own virtual SDN
switch that connects its border router to every other participant AS.
For example, AS A has a virtual switch connecting to the virtual
switches of ASes B and C.

Each AS can define forwarding policies as if it is the only participant
at the SDX, without influencing how other participants forward packets
on their own virtual switches. Each AS can have its own SDN applications
for dropping, modifying, or forwarding their traffic. The policies can
also be different based on the direction of the traffic (inbound or
outbound). An inbound policy is applied on the traffic coming from other
SDX participant on a virtual switch. An outbound policy is applied to
traffic from the participant's virtual switch port towards other
participants. The SDX is responsible to combine the policies from
multiple participants into a single policy for the physical switch.

To write policies SDX uses the Pyretic language to match header fields
of the packets and to express actions on the packets.

Let's consider the example of application-specific peering, and let's
see how a participant network expresses example policies:

[[file:media/image15.png]]

According to AS A's outbound policy, the HTTP traffic with destination
port 80 is forwarded to AS B and HTTPS traffic with destination port 443
is forwarded to AS C.

This is expressed using the match statement:

(match(dstport = 80) >> fwd(B)) +

(match(dstport = 443) >> fwd(C))

- 

  #+BEGIN_QUOTE
    The match statement filters and return the packets that match the
    specified port numbers.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    The sequential operator “>>” then forwards the returned packets to
    the next function.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    The fwd() function modifies the location (next destination) of the
    packet to the location of the corresponding switch.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    The parallel operator “+” applies each given policy to the packets
    and returns the combined output. If neither of the two policies
    matches, the packet is dropped.
  #+END_QUOTE

*SDN Applications: Wide Area Traffic Delivery*

In this section, we'll look at a few applications of SDN, specifically
SDX, in the domain of wide area traffic delivery.

/1. Application specific peering/

ISPs prefer dedicated ASes to handle the high volume of traffic flowing
from high bandwidth applications such as YouTube, Netflix. This can be
achieved by identifying a particular application's traffic using packet
classifiers and directing the traffic in a different path. However this
involves configuring additional and appropriate rules in the edge
routers of the ISP. This overhead can be eliminated by configuring
custom rules for flows matching a certain criteria at the SDX.

/2. Inbound traffic engineering/

An SDN enabled switch can be installed with forwarding rules based on
the source IP address and source port of the packets, thereby enabling
an AS to control how the traffic enters its network. This is in contrast
with BGP which performs routing based solely on the destination address
of a packet. Although there are workarounds such as using AS path
prepending and selective advertisements to control the inbound traffic
using BGP, they come with certain limitations. An AS's local preference
takes a higher priority for the outgoing traffic and the selective
advertisements can lead to pollution of the global routing tables.

/3. Wide-area server load balancing/

The existing approach of load balancing across multiple servers of a
service involves a client's local DNS server issuing a request to the
service's DNS server. As a response, the service DNS returns the IP
address of a server such that it balances the load in its system. This
involves DNS caching which can lead to slower responses in case of a
failure. A more efficient approach to load balancing can be achieved
with the help of SDX, as it supports modification of the packet headers.
A single anycast IP can be assigned to a service, and the destination IP
addresses of packets can be modified at the exchange point to the
desired backend server based on the request load.

/4. Redirection through middle boxes/

SDX can be used to address the challenges in existing approaches to
using middleboxes (firewalls, load balancers, etc). The placement of
middleboxes are usually targeted at important junctions, such as the
boundary of the enterprise networks with their upstream ISPs. To avoid
the high expenses involved in placing middleboxes at every location in
case of geographically large ISPs, the traffic is directed through a
fixed set of middleboxes by the ISPs. This is done by manipulating
routing protocols such as internal BGP to essentially hijack a subset of
traffic and sending it to a middlebox. This approach could result in
unnecessary additional traffic being redirected, and is also limited by
the fixed set of middleboxes. To overcome these issues, an SDX can
identify and redirect the desired traffic through a sequence of
middleboxes.

** Lesson 9 Internet Security
   :PROPERTIES:
   :CUSTOM_ID: lesson-9-internet-security
   :END:

*Introduction*

The Internet was not designed or built with security in mind. Security
came as an afterthought after adversaries started misusing or abusing
Internet services, resources and infrastructure. All the protocols that
we have learned so far can be exploited by attackers for malicious
purposes.

Internet security is a very exciting and active research area and
obviously very broad. In this lesson, we focus on specific topics which
we will explore through the lenses of what we have known so far in the
course.

Specifically, we focus on attacks that misuse the DNS protocol and the
DNS infrastructure. We learn about traffic attraction attacks that are
based on BGP abuse. We also learn about the most popular techniques to
infer network reputation. Finally, we learn about Denial of Service
attacks. Given the increasing popularity of Internet Exchange Points,
which we learned about at a previous lesson, we focus on the development
of recent DDoS defenses techniques that are offered at IXPs.

An attack on a system aims to compromise properties of a secure
communication.

We discuss how a Domain Name System (DNS) which uses Round Robin DNS
(RRDNS) and Content Distribution Networks (CDN) methods can become
vulnerable to such attacks. In particular, we look into how Fast Flux
networks operate which maintains multiple IP addresses for a single
domain name and rapidly changes them. To infer network reputation and
thereby the probability of a network abuse, we examine evidence of
abuse, interconnection patterns and likelihood of breach as possible
approaches. We next focus on the highly critical BGP hijacking attacks
which can happen via prefixes advertised by BGP, the AS-path
announcements and data-plane traffic manipulation. We go through the
possible motivations behind these attacks and try to classify them. We
look into a few ideas for detection of BGP hijacking and mitigation
techniques such as Prefix deaggregation and Mitigation with Multiple
Origin AS (MOAS). To better understand the impact of BGP hijacking, we
take up a case study on a real world incident known as the “Linktel
Incident” and analyze it. Another category of attacks is the Distributed
Denial of Service (DDoS) attack which overloads a server and network
resources, making it unavailable for other users. We briefly discuss the
different types of DDoS attacks and few mitigation strategies such as
filtering, scrubbing and FlowSpec at Internet Exchange Points (IXP). BGP
blackholing is a popular defense mechanism which is provided by network
providers as well as ISPs and IXPs. In blackholing, the traffic to the
target destination is dropped and we go in detail about how blackholing
is enabled by BGP. Although this is an effective defense mechanism in
case of high volume attacks, it has its own limitations.

*Properties of Secure Communication*

When establishing a communication, there are certain properties that are
important to ensure so the communication is secure even in the presence
of attackers:

- 

  #+BEGIN_QUOTE
    Confidentiality -- This is perhaps the first thing that comes to our
    mind when we think about a secure communication. We want to ensure
    that the message that is sent from the sender to the receiver is
    only available to the two parties. An attack scenario is that we
    have an intruder that can eavesdrop on the communication by sniffing
    or recording the exchanged messages. One measure to increase the
    chances that a communication is confidential is to encrypt the
    message so that even if the communication is intercepted, the
    message would be meaningless to the attacker.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Integrity -- In addition to confidentiality, it is important to
    ensure the message has not been somehow modified while in transit
    from the sender to the receiver. For example, an intruder could
    attack by modification, insertion or deletion of part of the
    messages send. As a countermeasure, we can introduce mechanisms that
    check for the integrity of the message.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Authentication -- When two parties are communicating, it is
    important to ensure that the two parties are who they say they are.
    For example, an intruder may try to steal information by
    impersonating another entity on the network. As a countermeasure
    against these attacks we use authentication mechanisms to verify the
    identity of a user.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Availability - A communication is not useful unless the information
    (or the service that is provided) is indeed available. So we will
    need to ensure that multiple aspects of the communication channel
    are functioning appropriately and we can cope with possible failures
    such as power outages, hardware failures, etc. or attacks that aim
    to render the system unavailable such as denial of service attacks.
  #+END_QUOTE

[[file:media/image118.png]]

In the following sections we will see examples about attackers abuse
several of the protocols that we have learned so far for malicious
purposes.

Each of these abuses may prohibit one or more of the desired properties
of secure communication.

*DNS Abuse*

We will start our discussion with DNS abuse.

Attackers have developed techniques abusing the DNS protocol so to
extend the uptime of domains that are used for malicious purposes (e.g.
Command and Control hosting infrastructure, phishing, spamming domains,
hosting illegal businesses, illegal content). The ultimate goal of this
abuse is to remain undetectable for longer.

In this lecture we will talk about some of the techniques that the
attackers have developed to abuse DNS. These techniques have their roots
in legitimate DNS-based techniques that legitimate businesses and
administrators use. We will start by talking about legitimate techniques
before seeing how attackers are twisting them for their own malicious
purposes.

/Round Robin DNS (RRDNS)/

This method is used by large websites to distribute the load of incoming
requests to several servers at a single physical location. It responds
to a DNS request with a list of DNS A records, which it then cycles
through in a round robin manner. The DNS client can then choose a record
using different strategies --choose the first record each time, use the
closest record in terms of network proximity, etc. Each A record also
has a Time to Live (TTL) for this mapping which specifies the number of
seconds the response is valid. If the lookup is repeated while the
mapping is still active, the DNS client will receive the same set of
records, albeit in a different order.

/DNS-based content delivery/

Content Distribution Networks (CDNs) also use DNS-based techniques to
distribute content but using more complex strategies. For example CDNs
distribute the load amongst multiple servers at a single location, but
also distribute these servers across the world. When accessing the name
of the service using DNS, the CDN computes the ‘nearest edge server' and
returns its IP address to the DNS client. It uses sophisticated
techniques based on network topology and current link characteristics to
determine the nearest server. This results in the content being moved
‘closer' to the DNS client which increases responsiveness and
availability. CDNs can react quickly to changes in link characteristics
as their TTL is lower than that in RRDNS.

/Fast-Flux Service Networks/

The previous two strategies provide reliability, scalability and
resilience, which is great for larger websites. However, this also
benefits spammers. Since using these techniques, a DNS request receives
multiple A records (each containing a different IP address), this makes
it harder to shut down online scams, as if even one IP address is
functional, the scam is still working. Similarly, spreading across
several servers makes the shutdown of these scams more complex!

Fast-Flux Service Networks (FFSN) is an extension of the ideas behind
RRDNS and CDN. As its name suggests, it is based on a ‘rapid' change in
DNS answers, with a TTL lower than that of RRDNS and CDN. One key
difference between FFSN and the other methods is that after the TTL
expires, it returns a different set of A records from a larger set of
compromised machines. These compromised machines act as proxies between
the incoming request and control node/mothership, forming a resilient,
robust, one-hop overlay network.

[[file:media/image30.png]][[file:media/image29.png]]

The figure above shows the content retrieval process of a benign HTTP
server. The DNS lookup returns the IP address of the control node of
that domain, and the request -- HTTP GET is sent to this control node.
The control node responds directly to the incoming request with the
content of the domain.

The figure above shows the content-retrieval process for content hosted
in a FFSN. The domain ‘www.thearmynext.info' was found in a spam email.
Here, the mothership is the control node where the actual content of the
scam is being hosted. The DNS lookup from the client returns several
different IP addresses, all belonging to compromised machines in the
network (flux agents). Each time the TTL expires, the lookup returns
completely different IP addresses. The flux-agent then relays the
request it receives (HTTP GET) to the control node, which sends content
to the flux-agent. Lastly, the content is delivered to the client. It is
interesting to note that these flux agents, although seemingly close to
each other, are usually located in different countries and belong to
different Autonomous Systems (AS).

An important aspect of Internet abuse is the infrastructure that
attackers use to support the abuse. For example, the attackers need
Internet infrastructure to support illegal content hosting, C&C
infrastructure hosting, etc.

Next we will talk about approaches that have been suggested to infer
network reputation, and hence the likelihood that a network will be
abused to facilitate attacks.

/How to Infer Network Reputation: Evidence of Abuse/

In this section, we discuss FIRE -- FInding Rogue nEtworks, a system
that monitors the Internet for rogue networks. Rogue networks are
networks whose main purpose is malicious activity such as phishing,
hosting spam pages, hosting pirated software, etc. It uses three main
data sources to identify hosts that likely belong to rogue networks

/1. Botnet command and control providers/

Several botnets still rely on centralized command and control (C&C). So
a bot-master would prefer to host their C&C on networks where it is
unlikely to be taken down. The two main types of botnets this system
considers are IRC-based botnets and HTTP-based botnets.

/2. Drive-by-download hosting providers/

Drive-by-download is a method of malware installation without
interaction with the user. It commonly occurs when the victim visits a
web page that contains an exploit for their vulnerable browser.

/3. Phish housing providers/

This data source contains URLs of servers that host phishing pages.
Phishing pages usually mimic authentic sites to steal login credentials,
credit card numbers and other personal information. These pages are
hosted on compromised servers and usually are up only for a short period
of time.

The key difference between rogue and legitimate networks is the
longevity of malicious behavior. Legitimate networks are usually able to
remove the malicious content within a few days whereas rogue networks
may let the content be up for weeks to more than a year! By disregarding
IP addresses that have been active for a short time, we ignore phishing
attacks hosted on legitimate networks and web servers that were
temporarily abused for botnet communication.

Each of these data sources produces a list (Li) of malicious IP
addresses daily. FIRE combines the information from these three lists to
identify rogue AS (organizations are considered equivalent to autonomous
systems). The approach is to identify the most malicious networks as
those which have the highest ratio of malicious IP addresses as compared
to the total owned IP addresses of that AS.

*How to Infer Network Reputation: Interconnection Patterns*

In this topic, we continue our discussion on approaches to infer network
reputation. In an earlier topic, we discussed an approach that is based
on data plane monitoring. With data plane monitoring only if a network
has a large enough concentration of blacklisted IPs it will be flagged
as malicious. We flag a network as malicious only after we have observed
indications of malicious behavior for a long enough period of time. For
example, let's say we have access to a blacklist and we observe a large
number of IPs that belong to an AS to be blacklisted for spamming,
phishing, hijacking, etc.

But in practice, it is not feasible to monitor the traffic of all
networks to detect malicious behaviors from the data plane. In addition,
the disadvantage of this approach is that it may take a long time until
a very large fraction of IPs makes it to a blacklist, and 2) the
approach does not differentiate well between networks that are
legitimate but abused, and those which are likely operated by
cyberactors.

This topic discusses a complementary approach -- ASwatch which uses
information exclusively from the control plane (ie. routing behavior) to
identify malicious networks. Also, this approach aims to detect
malicious networks that are likely run by cyberactors, or bulletproof as
they are called, rather than networks that may be badly abused.

[[file:media/image133.png]][[file:media/image80.png]]

[[file:media/image34.png]]

The approach is based on the observation that bulletproof ASes have
distinct interconnection patterns and overall different control plane
behavior from most legitimate networks. For example, let's observe
example snapshots of the topology around known bulletproof networks,
that are taken a few months apart. These networks shown as red in the
figures are found to be changing upstream providers more aggressively
than most legitimate networks, also they are found to behave
customer-provider or peering relationships with likely shady networks,
rather than connecting with directly with legitimate networks. These
behaviors help the bulletproof network to remain unnoticeable for
longer, and when complaints may start, the bulletproof network can
simply change an upstream provider.

The design of ASwatch is based on monitoring global BGP routing activity
to learn the control plane behavior of a network. The system has two
phases:

/1. Training phase/ - The system learns control-plane behavior typical
of both types of ASes. The system is given a list of known malicious and
legitimate ASes. It then tracks the behavior of these ASes over time to
track their business relationships with other ASes and their BGP
updates/withdrawals patterns. ASwatch then computes statistical features
of each AS. There are three main families of features:

- 

  #+BEGIN_QUOTE
    rewiring activity -- based on changes in the AS connecting activity.
    Frequent changes in customers/providers, connecting with less
    popular providers, etc. is usually suspicious behavior.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    IP Space Fragmentation and Churn - based on the advertised prefixes.
    Malicious ASes are likely to use small BGP prefixes to partition
    their IP address space and only advertise a small section of these
    (to avoid all of them being taken down at one if detected).
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    BGP Routing Dynamics -- The BGP announcements and withdrawals for
    malicious ASes follow different patterns from legitimate ones --
    such as periodically announcing prefixes for short periods of time.
  #+END_QUOTE

The system then uses supervised learning to capture the known behaviors
and patterns with a trained model.

/2. Operational phase/ - Given an unknown AS, it then calculates the
features for this AS. It uses the model to then assign a reputation
score to the AS. If the system assigns the AS a low reputation score for
several days in a row (indicating consistent suspicious behavior), it
identifies it as malicious.

*How to Infer Network Reputation: Likelihood of Breach*

In this section, we look at a system to predict the likelihood of a
security breach within an organization (such as the JP Morgan Chase
attack that affected almost 76 million households), by using only
externally observable features. This is important, as it allows the
model to be scalable to all organizations! The system uses these
features to train a Random Forest and predict the likelihood.

There are 3 classes of features used for this model:

/1. Mismanagement symptoms/ -- If there are misconfigurations in an
organization's network, it indicates that there may not be policies in
place to prevent such attacks or may not have the technological
capability to detect these failures. This increases the likelihood of a
breach. The features used are:

- 

  #+BEGIN_QUOTE
    Open Recursive Resolvers -- misconfigured open DNS resolvers
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    DNS Source Port Randomization -- many servers still do not implement
    this
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    BGP Misconfiguration -- short-lived routes can cause unnecessary
    updates to the global routing table
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Untrusted HTTPS Certificates -- can detect the validity of a
    certificate by TLS handshake
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Open SMTP Mail Relays -- servers should filter messages so that only
    those in the same domain can send mails/messages.
  #+END_QUOTE

/2. Malicious Activities/ -- Another factor to consider is the level of
malicious activities that are seen to originate from the organization's
network and infrastructure. We can determine this using spam traps,
darknet monitors, DNS monitors, etc. We create a reputation blacklist of
the IP addresses that are involved in some malicious activities. There
are 3 such types of malicious activities:

- 

  #+BEGIN_QUOTE
    Capturing spam activity -- for example, CBL, SBL, SpamCop
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Capturing phishing and malware activities -- for example, PhishTank,
    SURBL
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Capturing scanning activity -- for example, Dshield, OpenBL
  #+END_QUOTE

/3. Security Incident Reports/ -- Data based on actual security
incidents gives us the ground truth on which to train our machine
learning model on. The system uses 3 collections of such reports to
ensure a wider coverage area:

- 

  #+BEGIN_QUOTE
    VERIS Community Database -- This is a public effort to collect cyber
    security incidents in a common format. It is maintained by the
    Verizon RISK team. It contains more than 5000 incident reports.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    Hackmageddon -- This is an independently maintained blog that
    aggregates security incidents on a monthly basis.
  #+END_QUOTE

- 

  #+BEGIN_QUOTE
    The Web Hacking Incidents Database -- This is an actively maintained
    repository for cyber security incidents.
  #+END_QUOTE

This system uses a Random Forest (RF) classifier and compares it to a
baseline provided by a Support Vector Machine (SVM). It uses 258
features -- the features described above (divided into features based on
the timespan for which they are valid), secondary features based on
statistics from the other features, and the size of the organization.
These inputs are processed, then fed to a RF which produces a risk
probability (a float). By thresholding this value, we obtain the binary
class label. Since this data is sequential, the training-testing splits
of the data are strictly based on the time of each datapoint. The best
combination of parameters gives this model an accuracy of 90%!

*Traffic Attraction Attacks: BGP Hijacking*

/1. Classification by Affected Prefix:/ In this class of hijacking
attacks, we are primarily concerned with the IP prefixes that are
advertised by BGP. There are different ways the prefix can be targeted,
such as:

1. 

   #+BEGIN_QUOTE
     /Exact prefix hijacking/: When two different ASes (one is genuine
     and the other one is counterfeit) announce a path for the same
     prefix. This disrupts routing in such a way that traffic is routed
     towards the hijacker wherever the AS-path route is shortest,
     thereby disrupting traffic.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Sub-prefix hijacking/: This is an extension of exact prefix
     hijacking, except that in this case, the hijacking AS works with a
     sub-prefix of the genuine prefix of the real AS. This exploits the
     characteristic of BGP to favor more specific prefixes, and as a
     result route large/entire amount of traffic to the hijacking AS.
     Example: A given hijacking AS labelled AS2 announces that it has a
     path to prefix 10.10.0.0/24 which is a part of 10.10.0.0/16 owned
     by AS1.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Squatting/: In this type of attack, the hijacking AS announces a
     prefix that has not yet been announced by the owner AS.
   #+END_QUOTE

2. /Classification by AS-Path announcement/: In this class of attacks,
an illegitimate AS announces the AS-path for a prefix for which it
doesn't have ownership rights. There are different ways this can be
achieved:

1. 

   #+BEGIN_QUOTE
     /Type-0 hijacking/: This is simply an AS announcing a prefix not
     owned by itself.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Type-N hijacking/: This is an attack where the counterfeit AS
     announces an illegitimate path for a prefix that it does not own to
     create a fake link (path) between different ASes. For example,
     {AS2, ASx, ASy, AS1 -- 10.0.0.0/23} denotes a fake path between AS2
     and AS1, where there is no link between AS2 and ASx. The N denotes
     the position of the rightmost fake link in the illegitimate
     announcement, e.g. {AS2, ASy, AS1 -- 10.0.0.0/23} is a Type-2
     hijacking.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Type-U hijacking/: In this attack the hijacking AS does not modify
     the AS-PATH but may change the prefix.
   #+END_QUOTE

3. /Classification by Data-Plane traffic manipulation/: In this class of
attacks, the intention of the attacker is to hijack the network traffic
and manipulate the redirected network traffic on its way to the
receiving AS. There are three ways the attack can be realized under this
classification, i.e. traffic intercepted by the hijacker can be

1. 

   #+BEGIN_QUOTE
     /Dropped/, so that it never reaches the intended destination. This
     attack falls under the category of blackholing (BH) attack.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Eavesdropped or manipulated/ before it reaches the receiving AS,
     which is also called as man-in-the-middle attack (MM).
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Impersonated/, e.g. In this case the network traffic of the victim
     AS is impersonated and the response to this network traffic is sent
     back to the sender. This attack is called /imposture (IM) attack/.
   #+END_QUOTE

*Traffic Attraction Attacks: Motivations*

In the previous section, we looked at the types of BGP hijacking
attacks, and how to characterize them individually. In this section, we
are interested in understanding the causes or motivations behind these
attacks. Broadly viewed, the attacks can be classified as caused by:

1. 

   #+BEGIN_QUOTE
     /Human Error/: This is an accidental routing misconfiguration due
     to manual errors. This can lead to large scale exact-prefix
     hijacking. e.g: China Telecom accidentally leaked a full BGP table
     that led to large-scale Type-0 hijacking
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Targeted Attack/: In this type of attack, the hijacking AS usually
     intercepts network traffic (MM attack) while operating in stealth
     mode to remain under the radar on the control plane (Type-N and
     Type-U attacks). e.g: Visa and Mastercard's traffic were hijacked
     by Russian networks using this method in 2017
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /High Impact Attack:/ Here, the attacker is obvious in their intent
     to cause widespread disruption of services. e.g: Pakistan Telecom
     in a Type-0 sub-prefix hijacking, essentially blackholing all of
     YouTube's services worldwide for nearly 2 hours.
   #+END_QUOTE

To summarize, we can say that the motivation behind every hijacking
attempt is different, so there is no one answer when it comes to
choosing the best attack. Given the constraining scenario and intents of
the hijacking attempt, the hijacker may employ one or a combination of
methods to carry out the attack.

*Example BGP Hijack Attacks*

We briefly looked at the different types of BGP hijacking attacks in the
previous section. Let's look at how these attacks are carried out
through some examples.

Let's first look at a legitimate scenario. In the figure below, we have
a new prefix being announced by an AS to its neighbors:

1. 

   #+BEGIN_QUOTE
     AS1 announces a new prefix (10.10.0.0/16)
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     AS2, AS3, AS4 and AS5 that receive an announcement from the
     previous / neighboring AS, check whether this entry is present in
     the RIB, if new, add it, and send it to all neighboring ASes.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     As the announcements are made by AS2, AS3 and AS4; A5 eventually
     receives the full path and new prefix from A4 (4,2,1).
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     If multiple routes exist for a prefix, then the selected (best)
     route is highlighted. Also, this route is selected for announcement
     to the neighbors.
   #+END_QUOTE

[[file:media/image36.png]]

/Attack Scenario: Hijacking a prefix/

Let's look at the scenario of prefix hijacking. In this scenario, the
attacker uses a router at AS4 to send false announcements and hijack the
prefix 10.10.0.0/16 that belongs to AS1.

1. 

   #+BEGIN_QUOTE
     The attacker uses a router to announce the prefix 10.10.0.0/16 that
     belongs to AS1, with a new origin AS4, pretending that the prefix
     belongs to AS4.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     This new announcement causes a conflict of origin for the ASes that
     receive it (Multiple Origin AS or MOAS).
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     As a result of the new announcement, AS2, AS3 and AS5 receive the
     false advertisement and they compare it with the previous entries
     in their RIB.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     AS2 will not select the route as the best route as it has the same
     path length with an existing entry.
   #+END_QUOTE

5. 

   #+BEGIN_QUOTE
     AS3 and AS5 will believe the new advertisement, and they will
     update their entries (10.10.0.0/16 with path 4,2,1) to
     (10.10.0.0/16 with path 4). Therefore AS5 and AS3 will send all
     traffic for prefix 10.10.0.0/16 to AS4 instead of AS1.
   #+END_QUOTE

[[file:media/image38.png]]

/Attack Scenario: Hijacking a path/

In the figure below we see an attack scenario where a legitimate path is
hijacked.

In this scenario, the attacker manipulates received updates before
propagating them to neighbors.

1. 

   #+BEGIN_QUOTE
     AS1 advertises the prefix 10.10.0.0/16.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     AS2 and AS3 receive and propagate legitimately the path for the
     prefix.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     At AS4, the attacker compromises the update for the path by
     changing it to 4,1 and propagates it to the neighbors AS3, AS2, and
     AS5. Therefore it claims that it has direct link to AS1 so that
     others believe the new false path.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     AS5 receives the false path (4,1) “believes” the new false path and
     it adopts it. But the rest of the ASes don't adopt the new path
     because they either have an shorter path already or an equally long
     path to AS1 for the same prefix.
   #+END_QUOTE

The key observation here is that the attacker does not need not to
announce a new prefix, but rather it manipulates an advertisement before
propagating it.

[[file:media/image96.png]]

*Defending against BGP Hijacking: An example detection system*

In this section we will look at some high level ideas behind detecting
BGP hijacking. ARTEMIS is a system that is run locally by network
operators to safeguard its own prefixes against malicious BGP hijacking
attempts. The authors of the ARTEMIS paper (Sermpezis et al) describe a
self-operated manner of prefix hijacking detection.

The key ideas behind ARTEMIS are:

1. 

   #+BEGIN_QUOTE
     A /configuration file/: where all the prefixes owned by the network
     are listed here for reference. This configuration file is populated
     by the network operator.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     A /mechanism for receiving BGP updates/: this allows receiving
     updates from local routers and monitoring services. This is built
     into the system
   #+END_QUOTE

Using the local configuration file as a reference, for the received BGP
updates, ARTEMIS can check for prefixes and AS-PATH fields and trigger
alerts when there are anomalies.

[[file:media/image2.png]]

A point of consideration in BGP hijacking detection is the performance
of False Positive (FP) and False Negative (FN) rates when we use
different detection criteria. We ideally want a system with the least
number of FPs and FNs that are inconsequential. The ARTEMIS system also
allows the network operator to choose between a) accuracy and speed, and
b) FN which are inconsequential (less impact on control plane) for less
FP.

*Defending against BGP Hijacking: Example Mitigation Techniques*

For a system that protects against BGP hijacking attacks with less
manual intervention, we need automated ways of mitigation from BGP
hijacking attacks. The ARTEMIS system uses two automated techniques in
mitigating these attacks:

/1. Prefix deaggregation/: In a BGP attack scenario, the affected
network can either contact other networks or it can simply deaggregate
the prefixes that were targeted by announcing more specific prefixes of
a certain prefix. Remember our prior discussion of YouTube's services
being attacked by Pakistan Telecom. The targeted prefix was
208.65.153.0/24. Within 90 minutes, YouTube started announcing
208.65.153.128/25 and 208.65.153.0/25, thereby counteracting the attack.
Although the event required a long term solution, an immediate
mitigation was required for services to come back online.

/2. Mitigation with Multiple Origin AS (MOAS)/: Here, the idea is to
have third party organizations and service providers do BGP
announcements for a given network. It is akin to the current model that
exists for legitimizing network traffic by third parties that mitigate
DDoS attacks. When a BGP hijacking event occurs, the following steps
occur:

1. 

   #+BEGIN_QUOTE
     The third party receives a notification and immediately announces
     from their locations the hijacked prefix(es).
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     In this way, network traffic from across the world is attracted to
     the third party organization, which then scrubbs it and tunnels it
     to the legitimate AS
   #+END_QUOTE

The authors of the ARTEMIS paper put forth two main findings from their
research work:

1. 

   #+BEGIN_QUOTE
     /Outsource the task of BGP announcement to third parties:/ To
     combat against BGP hijacking attacks, having even just one single
     external organization to mitigate BGP attacks is highly effective
     against BGP attacks.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Comparison of outsourcing BGP announcements vs prefix filtering:/
     When compared against prefix filtering, which is the current
     standard defense mechanism, the research work found that filtering
     is less optimal when compared against BGP announcements.
   #+END_QUOTE

*DDoS: Background and Spoofing*

In this topic, we are talking about an another type of abuse; the Denial
of Service Attack (DDoS). We are also talking about how attackers
amplify DDoS attacks by using an additional technique called Spoofing.

A Distributed Denial of Service (DDoS) attack is an attempt to
compromise a server or network resources with a flood of traffic. To
achieve this, the attacker first compromises and deploys flooding
servers (slaves).

Later, when initiating an attack, the attacker instructs these flooding
servers to send a high volume of traffic to the victim. This results in
the victim host either becoming unreachable or in exhaustion of its
bandwidth.

[[file:media/image3.png]]

In the above figure, the master host, controlled by the attacker, sends
control messages to the three compromised slaves directing them to send
a huge amount of traffic to the victim. The packets sent from the slave
contain the source address as a random IP address and the destination as
the victim's IP address. This master slave configuration amplifies the
intensity of the attack while also making it difficult to protect
against it. The attack traffic sent by the slaves contain spoofed source
addresses making it difficult for the victim to track the slaves. Also,
since the traffic is sent from multiple sources, it's harder for the
victim to isolate and block the attack traffic.

/Spoofing/

IP spoofing is the act of setting a false IP address in the source field
of a packet with the purpose of impersonating a legitimate server. In
DDoS attacks, this can happen in two forms. In the first form, the
source IP address is spoofed, resulting in the response of the server
sent to some other client instead of the attacker's machine. This
results in wastage of network resources and the client resources while
also causing denial of service to legitimate users.

In the second type of attack, the attacker sets the same IP address in
both the source and destination IP fields. This results in the server
sending the replies to itself, causing it to crash.

*DDoS: Reflection and Amplification*

In this topic we are continuing our discussion on DDoS and we explore
two more techniques that the attackers are using to amplify the impact
of the attack; namely the techniques of reflection and amplification.

/Reflection and amplification attacks./

In a reflection attack, the attackers use a set of reflectors to
initiate an attack on the victim. A reflector is any server that sends a
response to a request. For example, any web server or a DNS server would
return a SYN ACK in response to a SYN packet as part of TCP handshake.
Other examples include query responses sent by a server or Host
Unreachable responses to a particular IP.

Here, the master directs the slaves to send spoofed requests to a very
large number of reflectors, usually in the range of 1 million. The
slaves set the source address of the packets to the victim's IP address,
thereby redirecting the response of the reflectors to the victim. Thus,
the victim receives responses from millions of reflectors resulting in
exhaustion of its bandwidth. In addition, the resources of the victim is
wasted in processing these responses, making it unable to respond to
legitimate requests. This forms the basis of a reflection attack. Let's
consider the below figure.

[[file:media/image7.png]]

The master commands the three slaves to send spoofed requests to the
reflectors, which in turn sends traffic to the victim. This is in
contrast with the conventional DDoS attack we saw in the previous
section, where the slaves directly send traffic to the victim. Note that
the victim can easily identify the reflectors from the response packets.
However, the reflector cannot identify the slave sending the spoofed
requests.

If the requests are chosen in such a way that the reflectors send large
responses to the victim, it is a reflection and amplification attack.
Not only would the victim receive traffic from millions of servers, the
response sent would be large in size, making it further difficult for
the victim to handle it.

*Defenses Against DDoS Attacks*

In this topic we are providing an overview of the tools that we have to
help with a DDoS attack is under the way or to help deter the attack.

/Traffic Scrubbing Services/

A scrubbing service diverts the incoming traffic to a specialized
server, where the traffic is “scrubbed” into either clean or unwanted
traffic. The clean traffic is then sent to its original destination.
Although this method offers fine-grained filtering of the packets, there
are monetary costs required for an in-time subscription, setup and other
recurring costs. The other limitations include reduced effectiveness due
to per packet processing and challenges in handling Tbps level attacks.
There's also a possibility of decreased performance as the traffic may
be rerouted and becoming susceptible to evasion attacks.

/ACL Filters/

Access Control List filters are deployed by ISPs or IXPs at their AS
border routers to filter out unwanted traffic. These filters, whose
implementation depends on the vendor-specific hardware, are effective
when the hardware is homogeneous and the deployment of the filters can
be automated. The drawbacks of these filters include limited scalability
and since the filtering does not occur at the ingress points, it can
exhaust the bandwidth to a neighboring AS.

/BGP Flowspec/

The flow specification feature of BGP, called Flowspec, helps to
mitigate DDoS attacks by supporting the deployment and propagation of
fine-grained filters across AS domain borders. It can be designed to
match a specific flow or be based on packet attributes like length and
fragment. It can also be based on the drop rate limit. Although flowspec
has been effective in intra-domain environment, it is not so popular in
inter-domain environments as it depends on trust and cooperation among
competitive networks.

BGP Flowspec is an extension to the BGP protocol which allows rules to
be created on the traffic flows and take corresponding actions. This
feature of BGP can help mitigate DDoS attacks by specifying appropriate
rules. The AS domain borders supporting BGP Flowspec are capable of
matching packets in a specific flow based on a variety of parameters
such as source IP, destination IP, packet length, protocol used, etc.

The following table shows the available components with an example for
FlowSpec:

[[file:media/image108.png]]

BGP Flowspec example: The following flow specification rule (specified
here in a dictionary format) filters all HTTP/HTTPS traffic from port 80
and 443 to one of the Google servers with IP 172.217.19.195 from subnet
130.89.161.0/24.

{

“type 1”: "172.217.19.195/32”

“type 2": "130.89.161.0/24"

"type 3": [6],

"type 5": [80, 443],

"action": {

"type ": "traffic-rate",

“value ": "0"

}

}

A “traffic-rate” action with value 0 discards the traffic. The other
possible actions include rate limiting, redirecting or filtering. If no
rule is specified, the default action for a rule is to accept the
incoming traffic.

In contrast to ACL filters, FlowSpec leverages the BGP control plane
making it easier to add rules to all the routers simultaneously.
Although FlowSpec is seen to be effective in intra-domain environment,
it is not so popular in inter-domain environments as it depends on trust
and cooperation among competitive networks. Also, it might not scale for
large attacks where the attack traffic originates from multiple sources
as it would multiple rules or combining the sources into one prefix.

*DDoS Mitigation Techniques: BGP Blackholing*

In this topic, we will talk about a technique called BGP blackholing,
that is a countermeasure to mitigate a DDoS attack.

With this mechanism, all the attack traffic to a targeted DoS
destination is dropped to a null location. The premise of this approach
is that the traffic is stopped closer to the source of the attack and
before it reaches the targeted victim. For a high volume attack, it
proves to be an effective strategy when compared to other mitigation
options.

This technique is implemented either the help of the upstream provider
or with the help of the IXP (if the network is peering at an IXP). With
this technique, the victim AS uses BGP to communicate the attacked
destination prefix to its upstream AS, which then drops the attack
traffic towards this prefix. Then either the provider (or the IXP) will
advertise a more specific prefix and modifying the next-hop address that
will divert the attack traffic to a null interface. The blackhole
messages are tagged with a specific BGP blackhole community attribute,
usually publicly available, to differentiate it from the regular routing
updates.

Let's look at the scenario, where blackholing is implemented with the
help of an upstream provider.

A network that offers blackholing service is known as a blackholing
provider. It is also responsible for providing the blackholing community
that should be used. Network or customer providers act as blackholing
providers at the network edge. Internet Service Providers (ISPs) or
Internet Exchange Points (IXPs) act as blackholing providers at the
Internet core.

If the blackholing provider is a peer or an upstream provider, the AS
must announce its associated blackhole community along with the
blackhole prefix. Let's consider the below figure. Assume the IP
130.149.1.1 in AS2 is under attack.

[[file:media/image132.png]]

To mitigate this attack, AS2 (victim network) announces a blackholing
message to AS1, which is the provider network. The message contains the
IP 130.149.1.1/32, which is the host IP under attack and the community
field set to AS1 : 666, which is the blackholing community of the AS1
provider. Once the provider receives the message, AS1 identifies it as a
blackholing message since it contains its blackholing community and sets
the next-hop field of the 130.149.1.1 IP to a blackholing IP, thereby
effectively dropping all the incoming traffic to host 130.149.1.1. Thus,
the victim host stops receiving the attack traffic that was sent to it.

Let's look at the scenario, where blackholing is implemented with the
help of the IXP where the victim network is already a participant.

In a similar manner, at IXPs, if the AS is a member of an IXP
infrastructure and it is under attack, it sends the blackholing messages
to the IXP route server when a member connects to the route server. The
route server then announces the message to all the connected IXP member
ASes, which then drops the traffic towards the blackholed prefix. The
null interface to which the traffic should be sent is specified by the
IXP. The blackholing message sent to the IXP should contain the IXP
blackhole community as shown in the following figure.

[[file:media/image25.png]]

Similar to the previous example, consider here an IP 130.149.1.1 in AS2
that is under attack. The victim AS, AS2 connects to the router server
of the IXP and sends a BGP blackholing message. The message contains the
IP under attack and the community field set to ASIXP : 666, which is the
blackholing community of the IXP. The route server identifies it as a
blackholing message and sets the next-hop of the 130.149.1.1 IP to a
blackholing IP. It propagates this announcement to all its member ASes,
which then drops all the traffic to host 130.149.1.1.

*DDoS Mitigation Techniques: BGP Blackholing Limitations and Problems*

One of the major drawbacks of BGP blackholing is that the destination
under attack becomes unreachable since all the traffic including the
legitimate traffic is dropped.

Consider the DDoS attack scenario in the given figure (a), where there
is no mitigation strategy in place. In the control plane, the prefix
100.10.10.0/24 is advertised by AS1. Suppose a web service running on IP
100.10.10.10 comes under attack, which falls under AS1. As shown in the
bottom section of figure (a), this results in unreachability of the
service by users from both AS2 and AS3 as the network port in AS1
becomes overloaded.

[[file:media/image131.png]]

Now, consider the scenario where AS1 uses BGP blackholing to send an
update to the IXP's route server. The message contains the prefix
100.10.10.10/32 along with the IXP's blackhole community (IXP_ASN: 666).
The route server propagates this update to the other ASes, AS2 and AS3,
shown in the top section of figure (b).

Let's assume the case where AS2 accepts the announcement and that AS3
rejects it. The possible reasons for an AS rejecting the announcement
could include voluntarily choosing not to participate in blackholing,
rejecting updates that require additional config changes or it could
simply be that the AS made a misconfiguration mistake.

Since AS2 accepts the announcement, the next hop IP for AS2 to reach the
prefix under attack is changed to the IXP's blackholing IP and traffic
towards IP 100.10.10.10/32 via AS2 is dropped. However, this causes
collateral damage since all the traffic including legitimate traffic via
AS2 is dropped.

[[file:media/image138.png]]

Also, since AS3 does not honor the announcement, it allows all the
traffic including the legitimate and attacks traffic towards IP
100.10.10.10/32 to flow via AS3.

As a result, and if the majority of the attack traffic is coming through
AS3, then the mitigation is ineffective. The same is true if a large
number of peers do not accept the blackholing announcements.

Let's look at the traffic distribution during an attack at a large IXP,
and gather some insights to understand the extent of the collateral
damage caused by blackholing.

[[file:media/image122.png]]

As can be seen from the above figure, the traffic mostly contains web
traffic on ports 80 (HTTP) and 443 (HTTPS). The attack traffic, shown in
red, is majorly from the UDP port 11211 and occupies almost 70% of the
entire traffic. This suggests an amplification attack. The ideal
solution here would be to block the traffic only from the UDP port
(11211), while allowing the remaining traffic from other ports to pass
through. However, the blackholing service drops all traffic including
the ones from other ports, which is still a significant amount.

** Lesson 10 Internet Surveillance and Censorship
   :PROPERTIES:
   :CUSTOM_ID: lesson-10-internet-surveillance-and-censorship
   :END:

*Introduction*

In this lecture, we focus on Internet Censorship, which is a special
case of Internet security. It is a more subtle category of attacks and
it presents its own unique challenges to detect it and measure it.

In this lecture, we learn about techniques that attackers have developed
to abuse popular protocols that we have learned about in earlier
lectures (such as DNS and BGP) with the goal to control access to
information.

We will focus on three types of censorship: Internet connectivity, DNS
censorship, and social media-based censorship.

*DNS Censorship: What is it?*

We will explore different censorship types and techniques. In this topic
we will start with DNS based censorship.

/What is DNS censorship?/

DNS censorship is a large scale network traffic filtering strategy opted
by a network to enforce control and censorship over Internet
infrastructure to suppress material which they deem as objectionable. An
example of large scale DNS censorship is that implemented by networks
located in China, which use a Firewall, popularly known as the Great
Firewall of China (GFW). This Firewall looks like an opaque system that
uses various techniques to censor China's internet traffic and block
access to various foreign websites.

[[file:media/image59.png]]

The GFW (shown in the figure above) works on injecting fake DNS record
responses so that access to a domain name is blocked. Since the GFW is
an opaque system, several different studies have been performed to
deduce the actual nature of the system and its functionality.

Researchers have tried to reverse engineer the GFW and to understand how
it works. Based on research (Towards a Comprehensive Picture of the
Great Firewall's DNS Censorship
https://www.usenix.org/system/files/conference/foci14/foci14-anonymous.pdf
(Links to an external site.)), researchers have started to identify some
of the properties:

1. 

   #+BEGIN_QUOTE
     /Locality of GFW nodes:/ There are two differing notions on whether
     the GFW nodes are present only at the edge ISPs or whether they are
     also present in non-bordering Chinese ASes. The majority view is
     that censorship nodes are present at the edge.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     /Centralized management:/ Since the blocklists obtained from two
     distinct GFW locations are the same, there is a high possibility of
     a central management (GFW Manager) entity that orchestrates
     blocklists.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     /Load balancing:/ GFW load balances between processes based on
     source and destination IP address. The processes are clustered
     together to collectively send injected DNS responses.
   #+END_QUOTE

/Organizations that track the GFW./

There are multiple organizations that monitor Chinese censorship for
censored domains on a continuous basis, some of which are listed below:

1. 

   #+BEGIN_QUOTE
     greatfire.org (since 2011)
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     hikinggfw.org (since 2012)
   #+END_QUOTE

*Example DNS Censorship Techniques*

In the previous topic we started talking about DNS censorship and we saw
one example based on the Great Firewall of China (GFW). Researchers have
identified (using active probing techniques and measurements) that one
of the main first censorship techniques implemented by GFW was based on
DNS injection. Let's see how that works.

/How does DNS injection work?/

DNS injection is one of the most common censorship technique employed by
the GFW. The GFW uses a ruleset to determine when to inject DNS replies
to censor network traffic. To start with, it is important to identify
and isolate the networks that use DNS injection for censorship. The
authors of the paper titled “Towards a Comprehensive Picture of the
Great Firewall's DNS Censorship” use probing techniques and vantage
points to search for injected paths and then evaluate the injection.

When tested against probes for restricted and benign domains, the
accuracy of DNS open resolvers to accurately pollute the response is
recorded over 99.9%. The steps involved in DNS injection are:

1. 

   #+BEGIN_QUOTE
     DNS probe is sent to the open DNS resolvers
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The probe is checked against the blocklist of domains and keywords
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     For domain level blocking, a fake DNS A record response is sent
     back. There are two levels of blocking domains: the first one is by
     directly blocking the domain, and the second one is by blocking it
     based on keywords present in the domain
   #+END_QUOTE

/What are the different DNS censorship techniques?/

In this section, we will provide an overview of different DNS censorship
techniques and look at their strengths and weaknesses. Some of the DNS
censorship techniques are more elementary, and some are more elaborate
in functioning and implementation. Usually, a censorship system
implements these techniques in combination to effect censorship on a
network.

/Technique 1: Packet Dropping/

As the name suggests, in packet dropping, all network traffic going to a
set of specific IP addresses is discarded. The censor identifies
undesirable traffic and chooses to not properly forward any packets it
sees associated with the traversing undesirable traffic instead of
following a normal routing protocol.

Strengths

1. 

   #+BEGIN_QUOTE
     Easy to implement
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Low cost
   #+END_QUOTE

Weaknesses

1. 

   #+BEGIN_QUOTE
     Maintenance of blocklist - It is challenging to stay up to date
     with the list of IP addresses to block
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Overblocking - If two websites share the same IP address and the
     intention is to only block one of them, there's a risk of blocking
     both
   #+END_QUOTE

/Technique 2: DNS Poisoning/

When a DNS receives a query for resolving hostname to IP address- if
there is no answer returned or an incorrect answer is sent to redirect
or mislead the user request, this scenario is called DNS Poisoning.

/Strength/

No overblocking: Since there is an extra layer of hostname translation,
access to specific hostnames can be blocked versus blanket IP address
blocking.

/Technique 3: Content Inspection/

3A. Proxy-based content inspection: This censorship technique is more
sophisticated, in that it allows for all network traffic to pass through
a proxy where the traffic is examined for content, and the proxy rejects
requests that serve objectionable content.

Strengths

1. 

   #+BEGIN_QUOTE
     Precise censorship: A very precise level of censorship can be
     achieved, down to the level of single web pages or even objects
     within the web page
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Flexible: Works well with hybrid security systems e.g. with a
     combination of other censorship techniques like packet dropping and
     DNS poisoning
   #+END_QUOTE

Weakness

Not scalable: They are expensive to implement on a large scale network
as the processing overhead is large (through a proxy)

3B. Intrusion detection system (IDS) based content inspection: An
alternative approach is to use parts of an IDS to inspect network
traffic. An IDS is easier and more cost effective to implement than a
proxy based system as it is more responsive than reactive in nature, in
that it informs the firewall rules for future censorship.

/Technique 4: Blocking with Resets/

The GFW employs this technique where it sends a TCP reset (RST) to block
individual connections that contain requests with objectionable content.
We can see this by packet capturing of requests that are normal and
requests that contain potentially flaggable keywords. Let's look at one
such example of packet capture.

Request 1: Requesting a benign web page:

Here, we see a packet trace of a web page which is benign:

cam(53382) → china(http) [SYN]

china(http) → cam(53382) [SYN, ACK]

cam(53382) → china(http) [ACK]

cam(53382) → china(http) GET / HTTP/1.0

china(http) → cam(53382) HTTP/1.1 200 OK (text/html) etc. . .
china(http) → cam(53382) . . . more of the web page

cam(53382) → china(http) [ACK]

. . . and so on until the page request is complete

Here, the request is from a client in Cambridge (cam53382) to a website
based in China (china(http)) which is served successfully

Request 2: Requesting with a potentially flaggable text within the HTTP
GET request

Here, we have a packet trace which contains flagged text:

cam(54190) → china(http) [SYN]

china(http) → cam(54190) [SYN, ACK] TTL=39

cam(54190) → china(http) [ACK]

cam(54190) → china(http) GET /?falun HTTP/1.0

china(http) → cam(54190) [RST] TTL=47, seq=1, ack=1

china(http) → cam(54190) [RST] TTL=47, seq=1461, ack=1

china(http) → cam(54190) [RST] TTL=47, seq=4381, ack=1

china(http) → cam(54190) HTTP/1.1 200 OK (text/html) etc. . . cam(54190)
→ china(http) [RST] TTL=64, seq=25, ack zeroed china(http) → cam(54190)
. . . more of the web page

cam(54190) → china(http) [RST] TTL=64, seq=25, ack zeroed china(http) →
cam(54190) [RST] TTL=47, seq=2921, ack=25

After the client (cam54190) sends the request containing flaggable
keywords, it receives 3 TCP RSTs corresponding to one request, possibly
to ensure that the sender receives a reset. The RST packets received
correspond to the sequence number of 1460 sent in the GET request

/Technique 5: Immediate Reset of Connections/

Censorship systems like GFW have blocking rules in addition to
inspecting content, to suspend traffic coming from a source immediately,
for a short period of time.

After sending a request with flaggable keywords (above), we see a series
of packet trace, like this:

cam(54191) → china(http) [SYN]

china(http) → cam(54191) [SYN, ACK] TTL=41

cam(54191) → china(http) [ACK]

china(http) → cam(54191) [RST] TTL=49, seq=1

The reset packet received by the client is from the firewall. It does
not matter that the client sends out legitimate GET requests following
one “questionable” request. It will continue to receive resets from the
firewall for a particular duration. Running different experiments
suggests that this blocking period is variable for “questionable”
requests.

*Why is DNS Manipulation Difficult to Measure?*

Anecdotal evidence suggests that more than 60 countries are currently
impacted by control of access to information through the Internet's
Domain Name System (DNS) manipulation. However, our understanding of
censorship around the world is relatively limited.

What are the challenges?

/1. Diverse Measurements:/ Such understanding would need a diverse set
of measurements spanning different geographic regions, ISPs, countries,
and regions within a single country. Since political dynamics can vary
so different ISPs can use various filtering techniques and different
organizations may implement censorship at multiple layers of the
Internet protocol stack and using different techniques. For example, an
ISP may be blocking traffic based on IP address, but another ISP may be
blocking individual web requests based on keywords.

Therefore, we need widespread longitudinal measurements to understand
global Internet manipulation and the heterogeneity of DNS manipulation,
across countries, resolvers, and domains.

/2. Need for Scale:/ At first, the methods to measure Internet
censorship were relying on volunteers who were running measurement
software on their own devices. Since this requires them to actually
install software and do measurements, we can see that this method is
unlikely to reach the scale required. There is a need for methods and
tools that are independent of human intervention and participation.

/3. Identifying the intent to restrict content access:/ While
identifying inconsistent or anomalous DNS responses can help to detect a
variety of underlying causes such as for example misconfigurations. But
identifying DNS manipulation is different and it requires that we detect
the intent to block access to content. It poses its own challenges.

So we need to rely on identifying multiple indications to infer DNS
manipulation.

/4. Ethics and Minimizing Risks:/ Obviously, there are risks associated
with involving citizens in censorship measurement studies, based on how
different countries maybe penalizing access to censored material.
Therefore it is safer to stay away from using DNS resolvers or DNS
forwarders in the home networks of individual users. Instead, it is
safer to rely on open DNS resolvers that are hosted in Internet
infrastructure, for example within Internet service providers or cloud
hosting providers).

*Example Censorship Detection Systems and Their Limitations*

/Main censorship detection systems and their limitations./

Global censorship measurement tools were created by efforts to measure
censorship by running experiments from diverse vantage points. For
example, CensMon used PlanetLab nodes in different countries. However,
many such methods are no longer in use. One the most common
systems/approaches is the OpenNet Initiative where volunteers perform
measurements on their home networks at different times since the past
decade. Relying on volunteer efforts make continuous and diverse
measurements very difficult.

In addition, Augur (about which we will talk about next)) is a new
system created to perform longitudinal global measurements using TCP/IP
side channels. However, this system focuses on identifying IP-based
disruptions as opposed to DNS-based manipulations.

*DNS Censorship: A Global Measurement Methodology*

In this section, we explore a method to identify DNS manipulation via
machine learning with a system called Iris. The figure below shows an
overview of the identification process.

[[file:media/image89.png]]

In previous sections, we discussed how the lack of diversity is an issue
while studying DNS manipulation. In order to counter that, Iris uses
open DNS resolvers located all over the globe. In order to avoid using
home routers (which are usually open due to configuration issues), this
dataset is then restricted to a few thousand that are part of the
Internet infrastructure. There are two main steps associated with this
process:

1. 

   #+BEGIN_QUOTE
     Scanning the Internet's IPv4 space for open DNS resolvers
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Identifying Infrastructure DNS Resolvers
   #+END_QUOTE

Now that we've obtained a global set of open DNS resolvers, we need to
perform the measurements. The figure below shows the overall measurement
process. The steps involved in this measurement process are:

1. 

   #+BEGIN_QUOTE
     Performing global DNS queries -- Iris queries thousands of domains
     across thousands of open DNS resolvers. To establish a baseline for
     comparison, the creators included 3 DNS domains which were under
     their control to help calculate metrics used for evaluation DNS
     manipulation.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Annotating DNS responses with auxiliary information -- To enable
     the classification, Iris annotates the IP addresses with additional
     information such as their geo-location, AS, port 80 HTTP responses,
     etc. This information is available from the Censys dataset.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     Additional PTR and TLS scanning -- One IP address could host
     several websites via virtual hosting. So, when Censes retrieves
     certificates from port 443, it could differ from one retrieved via
     TLS's Server Name Indication (SNI) extension. This results in
     discrepancies that could cause IRIS to label virtual hosting as DNS
     inconsistencies. To avoid this, Iris adds PTR and SNI certificates.
   #+END_QUOTE

After annotating the dataset, techniques are performed to clean the
dataset, and identify whether DNS manipulation is taking place or not.
Iris uses two types of metrics to identify this manipulation:

1. 

   #+BEGIN_QUOTE
     Consistency Metrics --
   #+END_QUOTE

Domain access should have some consistency, in terms of network
properties, infrastructure or content, even when accessed from different
global vantage points. Using one of the domains Iris controls gives a
set of high-confidence consistency baselines. Some consistency metrics
used are IP address, Autonomous System, HTTP Content, HTTPS Certificate,
PTRs for CDN.

2. 

   #+BEGIN_QUOTE
     Independent Verifiability Metrics
   #+END_QUOTE

In addition to the consistency metrics, they also use metrics that could
be externally verified using external data sources. Some of the
independent verifiability metrics used are: HTTPS certificate (whether
the IP address presents a valid, browser trusted certificate for the
correct domain name when queried without SNI) and HTTPS Certificate with
SNI.

If neither metrics are satisfied, the response is said to be
manipulated.

*Censorship Through Connectivity Disruptions*

In this topic we are talking about a different class of approach to
censorship, that is based on connectivity disruptions.

The highest level of Internet censorship is to completely block access
to the Internet. Intuitively, this can be done by manually disconnecting
the hardware that are critical to connect to the Internet. Although this
seems simple, it may not be feasible as the infrastructure could be
distributed over a wide area.

A more subtle approach is to use software to interrupt the routing or
packet forwarding mechanisms. Let's look at how these mechanisms would
work:

/1. Routing disruption:/ A routing mechanism decides which part of the
network can be reachable. Routers use BGP to communicate updates to
other routers in the network. The routers share which destinations it
can reach and continuously update its forwarding tables to select the
best path for an incoming packet. If this communication is disrupted or
disabled on critical routers, it could result in unreachability of the
large parts of a network. Using this approach can be easily detectable,
as it involves withdrawing previously advertised prefixes must be
withdrawn or re-advertising them with different properties and therefore
modifying the global routing state of the network, which is the control
plane.

/2. Packet filtering:/ Typically, packet filtering is used as a security
mechanism in firewalls and switches. But to disrupt a network's
connectivity, packet filtering can be used to block packets matching a
certain criteria disrupting the normal forwarding action. This approach
can be harder to detect and might require active probing of the
forwarding path or monitoring traffic of the impacted network.

Connectivity disruption can include multiple layers apart from the two
methods described above. It can include DNS-based blocking, deep packet
inspection by an ISP or the client software blocking the traffic, to
list a few.

*Connectivity disruptions: Detection*

As we saw in a previous section, obtaining a view of global censorship
can be challenging due to a variety of reasons. In this section, we
focus on a system, Augur, which uses a measurement machine to detect
filtering between hosts.

The system aims to detect if filtering exists between two hosts, a
reflector and a site. A reflector is a host which maintains a global IP
ID. A site is a host that may be potentially blocked. To identify if
filtering exists, it makes use of a third machine called the measurement
machine.

/IP ID:/

The strategy used by Augur takes advantage of the fact that any packet
that is sent by a host is assigned a unique 16-bit IP identifier (“IP
ID”), which the destination host can use to reassemble a fragmented
packet. This IP ID should be different for the packets that are
generated by the same host. Although there are multiple methods
available to determine the IP ID of a packet (randomly, per-connection
counter, etc.), maintaining a single global counter is the most commonly
used approach. The global counter is incremented for each packet that is
generated and helps in keeping track of the total number of packets
generated by that host. Using this counter, we can determine if and how
many packets are generated by a host.

In addition to the IP ID counter, the approach also leverages the fact
that when an unexpected TCP packet is sent to a host, it sends back a
RST (TCP Reset) packet. It also assumes there is no complex factors
involved such as cross-traffic or packet loss. Let's look at two
important mechanisms used by the approach:

/Probing:/

Probing is a mechanism to monitor the IP ID of a host over time. We use
the measurement machine to observe the IP ID generated by the reflector.
To do so, the measurement machine sends a TCP SYN-ACK to the reflector
and receives a TCP RST packet as the response. The RST packet received
would contain the latest IP ID that was generated by the reflector.
Thus, the measurement machine can track the IP ID counter of the
reflector at any given point.

/Perturbation:/

This is a mechanism which forces a host to increment its IP ID counter
by sending traffic from different sources such that the host generates a
response packet. The flow here is as follows:

1. 

   #+BEGIN_QUOTE
     The measurement machine sends a spoofed TCP SYN packet to the site
     with source address set to the reflector's IP address.
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     The site responds to the reflector with a TCP SYN-ACK packet.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     The reflector returns a TCP RST packet to the site while also
     incrementing its global IP ID counter by 1.
   #+END_QUOTE

Now that we know how to probe and perturb the IP ID values at a host,
let's analyze the different possible scenarios. Let the initial IP ID
counter of the reflector be 5.

/No filtering/

Assume a scenario where there's no filtering as shown in the below
figure.

[[file:media/image10.png]]

 The sequence of events is as follows:

1. 

   #+BEGIN_QUOTE
     The measurement machine probes the IP ID of the reflector by
     sending a TCP SYN-ACK packet. It receives a RST response packet
     with IP ID set to 6 (IPID (t1)).
   #+END_QUOTE

2. 

   #+BEGIN_QUOTE
     Now, the measurement machine performs perturbation by sending a
     spoofed TCP SYN to the site.
   #+END_QUOTE

3. 

   #+BEGIN_QUOTE
     The site sends a TCP SYN-ACK packet to the reflector and receives a
     RST packet as a response. The IP ID of the reflector is now
     incremented to 7.
   #+END_QUOTE

4. 

   #+BEGIN_QUOTE
     The measurement machine again probes the IP ID of the reflector and
     receives a response with the IP ID value set to 8 (IPID (t4)).
   #+END_QUOTE

The measurement machine thus observes that the difference in IP IDs
between steps 1 and 4 is 2 and infers that communication has occurred
between the two hosts.

/Inbound blocking/

The scenario where filtering occurs on the path from the site to the
reflector is termed as inbound blocking. In this case, the SYN-ACK
packet sent from the site in step 3 does not reach the reflector. Hence,
there is no response generated and the IP ID of the reflector does not
increase. The returned IP ID in step 4 will be 7 (IPID(t4)) as shown in
the figure. Since the measurement machine observes the increment in IP
ID value as 1, it detects filtering on the path from the site to the
reflector.

[[file:media/image111.png]]

/Outbound blocking/

Outbound blocking is the filtering imposed on the outgoing path from the
reflector. Here, the reflector receives the SYN-ACK packet and generates
a RST packet. As per our example, in step 3, the IP ID increments to 7.
However, the RST packet does not reach the site. When the site doesn't
receive a RST packet, it continues to resend the SYN-ACK packets at
regular intervals depending on the site's OS and its configuration. This
is shown in step 5 of the figure. It results in further increment of the
IP ID value of the reflector. In step 6, the probe by the measurement
machine reveals the IP ID has again increased by 2, which shows that
retransmission of packets has occurred. In this way, outbound blocking
can be detected.

[[file:media/image55.png]]
